{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013ea3be",
   "metadata": {},
   "source": [
    "# Pilot Before After -- Pass Three\n",
    "This is the same as Pass Two except with an additional \"sensitivity analysis\" section at the end. Also, possible minor modifications to increase compatibility with different types of database dump.\n",
    "\n",
    "## Pass Two\n",
    "This reimplements the elements of `Pilot Before After -- Pass One` that we actually use in a more streamlined manner and also does some pilot-specific analysis. This is the main analysis code used to generate the results in the paper with the working title \"A configurable approach to requesting user input and validation of low-confidence trip label inferences.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac424c9",
   "metadata": {},
   "source": [
    "## Usage\n",
    "To use, starting with a database dump tarball for each of the pilot programs plus stage:\n",
    "\n",
    "  1. Expand each tarball into its own folder\n",
    "  2. Make a new empty folder and start `mongod`, using the `--dbpath` option to tell it to store the database in that empty folder\n",
    "  3. `mongorestore` each of the dumps, one at a time, verifying that there are no weird errors. I did this in ascending order of dump size except ending with stage.\n",
    "  4. Ideally you would have \"0 document(s) failed to restore\" for all of the `mongorestore`s, but I consistently get `1400` documents failing to restore for `fc` and `1402` for `cc`.\n",
    "  5. Set the proper environment variables so this notebook can find the `emission` scripts (I do this for myself with a slightly hacky `sys.path.insert` below)\n",
    "  6. Run the notebook top to bottom. It should take on the order of 20 minutes (benchmarked on a 2015 MacBook Air) and run without errors. It takes massive (a few gigabytes) amounts of RAM, both in the `python` process and on the part of `mongod`.\n",
    "  7. The notebook is structured to load everything it needs from the database in the first few cells and then does not rely on the database again, so if you are tweaking later analyses and want to recover some RAM you can terminate the `mongod` process.\n",
    "\n",
    "tl;dr: this relies on a rather specific database configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50e42a",
   "metadata": {},
   "source": [
    "## Desired Output\n",
    "\n",
    "### All the useful information we want to keep from the previous notebook\n",
    "Dataset 1 -- no \"after update\" condition necessary:\n",
    " * Number of participants\n",
    " * Unlabeled trips that users need not interact with at all\n",
    " * Trips that would be in To Label with no red labels\n",
    "\n",
    "Dataset 2 (a subset of Dataset 1) -- yes \"after update\" condition necessary:\n",
    " * Number of participants\n",
    " * Frequency of app opens\n",
    " * Taps actually avoided by verify button\n",
    " * Taps actually avoided by hiding high confidence trips\n",
    " * Overall taps avoided (total, per trip, percentage of taps)\n",
    " * Fraction of users who used the verify button\n",
    " * Fraction of trips finalized using the verify button\n",
    "\n",
    "### New features\n",
    " * Graphs of weekly labeling percentage and number of days per week the app was used over time\n",
    " * Comparisons of pilot programs that started before the update was released to pilot programs that started with the update already installed\n",
    " * Histogram of expectation confidences, segmented by how they are presented to the user\n",
    " * Weekly labeling visualizations at per-user granularity\n",
    " * Visualization of how we save taps\n",
    " * Summary of all the numbers used in the draft paper\n",
    "\n",
    "### Still to do\n",
    " * What happens if we were to change the confidence thresholds? Can we save users more taps? This should be explored by figuring out how much of the current tapping is correcting the algorithm vs. filling in red labels vs. simply not using the new features -- which can be done with the \"select_label\" instrumentation event. If we lower the low threshold, we would expect the number of taps used to fill in red labels to decrease, but not the other two categories of taps.\n",
    " * Try to measure the time spent on each of the screens from which it is possible to label (To Label, All Unlabeled, Diary, etc.) -- this might be difficult, but it would be useful both to do a comparison between screens and also to see if this update changes the amount of time people spend labeling.\n",
    " * Maybe some more per-program breakdowns would be useful?\n",
    " * Statistical tests might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTINGMODE = False\n",
    "USING_LABEL_TEST_DB = False\n",
    "USING_MINI_TRIP_ANALYSIS_VALIDATION_DB = False\n",
    "USING_ANALYSIS_VALIDATION_DB = False\n",
    "\n",
    "# Declarations -- we declare variables here so that we don't accidentally clear them later quite as much\n",
    "EXCLUDE_UUIDS = []\n",
    "stats = {}             # dictionary with the following database keys:\n",
    "                       # \"time\": \"stats/client_time\", \"error\": \"stats/client_error\", \"nav\": \"stats/client_nav_event\"\n",
    "                       # These are found in the Stage_timeseries collection\n",
    "user_info = {}\n",
    "confirmed_trip_df_map = {}\n",
    "user_before_start = {}  # When the \"before\" period starts for each user\n",
    "user_after_start = {}  # When the \"after\" period starts for each user\n",
    "filter1_users = []  # Users with enough total trips\n",
    "filter2_users = []  # Users that have installed the update\n",
    "filter3_users = []  # Users with enough before trips\n",
    "filter4_users = [] # Users with enough after trips\n",
    "server_filtered_users = []\n",
    "filtered_users = []\n",
    "match_histogram = {}\n",
    "g_high_confidence_n_after_unlabeled = None  # Hack to give old code access to this later. TODO: rework this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ac209",
   "metadata": {},
   "source": [
    "## Choices\n",
    " * Let \"before\" be from June 1 until the user installed the update\n",
    " * Let \"after\" be from when the user installed the update until the most recent data available (as of writing, October 18)\n",
    " * Require 30 total trips for inclusion in Dataset 1\n",
    " * Require 10 trips after the switch for inclusion in Dataset 2\n",
    " * For looking at frequency of app opens, analyze the entire Dataset 2 and then look at those who have opened the app at least 5 times before the switch and 5 times after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_TRIPS_TOTAL = 30 if not USING_MINI_TRIP_ANALYSIS_VALIDATION_DB else 1 # Must be at least 1 to prevent division by zero\n",
    "if TESTINGMODE: REQUIRED_TRIPS_TOTAL = 1\n",
    "REQUIRED_TRIPS_BEFORE = 0  # Changed from 10 on 2021-12-18 -- this is a significant change in methodology!\n",
    "REQUIRED_TRIPS_AFTER = 10 if not USING_MINI_TRIP_ANALYSIS_VALIDATION_DB else 1  # Changed from 0 to 10 on 2021-12-18\n",
    "REQUIRED_OPENS_TOTAL = 0\n",
    "REQUIRED_OPENS_BEFORE = 0  # Changed from 5 on 2021-12-18 -- this is a significant change in methodology!\n",
    "REQUIRED_OPENS_AFTER = 5\n",
    "import arrow\n",
    "MY_TZ = \"America/Denver\"  # Timezone we use when that information is absent (TODO this can be inferred from other data structures)\n",
    "BEFORE_START = arrow.get(\"2021-06-01T00:00-06:00\")\n",
    "AFTER_END = arrow.get(\"2021-11-15T09:00-08:00\")\n",
    "weeks = list(arrow.Arrow.span_range(\"week\", BEFORE_START, AFTER_END))[:-1]  # The weeks we care about when doing weekly analyses\n",
    "from uuid import UUID  # This part is for if you want to manually exclude certain users\n",
    "# EXCLUDE_UUIDS = [UUID(s) for s in input(\"Enter UUIDs to exclude, separated by spaces: \").split(\" \") if len(s) > 0]\n",
    "print(EXCLUDE_UUIDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dface1ac",
   "metadata": {},
   "source": [
    "## Settings\n",
    "The below settings correctly mirror the production configuration; however, **the confidence thresholds are different for stage**. This must be kept in mind when doing analysis of stage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085830fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CATEGORIES = [\"mode_confirm\", \"purpose_confirm\", \"replaced_mode\"]\n",
    "HIGH_CONFIDENCE_THRESHOLD_PRODUCTION = 0.89  # Confidence we need to not put a trip in To Label\n",
    "# if TESTINGMODE: HIGH_CONFIDENCE_THRESHOLD_PRODUCTION = 0.80\n",
    "LOW_CONFIDENCE_THRESHOLD_PRODUCTION = 0.25  # confidenceThreshold from the config file\n",
    "OLD_TAPS = 2*len(LABEL_CATEGORIES)  # Number of taps each trip required to fully label under the old UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ba4cc",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b1cdb",
   "metadata": {},
   "source": [
    "### Imports, aliases, logging, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/mallen2/All_Label_Data/e-mission-server\")  # Works for my configuration; might be different for you\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as estag\n",
    "import emission.storage.timeseries.timequery as estt\n",
    "from statistics import mean\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Various ways to play with how large a firehose of information you want\n",
    "_default_log_level = logging.DEBUG\n",
    "def set_log_level(level):\n",
    "    logging.getLogger().setLevel(level)\n",
    "def reset_log_level():\n",
    "    global _default_log_level\n",
    "    logging.getLogger().setLevel(_default_log_level)\n",
    "set_log_level(logging.WARNING)\n",
    "\n",
    "agts = estag.AggregateTimeSeries()       \n",
    "\n",
    "db_keys = {\n",
    "    \"time\": \"stats/client_time\",\n",
    "    \"error\": \"stats/client_error\",\n",
    "    \"nav\": \"stats/client_nav_event\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381df79",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85120d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_between = lambda dataset, key, start, end: dataset[(dataset[key] >= start) & (dataset[key] <= end)]\n",
    "fu = lambda d, users=filtered_users: {k: d[k] for k in d if k in users}  # Filter users, builds a dictionary\n",
    "\n",
    "def filter_update(new, old, reason):\n",
    "    print(f\"Excluded {len(old)-len(new)} users, left with {len(new)}: {reason}\")\n",
    "\n",
    "format_frac_percent = lambda num, denom: f\"{num}/{denom}={(num/denom if denom != 0 else float('NaN')):.2%}\"\n",
    "\n",
    "delta2days = lambda d: d.days+d.seconds/86400\n",
    "\n",
    "format_arrow_comma = lambda a, b, c: f\"{a:.2f}->{b:.2f}, {c}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a4967",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56ebdb",
   "metadata": {},
   "source": [
    "### Load stats and user databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f211723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 6m and 10m on an early-2015 MacBook Air\n",
    "for key in db_keys:\n",
    "    print(f'Adding \"{db_keys[key]}\" to stats as \"{key}\"')\n",
    "    stats[key] = agts.get_data_df(db_keys[key])        # agts is a emission.storage.timeseries.aggregate_timeseries AggregateTimeSeries object defined earlier\n",
    "    print(f\"-> Done; found {stats[key].shape[0]} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "programs_all = {\"ens\": [], \"before_after\": [], \"only_after\": []}  \n",
    "# This will be a dict where keys are all pilot programs plus \n",
    "# \"ens\" for \"ensemble, no stage\" and \"stage\" for stage; \n",
    "# and values are the users corresponding to each\n",
    "# Late in this process, I added \"before_after\" and \"only_after\": \"only_after\" is an ensemble for the programs \n",
    "# where users were given the app with the update already installed, \n",
    "# and \"before_afer\" is an ensemble for the remaining programs\n",
    "only_after_programs = [\"vail\", \"pc\"]\n",
    "for u in edb.get_uuid_db().find():         # add users to proper locations in programs \n",
    "    program = u[\"user_email\"].split(\"_\")[0]    # This info is in the Stage_uuids collection of the database\n",
    "    uuid = u[\"uuid\"]\n",
    "    u[\"program\"] = program\n",
    "    if program not in programs_all.keys(): programs_all[program] = []\n",
    "    if program != \"stage\":\n",
    "        programs_all[\"ens\"].append(uuid)\n",
    "        if program in only_after_programs:\n",
    "            programs_all[\"only_after\"].append(uuid)\n",
    "        else:\n",
    "            programs_all[\"before_after\"].append(uuid)\n",
    "    programs_all[u[\"program\"]].append(uuid)\n",
    "    user_info[uuid] = u\n",
    "print(\"Programs all: \"+str({k: len(programs_all[k]) for k in programs_all}))\n",
    "\n",
    "# Ignore the small ensembles in certain cases\n",
    "programs_some = programs_all.copy()\n",
    "programs_some.pop(\"before_after\")\n",
    "programs_some.pop(\"only_after\")\n",
    "print(\"Programs some: \"+str({k: len(programs_some[k]) for k in programs_some}))\n",
    "\n",
    "programs = programs_all  # For backwards compatibility\n",
    "# The upside to this way of doing ensembles is it's really easy. The downside is a given user's data is calculated as many times as that user appears in the list of programs -- so with our current ensembles, we're doing most calculations three times when we only really need to do them once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd669f0",
   "metadata": {},
   "source": [
    "### Get Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 3m10 on an early-2015 MacBook Air\n",
    "set_log_level(logging.INFO)\n",
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "print(f\"Working with {len(all_users)} initial users\")\n",
    "\n",
    "filter0_users = [u for u in all_users if u not in EXCLUDE_UUIDS]  # Users that we don't explicitly exclude\n",
    "filter_update(filter0_users, all_users, \"presence on exclusion list\")\n",
    "\n",
    "if TESTINGMODE: gooduser = filter0_users[1]\n",
    "\n",
    "# filter out users that don't have enough trips\n",
    "for u in filter0_users:\n",
    "    ts = esta.TimeSeries.get_time_series(u if not TESTINGMODE else gooduser)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "    confirmed_trip_df_map[u] = ct_df\n",
    "    if ct_df.shape[0] >= REQUIRED_TRIPS_TOTAL: filter1_users.append(u)\n",
    "filter_update(filter1_users, filter0_users, \"not enough total trips\")   # (new, old, reason)\n",
    "\n",
    "# To find a user's UUID based on the end date of their first trip:\n",
    "# for u in filter1_users:\n",
    "#     ct_df = confirmed_trip_df_map[u].copy()\n",
    "#     ct_df.sort_values(\"end_ts\", ascending=True, inplace=True)\n",
    "#     print(u)\n",
    "#     print(arrow.get(ct_df.iloc[0][\"end_ts\"]).to(\"America/Chicago\"))\n",
    "#     print()\n",
    "\n",
    "for u in filter1_users:\n",
    "    # Convert timestamps to more usable values; find per-user starting points\n",
    "    # Makes new columns of converted timestamps for each timestamp type \n",
    "    # I used to do this later in the process, but it turns out end_arrow and user_before_start \n",
    "    # are useful for server_confirmed_users too\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    if \"end_ts\" in ct_df:\n",
    "        ct_df[\"end_arrow\"] = ct_df[\"end_ts\"].apply(arrow.get)\n",
    "        ct_df.sort_values(\"end_arrow\", ascending=True, inplace=True)\n",
    "    else:\n",
    "        print(\"end_ts not in dataframe for \"+str(u))\n",
    "    if \"metadata_write_ts\" in ct_df:\n",
    "        ct_df[\"write_arrow\"] = ct_df[\"metadata_write_ts\"].apply(arrow.get)\n",
    "    else:\n",
    "        print(\"metadata_write_ts not in dataframe for \"+str(u))\n",
    "    this_before_start = max(ct_df.iloc[0][\"end_arrow\"], BEFORE_START)\n",
    "    user_before_start[u] = this_before_start\n",
    "\n",
    "for user in filter1_users: \n",
    "    if user not in server_filtered_users:\n",
    "        server_filtered_users.append(user)\n",
    "print(f\"For metrics that don't need user interaction, working with {len(server_filtered_users)} filtered users\")\n",
    "\n",
    "reset_log_level()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5fdfc",
   "metadata": {},
   "source": [
    "### Get Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b25da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 30s and 50s on an early-2015 MacBook Air\n",
    "for u in filter1_users:\n",
    "    lts = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"label_tab_switch\")]\n",
    "    if len(lts) > 0:\n",
    "        filter2_users.append(u)\n",
    "        lts = lts.copy()\n",
    "        lts.sort_values(\"ts\", ascending=True, inplace=True)\n",
    "        this_after_start = arrow.get(lts.iloc[0][\"ts\"])   \n",
    "        user_after_start[u] = this_after_start\n",
    "filter_update(filter2_users, filter1_users, \"have not installed the update\")\n",
    "\n",
    "for u in filter2_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    n_before = filter_between(ct_df, \"end_arrow\", user_before_start[u], user_after_start[u]).shape[0]\n",
    "    if n_before >= REQUIRED_TRIPS_BEFORE: filter3_users.append(u)\n",
    "filter_update(filter3_users, filter2_users, \"not enough before trips\")\n",
    "\n",
    "for u in filter3_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    n_after = ct_df[(ct_df[\"end_arrow\"] >= user_after_start[u])].shape[0]\n",
    "    if n_after >= REQUIRED_TRIPS_AFTER: filter4_users.append(u)\n",
    "filter_update(filter4_users, filter3_users, \"not enough after trips\")\n",
    "\n",
    "for user in filter4_users:\n",
    "    if user not in filtered_users:\n",
    "        filtered_users.append(user)\n",
    "print(f\"For metrics that do need user interaction, working with {len(filtered_users)} filtered users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37ed61",
   "metadata": {},
   "source": [
    "## Results from Dataset 1!\n",
    "To review, we want:\n",
    " * Number of participants\n",
    " * Unlabeled trips that users need not interact with at all\n",
    " * Trips that would be in To Label with no red labels\n",
    "\n",
    "and breakdowns of (all of? some of?) the above for each individual pilot program region (\"program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0828b5b",
   "metadata": {},
   "source": [
    "### Number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUMBER OF USERS IN DATASET 1: {len(server_filtered_users)}\")\n",
    "print(\"Breakdown by program: \"+str({k: len([u for u in programs[k] if u in server_filtered_users]) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f906fb",
   "metadata": {},
   "source": [
    "### Unlabeled trips that users need not interact with at all\n",
    "Note: for this one, we calculate a stat across All of time, one for Before, and one for After. Currently, Before includes all of Dataset 1 (and obviously After only includes Dataset 2). TODO: figure out whether it might be a better comparison to report only Dataset 2 for Before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ed2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 30s and 50s on an early-2015 MacBook Air\n",
    "# Load the user confirmation data from the database\n",
    "# These are in Stage_timeseries (metadata.key: \"manual/label\")\n",
    "manuals = {label: agts.get_data_df(\"manual/\"+label) for label in LABEL_CATEGORIES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seems to work but is way too slow; see below for the faster way \n",
    "set_log_level(logging.WARNING)\n",
    "import emission.storage.decorations.trip_queries as esdt\n",
    "import emission.core.wrapper.entry as ecwe\n",
    "###trip_to_manuals = {}  # Dictionary by user of (dictionary by trip ID of (dictionary by label type of ()))\n",
    "trip_write_times = {} # Dictionary by user of (dictionary by trip ID)\n",
    "\n",
    "n_mislabel_events = 0\n",
    "\n",
    "malleable = lambda: type('', (), {})  # An object we can do anything with\n",
    "n_confirmed_trips = sum([confirmed_trip_df_map[u].shape[0] for u in all_users])\n",
    "print(n_confirmed_trips)\n",
    "print(sum([confirmed_trip_df_map[u].shape[1] for u in all_users]))  \n",
    "\n",
    "def final_candidate(filter_fn, potential_candidates):\n",
    "    potential_candidate_objects = [ecwe.Entry(c) for c in potential_candidates]\n",
    "    extra_filtered_potential_candidates = list(filter(filter_fn, potential_candidate_objects))\n",
    "    if len(extra_filtered_potential_candidates) == 0:\n",
    "        return None\n",
    "\n",
    "    # In general, all candidates will have the same start_ts, so no point in\n",
    "    # sorting by it. Only exception to general rule is when user first provides\n",
    "    # input before the pipeline is run, and then overwrites after pipeline is\n",
    "    # run\n",
    "    sorted_pc = sorted(extra_filtered_potential_candidates, key=lambda c:c[\"metadata\"][\"write_ts\"])\n",
    "    #print('Printing sorted_pc write time differences:')\n",
    "    #write_time_diff = sorted_pc[1]['metadata']['write_ts'] - sorted_pc[0]['metadata']['write_ts'] if len(sorted_pc) > 1 else 'NA'\n",
    "\n",
    "    # Need to count the number of sorted pc candidates and output it.\n",
    "    n_good_candidates = len(sorted_pc)\n",
    "    #print(f'len(sorted_pc) = {n_good_candidates}, write time difference is {write_time_diff}')\n",
    "\n",
    "    entry_detail = lambda c: c.data.label if \"label\" in c.data else c.data.start_fmt_time\n",
    "    logging.debug(\"sorted candidates are %s\" %\n",
    "        [{\"write_fmt_time\": c.metadata.write_fmt_time, \"detail\": entry_detail(c)} for c in sorted_pc])\n",
    "    most_recent_entry = sorted_pc[-1]\n",
    "    logging.debug(\"most recent entry is %s, %s\" %\n",
    "        (most_recent_entry.metadata.write_fmt_time, entry_detail(most_recent_entry)))\n",
    "    return most_recent_entry, n_good_candidates\n",
    "\n",
    "def get_user_input_for_trip_object(ts, trip_obj, user_input_key):\n",
    "    tq = estt.TimeQuery(\"data.start_ts\", trip_obj.data.start_ts, trip_obj.data.end_ts)\n",
    "    potential_candidates = ts.find_entries([user_input_key], tq)\n",
    "    return final_candidate(esdt.valid_user_input(ts, trip_obj), potential_candidates)\n",
    "\n",
    "for u in all_users:\n",
    "    #print(u)\n",
    "    ###trip_to_manuals[u] = {}\n",
    "    trip_write_times[u] = {}\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    print(confirmed_trip_df_map[u].shape)\n",
    "\n",
    "    for i, trip in confirmed_trip_df_map[u].iterrows():\n",
    "        #print(trip.keys())\n",
    "        #break\n",
    "        #print(i, end=\" \")\n",
    "        ###trip_to_manuals[u][trip._id] = {}\n",
    "        single_trip_write_times = {}\n",
    "        for label in manuals:\n",
    "            # ui = esdt.get_user_input_for_trip(\"analysis/confirmed_trip\", u, trip._id, \"manual/\"+label)\n",
    "            # trip_obj = ts.get_entry_from_id(\"analysis/confirmed_trip\", trip._id)\n",
    "            trip_obj = malleable()\n",
    "            trip_obj.data = trip\n",
    "            trip_obj.metadata = malleable()\n",
    "            trip_obj.metadata.time_zone = trip.start_local_dt_timezone\n",
    "\n",
    "            user_input_info = get_user_input_for_trip_object(ts, trip_obj, \"manual/\"+label)\n",
    "\n",
    "            if user_input_info is not None: \n",
    "                ui = user_input_info[0]\n",
    "                n_mislabel_events += user_input_info[1] - 1\n",
    "                single_trip_write_times[label] = ui['metadata']['write_ts']\n",
    "\n",
    "            ###trip_to_manuals[u][trip._id][label] = ui  #old code\n",
    "        \n",
    "        trip_write_times[u][trip._id] = min(single_trip_write_times.values()) if len(single_trip_write_times) > 0 else float(\"-inf\")\n",
    "\n",
    "mislabel_proportion = n_mislabel_events/n_confirmed_trips\n",
    "print(mislabel_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_matching(users):\n",
    "    for user in users:\n",
    "        ct_df = confirmed_trip_df_map[user]\n",
    "        ct_df[\"label_write_time\"] = ct_df['_id'].map(trip_write_times[u])\n",
    "        \n",
    "do_matching(server_filtered_users)\n",
    "'''\n",
    "# This may take a while -- clocked at 5m27 on an early-2015 MacBook Air\n",
    "# Calculate which trips were manually labeled before the inference algorithm ran. \n",
    "# These trips are part of the \"train\" dataset, so we need to exclude them from the \"test\" dataset.\n",
    "# To do this, we must match confirmed trip entries with entries from the user input database.\n",
    "# TODO My matcher is a rather blunt tool, and it misses a lot of matches. \n",
    "# Can we get something with success rates approaching that of esdt.get_user_input_for_trip_object \n",
    "# without sacrificing so much time?\n",
    "import time\n",
    "def filter_time_permissive(df, trip, threshold=15):       # 15 seconds threshold\n",
    "    start = trip[\"start_ts\"]\n",
    "    end = trip[\"end_ts\"]\n",
    "    before_g = df[\"start_ts\"] >= start-threshold\n",
    "    before_l = df[\"start_ts\"] <= start+threshold\n",
    "    after_g = df[\"end_ts\"] >= end-threshold\n",
    "    after_l = df[\"end_ts\"] <= end+threshold\n",
    "    result = df[before_g & before_l & after_g & after_l]\n",
    "    return result\n",
    "\n",
    "def get_write_time(df, trip):\n",
    "    if len(trip[\"user_input\"]) == 0: return float(\"NaN\")\n",
    "    candidates = filter_time_permissive(df, trip)\n",
    "    if len(candidates) not in match_histogram: match_histogram[len(candidates)] = 0\n",
    "    match_histogram[len(candidates)] += 1\n",
    "    write_times = candidates[\"metadata_write_ts\"].values\n",
    "    return min(write_times) if len(write_times) > 0 else float(\"-inf\") # If we can't find a match, assume the worst\n",
    "\n",
    "def get_write_times(trip):\n",
    "    times = [get_write_time(df, trip) for df in manuals.values()]    # for a given trip, get the write time for each label category\n",
    "    return min(times)\n",
    "\n",
    "def explore_matching(user):\n",
    "    print(user)\n",
    "    ct_df = confirmed_trip_df_map[user].copy()\n",
    "    print(ct_df.shape)\n",
    "    to_match = manuals[\"mode_confirm\"]\n",
    "    print(to_match.keys())\n",
    "\n",
    "    t1 = time.time()\n",
    "    for i, trip in ct_df.iterrows():\n",
    "        if len(trip[\"user_input\"]) == 0: continue\n",
    "        t_start = trip[\"start_ts\"]\n",
    "        t_end = trip[\"end_ts\"]\n",
    "        # print(t_start)\n",
    "        THRESHOLD = 60\n",
    "        filtered = filter_time_permissive(to_match, t_start, t_end)\n",
    "        if (filtered.shape[0] != 1):\n",
    "            print(filtered[\"metadata_write_ts\"].values)\n",
    "            print(filtered.shape[0])\n",
    "    print(time.time()-t1)\n",
    "\n",
    "# explore_matching(filtered_users[1])\n",
    "\n",
    "def do_matching(users):\n",
    "    for user in users:\n",
    "        # print(user)\n",
    "        ct_df = confirmed_trip_df_map[user]\n",
    "        # print(ct_df.keys())\n",
    "        # print(ct_df.shape)\n",
    "        ct_df[\"label_write_time\"] = ct_df.apply(lambda trip: get_write_times(trip), axis=1)\n",
    "\n",
    "do_matching(server_filtered_users)\n",
    "print(sorted(match_histogram.items()))  # We want as many items as possible to have exactly one match. Zero matches means we will be forced to exclude the trip, and multiple matches means we must take the most pessimistic match.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 19s on an early-2015 MacBook Air\n",
    "\n",
    "# Previously, we only operated on trips that were *actually* unlabeled. \n",
    "# Now, we operate on trips that were unlabeled at the time of expectation generation.\n",
    "def filter_unlabeled(df):\n",
    "    # return df[df[\"user_input\"].apply(len) == 0]\n",
    "    # Check first for label_write_time is NaN and then for label_write_time after expectation generation\n",
    "    return df[(df[\"label_write_time\"].isna()) | (df[\"label_write_time\"] > df[\"metadata_write_ts\"])]\n",
    "\n",
    "def is_unlabeled(trip):\n",
    "    # return len(trip[\"user_input\"]) == 0\n",
    "    # print(\"Labeled at: \"+str(trip[\"label_write_time\"]))\n",
    "    # print(\"Inferred at: \"+str(trip[\"metadata_write_ts\"]))\n",
    "    # tried .isna on the left but got the error: 'float' object has no attribute 'isna'\n",
    "    return (trip[\"label_write_time\"] != trip[\"label_write_time\"]) | (trip[\"label_write_time\"] > trip[\"metadata_write_ts\"])\n",
    "\n",
    "def high_stats(users):\n",
    "    global g_high_confidence_n_after_unlabeled  # see the early variable declarations near the top.\n",
    "    # if this is None, it gets set to the value we find for high_confidence_n_after_unlabeled\n",
    "    \n",
    "    total_trip_n = {}\n",
    "    total_trip_n_after = {}\n",
    "    total_trip_n_unlabeled = {}\n",
    "    total_trip_n_after_unlabeled = {}\n",
    "    high_confidence_n = {}  # Trips with inferences so confident they don't need to go in To Label\n",
    "    high_confidence_n_after = {}\n",
    "    high_confidence_n_unlabeled = {}\n",
    "    high_confidence_n_after_unlabeled = {}\n",
    "    mid_confidence_n = {}  # Trips that need to go in To Label but have no red labels\n",
    "    mid_confidence_n_after = {}\n",
    "    mid_confidence_n_unlabeled = {}\n",
    "    mid_confidence_n_after_unlabeled = {}\n",
    "    high_confidence_frac = {}\n",
    "    mid_confidence_frac = {}\n",
    "    mid_confidence_any = {}  # Trips that need to go in To Label but have at least one yellow label\n",
    "    mid_confidence_any_after = {}\n",
    "    all_confidences = []\n",
    "\n",
    "    for u in users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        total_trip_n[u] = ct_df.shape[0]\n",
    "        high_confidence_n[u] = 0\n",
    "        mid_confidence_n[u] = 0\n",
    "\n",
    "        total_trip_n_unlabeled[u] = filter_unlabeled(ct_df).shape[0]\n",
    "        high_confidence_n_unlabeled[u] = 0\n",
    "        mid_confidence_n_unlabeled[u] = 0\n",
    "\n",
    "        mid_confidence_any[u] = 0\n",
    "        mid_confidence_any_after[u] = 0\n",
    "\n",
    "        if u in filtered_users:\n",
    "            high_confidence_n_after[u] = 0\n",
    "            high_confidence_n_after_unlabeled[u] = 0\n",
    "            mid_confidence_n_after[u] = 0\n",
    "            mid_confidence_n_after_unlabeled[u] = 0\n",
    "            this_after_start = user_after_start[u]\n",
    "            trips_after = ct_df[(ct_df[\"end_arrow\"] >= this_after_start)]\n",
    "            total_trip_n_after[u] = trips_after.shape[0]\n",
    "            total_trip_n_after_unlabeled[u] = filter_unlabeled(trips_after).shape[0]\n",
    "            ids = []\n",
    "            for _, trip in trips_after.iterrows():\n",
    "                ids.append(trip[\"_id\"])\n",
    "\n",
    "        # Get each trip's inference tuples.\n",
    "        # Add to a confidence sum for each label value using the p value field of the inference tuples\n",
    "        # For a given label type, choose the highest confidence sum as the confidence value for the label type.\n",
    "        # Choose the confidence of the least confident label type as the trip's confidence.\n",
    "        for _, trip in ct_df.iterrows():\n",
    "            inference = trip[\"inferred_labels\"]\n",
    "            # Here goes a quick and partial reimplementation of the on-the-fly (client-side) inference algorithm\n",
    "            confidences = {}\n",
    "            for label_type in LABEL_CATEGORIES:\n",
    "                counter = {}\n",
    "                for line in inference:\n",
    "                    if label_type not in line[\"labels\"]: continue  # Seems we have some incomplete tuples!\n",
    "                    val = line[\"labels\"][label_type]\n",
    "                    if val not in counter: counter[val] = 0\n",
    "                    counter[val] += line[\"p\"]\n",
    "                confidences[label_type] = max(counter.values()) if len(counter) > 0 else 0 # This needs to be max, not sum!!! A major bug in a previous version.\n",
    "            trip_confidence = min(confidences.values())\n",
    "            all_confidences.append(trip_confidence)\n",
    "            # if (trip_confidence >= 0.01 and trip_confidence <= 0.99): print(trip_confidence)\n",
    "            if trip_confidence > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                high_confidence_n[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                    high_confidence_n_after[u] += 1\n",
    "                    if is_unlabeled(trip): high_confidence_n_after_unlabeled[u] += 1\n",
    "                if is_unlabeled(trip): high_confidence_n_unlabeled[u] += 1\n",
    "            if trip_confidence > LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                mid_confidence_n[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                    mid_confidence_n_after[u] += 1\n",
    "                    if is_unlabeled(trip): mid_confidence_n_after_unlabeled[u] += 1\n",
    "                if is_unlabeled(trip): mid_confidence_n_unlabeled[u] += 1\n",
    "            \n",
    "            # What's happening with mid_any?\n",
    "            if max(confidences.values()) > LOW_CONFIDENCE_THRESHOLD_PRODUCTION and trip_confidence <= LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                mid_confidence_any[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                        mid_confidence_any_after[u] += 1\n",
    "        high_confidence_frac[u] = high_confidence_n[u] / total_trip_n[u]\n",
    "        in_to_label = total_trip_n[u]-high_confidence_n[u]\n",
    "        mid_confidence_frac[u] = (mid_confidence_n[u]-high_confidence_n[u]) / in_to_label if in_to_label != 0 else float(\"NaN\")\n",
    "        \n",
    "    results = {\"high\": {\n",
    "                \"All\": (sum(high_confidence_n_unlabeled.values()), sum(total_trip_n_unlabeled.values())),\n",
    "                \"Before\": (sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_unlabeled.values())-sum(total_trip_n_after_unlabeled.values())),\n",
    "                \"After\": (sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_after_unlabeled.values()))\n",
    "               },\n",
    "               \"mid\": {  # For mid confidence, we subtract the high confidence counts from both numerator and denominator to only capture what's going on in To Label\n",
    "                \"All\": (sum(mid_confidence_n_unlabeled.values())-sum(high_confidence_n_unlabeled.values()), sum(total_trip_n_unlabeled.values())-sum(high_confidence_n_unlabeled.values())),\n",
    "                \"Before\": ((sum(mid_confidence_n_unlabeled.values())-sum(mid_confidence_n_after_unlabeled.values()))-(sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values())), (sum(total_trip_n_unlabeled.values())-sum(total_trip_n_after_unlabeled.values()))-(sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()))),\n",
    "                \"After\": (sum(mid_confidence_n_after_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_after_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()))\n",
    "               },\n",
    "               # Should there be denominators for these?\n",
    "               \"mid_any\": {\n",
    "                   \"All\": sum(mid_confidence_any.values()),\n",
    "                   \"Before\": sum(mid_confidence_any.values())-sum(mid_confidence_any_after.values()),\n",
    "                   \"After\": sum(mid_confidence_any_after.values())\n",
    "               },\n",
    "               \"all_confidences\": all_confidences}\n",
    "    # Let's rework this hack? Does it only compute for the first program?\n",
    "    if g_high_confidence_n_after_unlabeled is None: g_high_confidence_n_after_unlabeled = high_confidence_n_after_unlabeled\n",
    "\n",
    "    return results, high_confidence_n_after_unlabeled\n",
    "\n",
    "# Get the inference confidences for each program\n",
    "high_stats_each_program = {k: high_stats([u for u in programs[k] if u in server_filtered_users]) for k in programs}\n",
    "\n",
    "complete_results = {k: high_stats_each_program[k][0] for k in programs}\n",
    "all_high_confidence_n_after_unlabeled = {k: high_stats_each_program[k][1] for k in programs}\n",
    "\n",
    "if len(all_high_confidence_n_after_unlabeled['ens']) >0:\n",
    "    g_high_confidence_n_after_unlabeled = all_high_confidence_n_after_unlabeled['ens'] \n",
    "else:   # if there's nothing in ensemble, we are probably working with stage data only\n",
    "    g_high_confidence_n_after_unlabeled = all_high_confidence_n_after_unlabeled['stage'] \n",
    "\n",
    "# Info on mini-trip-analysis-validation-db:\n",
    "# counts of each confirmed trip in Stage_analysis_timeseries: low/mid/high/labeled/incomplete_inference: 32/24/10/5/2\n",
    "# total confirmed trips: 73. number of unlabeled trips: 68\n",
    "# Totals after: All,high,mid,low,incomplete,labeled:\n",
    "#   [24, 2, 6, 16, 0, 0]\n",
    "if USING_MINI_TRIP_ANALYSIS_VALIDATION_DB: \n",
    "    test_results = complete_results['stage'] \n",
    "    # high\n",
    "    assert sum(g_high_confidence_n_after_unlabeled.values()) == 2\n",
    "    assert test_results['high']['All'] == (10,68)\n",
    "    assert test_results['high']['Before'] == (10-2,68-24)  # the total number of unlabeled 'after' trips is 24\n",
    "    assert test_results['high']['After'] == (2,24)\n",
    "\n",
    "    # mid\n",
    "    assert test_results['mid']['All'] == (24,68-10)\n",
    "    assert test_results['mid']['Before'] == (24-6,68-24-8)\n",
    "    assert test_results['mid']['After'] == (6,24-2)\n",
    "\n",
    "print(\"Considering only unlabeled data, we calculate the average percentage of trips users do not need to interact at all with:\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"high\"]:\n",
    "        print(f\"\\t{s}: {format_frac_percent(*complete_results[k]['high'][s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for u in server_filtered_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    for _,row in ct_df.iterrows():\n",
    "        if is_unlabeled(row):\n",
    "            i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc062722",
   "metadata": {},
   "source": [
    "### Trips that would be in To Label with no red labels\n",
    "Same note as above applies here.\n",
    "\n",
    "Numerator is number of trips in To Label with no red labels, denominator is number of trips in To Label at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considering only unlabeled data, for trips that would appear in To Label, we calculate the percent of trips with no red labels\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"mid\"]:\n",
    "        print(f\"\\t{s}: {format_frac_percent(*complete_results[k]['mid'][s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88198257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considering only unlabeled data, we calculate the number of trips appearing in To Label with some red labels but with at least yellow labels, just to reassure ourselves that it is a lot:\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"mid_any\"]:\n",
    "        print(f\"\\t{s}: {complete_results[k]['mid_any'][s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027ade4",
   "metadata": {},
   "source": [
    "# Construct the test database\n",
    "##### Collect trips with each confidence level from the analysis-validation database.\n",
    "I also mongodumped the full Stage_timeseries and Stage_uuid collections from analysis-validation-db\\\n",
    "and mongorestored them into mini-trip-analysis-validation-db.\\\n",
    "eg docker exec analysis-validation-db sh -c 'mongodump --archive --db=Stage_database --collection=Stage_uuids --query='{}' ' > db_uuids.dump\\\n",
    "And for mongorestore:\\\n",
    "docker exec -i mini-trip-analysis-validation-db sh -c \"cd ~/mini_analysis_validation && mongorestore --archive\" < ~/All_Label_Data/ mini_analysis_validation/db_uuids.dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ace3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if USING_ANALYSIS_VALIDATION_DB:\n",
    "HIGHS_PER_USER = 2\n",
    "MIDS_PER_USER = 3\n",
    "LOWS_PER_USER = 4\n",
    "LABELED_TRIPS_PER_USER = 5\n",
    "N_INCOMPLETES = 2\n",
    "\n",
    "high_list = []\n",
    "mid_list = []\n",
    "low_list = []\n",
    "labeled_list = []\n",
    "incomplete_inference_list = []  # I'm calling an inference incomplete if it has any incomplete tuples. It looks like they all have low confidence\n",
    "incompletes = 0\n",
    "\n",
    "for u in server_filtered_users:\n",
    "    #print(f\"user id: {u}\")\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "\n",
    "    # Reset the counts that keep track of the number of each trip confidence per user\n",
    "    highs = 0\n",
    "    mids = 0\n",
    "    lows = 0\n",
    "    labeled_count = 0\n",
    "\n",
    "    # Get each trip's inference tuples.\n",
    "    # Add to a confidence sum for each label value using the p field of the inference tuples\n",
    "    # For a given label type, choose the highest confidence sum as the confidence value for the label type.\n",
    "    # Choose the confidence of the least confident label type as the trip's confidence.\n",
    "    for j, trip in ct_df.iterrows():\n",
    "\n",
    "        if not is_unlabeled(trip): \n",
    "            if labeled_count < LABELED_TRIPS_PER_USER:    # side note: it looks like only the first user in analysis-validation-db \n",
    "                                                        # has trips that are not \"unlabeled\"\n",
    "                labeled_list.append(ct_df['_id'].iloc[j])\n",
    "            labeled_count += 1\n",
    "            continue    # prevents labeled trips from showing up in the other lists\n",
    "\n",
    "        inference = trip[\"inferred_labels\"]\n",
    "        confidences = {}\n",
    "            \n",
    "        incomplete_inference = False\n",
    "        for label_type in LABEL_CATEGORIES:\n",
    "            label_confidence_sums = {}\n",
    "\n",
    "            for line in inference:\n",
    "                if label_type not in line[\"labels\"]: \n",
    "                    incomplete_inference = True\n",
    "                    continue \n",
    "                label_value = line[\"labels\"][label_type]\n",
    "                if label_value not in label_confidence_sums: \n",
    "                    label_confidence_sums[label_value] = 0\n",
    "                label_confidence_sums[label_value] += line[\"p\"]\n",
    "            confidences[label_type] = max(label_confidence_sums.values()) if len(label_confidence_sums) > 0 else 0    \n",
    "\n",
    "        trip_confidence = min(confidences.values())\n",
    "\n",
    "        # if the inference is incomplete, add to the incomplete list and move to the next trip so it doesn't go in another list.\n",
    "        if incomplete_inference:\n",
    "            if incompletes < N_INCOMPLETES:\n",
    "                incomplete_inference_list.append(ct_df['_id'].iloc[j])\n",
    "                if trip_confidence > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                    conf_type = 'high'\n",
    "                if trip_confidence > LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                    conf_type = 'mid'\n",
    "                else:\n",
    "                    conf_type = 'low'\n",
    "                print(f'Trip confidence for this incomplete inference tuple is: {conf_type}')\n",
    "\n",
    "            incompletes += 1\n",
    "            continue\n",
    "\n",
    "        if trip_confidence > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "            if highs < HIGHS_PER_USER:\n",
    "                high_list.append(ct_df['_id'].iloc[j])\n",
    "            highs += 1            \n",
    "        elif trip_confidence > LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "            if mids < MIDS_PER_USER:\n",
    "                mid_list.append(ct_df['_id'].iloc[j])\n",
    "            mids += 1\n",
    "        \n",
    "        else:\n",
    "            if lows < LOWS_PER_USER:\n",
    "                low_list.append(ct_df['_id'].iloc[j])\n",
    "            lows += 1\n",
    "\n",
    "    print(f'{j} trips for this user. low/mid/high/labeled: {lows}/{mids}/{highs}/{labeled_count}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225225ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numbers of each type of trip that we have. \n",
    "# Compare these with the numbers when we run on the notebook on only the trips in full_list.\n",
    "# add if using analysis validation db:\n",
    "#if USING_ANALYSIS_VALIDATION_DB:\n",
    "full_list = low_list + mid_list + high_list + labeled_list + incomplete_inference_list\n",
    "print(f\"list lengths low/mid/high/labeled/incomplete: {len(low_list)}/{len(mid_list)}/{len(high_list)}/{len(labeled_list)}/{len(incomplete_inference_list)}\")\n",
    "print(f'The incomplete inference trips in mini-db: {incomplete_inference_list}')\n",
    "print(f'length of full list: {len(full_list)}. Number of unlabeled in full list: {len(full_list) - len(labeled_list)}')\n",
    "\n",
    "after_count = {\"user\": ('all,high,mid,low,incomplete,labeled')}\n",
    "after_users = []\n",
    "for u in filter1_users:\n",
    "    lts = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"label_tab_switch\")]\n",
    "    if len(lts) > 0:\n",
    "        after_users.append(u)\n",
    "        lts = lts.copy()\n",
    "        lts.sort_values(\"ts\", ascending=True, inplace=True)\n",
    "        this_after_start = arrow.get(lts.iloc[0][\"ts\"])   \n",
    "        user_after_start[u] = this_after_start\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        ct_sub = ct_df[ct_df['_id'].isin(full_list)]\n",
    "\n",
    "        n_after = ct_sub[(ct_sub[\"end_arrow\"] >= user_after_start[u])].shape[0]\n",
    "        n_high_after = ct_sub[(ct_sub[\"end_arrow\"] >= user_after_start[u]) & ct_sub[\"_id\"].isin(high_list)].shape[0]\n",
    "        n_mid_after = ct_sub[(ct_sub[\"end_arrow\"] >= user_after_start[u]) & ct_sub[\"_id\"].isin(mid_list)].shape[0]\n",
    "        n_low_after = n_after - n_high_after - n_mid_after   # since I know my incomplete inferences are low confidence\n",
    "        n_incomplete_after = ct_sub[(ct_sub[\"end_arrow\"] >= user_after_start[u]) & ct_sub[\"_id\"].isin(incomplete_inference_list)].shape[0]\n",
    "        n_labeled_after = ct_sub[(ct_sub[\"end_arrow\"] >= user_after_start[u]) & ct_sub[\"_id\"].isin(labeled_list)].shape[0]\n",
    "        after_count[u] = [n_after,n_high_after,n_mid_after, n_low_after, n_incomplete_after,n_labeled_after]\n",
    "\n",
    "print(\"Users that will have \\'after\\' trips in the mini-db?\")\n",
    "print(after_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b213b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for u in filter1_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    ct_sub = ct_df[ct_df['_id'].isin(full_list)]\n",
    "\n",
    "    for _,row in ct_sub.iterrows():\n",
    "        if is_unlabeled(row):\n",
    "            i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3626b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if USING_ANALYSIS_VALIDATION_DB:\n",
    "after_count_df = pd.DataFrame(after_count)\n",
    "totals = []\n",
    "\n",
    "for _, row in after_count_df.iterrows():\n",
    "    totals.append(sum(row[1:len(row)]))\n",
    "print('Totals after: All,high,mid,low,incomplete,labeled')\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24beac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a text file or print it\n",
    "# You also need to include Stage_timeseries and Stage_uuids to test properly. \n",
    "# The matching uses Stage_timeseries in get_user_input_for_trip_object(ts, trip_obj, \"manual/\"+label)\n",
    "if USING_ANALYSIS_VALIDATION_DB:\n",
    "    full_list = low_list + mid_list + high_list + labeled_list + incomplete_inference_list\n",
    "    trip_query_file = open(\"trip_query.txt\",\"w\")\n",
    "    trip_query_file.write(\"docker exec analysis-validation-db sh -c 'mongodump --archive --db=Stage_database --collection=Stage_analysis_timeseries --query=\\\"\")\n",
    "    trip_query_file.write(\"{\\\\\\\"_id\\\\\\\":{\\\\\\\"\\\\$in\\\\\\\":[\") \n",
    "\n",
    "    # should look something like:\n",
    "    #query=\"{\\\"_id\\\": {\\\"\\$in\\\": [{\\\"\\$oid\\\":\\\"<objectId>\\\"}]} }\"' > db_testing.dump\n",
    "\n",
    "    for k in range(0,len(full_list)):\n",
    "        if k != len(full_list)-1:\n",
    "            trip_query_file.write(f\"{{\\\\\\\"\\\\$oid\\\\\\\":\\\\\\\"{str(full_list[k])}\\\\\\\"}},\")\n",
    "        else:\n",
    "            trip_query_file.write(f\"{{\\\\\\\"\\\\$oid\\\\\\\":\\\\\\\"{str(full_list[k])}\\\\\\\"}} ] }} }}\\\"\\' > db_trip_queried.dump\")\n",
    "            #trip_query_file.write(f\"ObjectId(\\\\\\\"{str(full_list[k])}\\\\\\\")]}} }}\\\" \\' > db_trip_queried.dump\")\n",
    "    trip_query_file.close()\n",
    "    #print(full_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a1cb6",
   "metadata": {},
   "source": [
    "## Results from Dataset 2!\n",
    "To review, we want:\n",
    " * Number of participants\n",
    " * Frequency of app opens\n",
    " * Taps actually avoided by verify button\n",
    " * Taps actually avoided by hiding high confidence trips\n",
    " * Overall taps avoided (total, per trip, percentage of taps)\n",
    " * Fraction of users who used the verify button\n",
    " * Fraction of trips finalized using the verify button\n",
    " * How much To Label is used vs. other tabs of Label vs. Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ca3e3",
   "metadata": {},
   "source": [
    "### Number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUMBER OF USERS IN DATASET 2: {len(filtered_users)}\")\n",
    "print(\"Breakdown by program: \"+str({k: len([u for u in programs[k] if u in filtered_users]) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288c69f",
   "metadata": {},
   "source": [
    "## Frequency of app opens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequencies(users):\n",
    "    days_before = {}\n",
    "    days_after = {}\n",
    "    opens_before = {}\n",
    "    opens_after = {}\n",
    "    opens_per_day_before = {}\n",
    "    opens_per_day_after = {}\n",
    "    \n",
    "    for u in users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        this_before_start = user_before_start[u]\n",
    "        this_before_end = user_after_start[u]\n",
    "        this_after_start = user_after_start[u]\n",
    "        this_after_end = AFTER_END\n",
    "\n",
    "        days_before[u] = delta2days(this_before_end-this_before_start)\n",
    "        days_after[u] = delta2days(this_after_end-this_after_start)\n",
    "\n",
    "        opens = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] == \"opened_app\")].copy()\n",
    "        opens[\"ts_arrow\"] = opens[\"ts\"].apply(arrow.get)\n",
    "        opens_before[u] = filter_between(opens, \"ts_arrow\", this_before_start, this_before_end).shape[0]\n",
    "        opens_after[u] = filter_between(opens, \"ts_arrow\", this_after_start, this_after_end).shape[0]\n",
    "\n",
    "        opens_per_day_before[u] = opens_before[u]/days_before[u]\n",
    "        opens_per_day_after[u] = opens_after[u]/days_after[u]\n",
    "    \n",
    "    print(\"Everybody in Dataset 2:\")\n",
    "    output_results(users, opens_per_day_before, opens_per_day_after, opens_before, opens_after)\n",
    "    \n",
    "    opens_filtered_users = [u for u in users if opens_before[u] >= REQUIRED_OPENS_BEFORE and opens_after[u] >= REQUIRED_OPENS_AFTER and opens_before[u]+opens_after[u] >= REQUIRED_OPENS_TOTAL]\n",
    "    print(f\"\\nOnly those with >={REQUIRED_OPENS_TOTAL} opens total, >={REQUIRED_OPENS_BEFORE} opens Before, >={REQUIRED_OPENS_AFTER} opens After:\")\n",
    "    output_results(opens_filtered_users, opens_per_day_before, opens_per_day_after, opens_before, opens_after)\n",
    "    \n",
    "def output_results(users, opens_per_day_before, opens_per_day_after, opens_before, opens_after, do_breakdown=True):\n",
    "    opens_per_day_before, opens_per_day_after, opens_before, opens_after = map(lambda d: {k: d[k] for k in d if k in users}, [opens_per_day_before, opens_per_day_after, opens_before, opens_after])\n",
    "    n_users = len(users)\n",
    "    print(\"App opens per day before->after, total opens before+after:\")\n",
    "    print(\"SUM:\")\n",
    "    print(format_arrow_comma(sum(opens_per_day_before.values()), sum(opens_per_day_after.values()), sum(opens_before.values())+sum(opens_after.values())))\n",
    "    print(\"AVERAGE:\")\n",
    "    if n_users > 0:\n",
    "        print(format_arrow_comma(sum(opens_per_day_before.values())/n_users, sum(opens_per_day_after.values())/n_users, (sum(opens_before.values())+sum(opens_after.values()))/n_users))\n",
    "    else: print(\"N/A\")\n",
    "    if not do_breakdown: return\n",
    "    print(\"User breakdown:\")\n",
    "    if n_users > 0:\n",
    "        for u in users:\n",
    "            print(format_arrow_comma(opens_per_day_before[u], opens_per_day_after[u], opens_before[u]+opens_after[u]))\n",
    "    else: print(\"N/A\")\n",
    "    \n",
    "compute_frequencies(filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d87642",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_ANALYSIS_VALIDATION_DB:\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient() \n",
    "    db = client.Stage_database\n",
    "    Stage_timeseries = db.Stage_timeseries\n",
    "\n",
    "    # Make a list of objectIds\n",
    "    # I counted 6 verify fully labels events and 2 label fully labels events\n",
    "    event_list = []\n",
    "    for u in sorted(filtered_users)[:4]:\n",
    "        print('new user')\n",
    "        verify_docs = Stage_timeseries.find({  \n",
    "                    \"$and\": [ {\"metadata.key\":\"stats/client_time\"},{\"user_id\": u},{\"data.name\": \"verify_trip\"}],\n",
    "                    }).limit(3)\n",
    "        for doc in verify_docs:\n",
    "            event_list.append(doc['_id'])\n",
    "            #print(doc['data']['reading'])\n",
    "\n",
    "        label_docs = Stage_timeseries.find({  \n",
    "                \"$and\": [ {\"metadata.key\":\"stats/client_time\"},{\"user_id\": u},{\"data.name\": \"select_label\"}],\n",
    "                }).limit(3)\n",
    "        for doc in label_docs:\n",
    "            event_list.append(doc['_id'])\n",
    "\n",
    "\n",
    "    trip_query_file = open(\"label_and_verify_events_query.txt\",\"w\")\n",
    "    trip_query_file.write(\"docker exec analysis-validation-db sh -c 'mongodump --archive --db=Stage_database --collection=Stage_timeseries --query=\\\"\")\n",
    "    trip_query_file.write(\"{\\\\\\\"_id\\\\\\\":{\\\\\\\"\\\\$in\\\\\\\":[\") \n",
    "\n",
    "    # should look something like:\n",
    "    #query=\"{\\\"_id\\\": {\\\"\\$in\\\": [{\\\"\\$oid\\\":\\\"<objectId>\\\"}]} }\"' > db_testing.dump\n",
    "\n",
    "    for k in range(0,len(event_list)):\n",
    "        if k != len(event_list)-1:\n",
    "            trip_query_file.write(f\"{{\\\\\\\"\\\\$oid\\\\\\\":\\\\\\\"{str(event_list[k])}\\\\\\\"}},\")\n",
    "        else:\n",
    "            trip_query_file.write(f\"{{\\\\\\\"\\\\$oid\\\\\\\":\\\\\\\"{str(event_list[k])}\\\\\\\"}} ] }} }}\\\"\\' > label_and_verify_events.dump\")\n",
    "    trip_query_file.close()\n",
    "\n",
    "    # get the object ids, print them to a file in a query\n",
    "    # Include all confirmed trips in this test db so you can get the correct users in filtered users\n",
    "    # and uuids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b8e2c",
   "metadata": {},
   "source": [
    "### Taps actually avoided by verify button\n",
    "### Taps actually avoided by hiding high confidence trips\n",
    "### Overall taps avoided (total, per trip, percentage of taps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4836fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USING_ANALYSIS_VALIDATION_DB:\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient() \n",
    "    db = client.Stage_database\n",
    "    Stage_timeseries = db.Stage_timeseries\n",
    "\n",
    "# This may take a while -- clocked at between 10s and 30s on an early-2015 MacBook Air\n",
    "# Whether or not a press of the verify button fully labels a trip\n",
    "def verify_fully_labels(event):\n",
    "    if not event[\"reading\"][\"verifiable\"]: return False  # Forgot about this case until working with real pilot program data...\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "    final_inference = json.loads(event[\"reading\"][\"finalInference\"])\n",
    "\n",
    "    # If the user has only labeled some of the labels AND the inference fills in the rest, \n",
    "    # then hitting the verify button fully labeled the trip. We make sets to get only unique inputs.\n",
    "    return len(user_input) < len(LABEL_CATEGORIES) and len(set(user_input.keys()) | set(final_inference.keys())) == len(LABEL_CATEGORIES)\n",
    "\n",
    "# Whether or not a given label dropdown menu selection fully labels a trip\n",
    "def label_fully_labels(event):\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "\n",
    "    # If the trip was not fully labeled before  \n",
    "    # and the label the user is currently inputting is not an input that they have already made, return true\n",
    "    return len(user_input) == len(LABEL_CATEGORIES)-1 and event[\"reading\"][\"inputKey\"] not in event[\"reading\"][\"userInput\"]\n",
    "\n",
    "verifieds = {}\n",
    "taps = {}\n",
    "trips_labeled = {}\n",
    "taps_avoided = {}\n",
    "taps_avoided_per_trip = {}\n",
    "verifiers = set()\n",
    "\n",
    "vevents_total = 0\n",
    "levents_total = 0\n",
    "for u in filtered_users:\n",
    "    trips_labeled[u] = 0\n",
    "    verifieds[u] = 0\n",
    "    # TODO: maybe only consider unlabeled-at-time-of-inference-generation trips here?\n",
    "    # below is basically stats[time][user id is u & the event is verify trip]\n",
    "\n",
    "    # if is_unlabeled() \n",
    "\n",
    "    verify_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"verify_trip\")]\n",
    "    vevents_total += len(verify_events)\n",
    "    if len(verify_events) > 0: verifiers.add(u)\n",
    "    label_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"select_label\")]\n",
    "    levents_total += len(label_events)\n",
    "    taps[u] = len(verify_events)+2*len(label_events)\n",
    "    # The testing user seems to have an unusually high number of mistaps. When crunching real data, we will let this count against taps saved,\n",
    "    # but to have useful data to debug the sensitivity analysis that appears later, let's artificially assume that 1 in 10 label_events is a mistap and ignore those.\n",
    "    if TESTINGMODE: taps[u] = len(verify_events)+(1-0.10)*(2*len(label_events))\n",
    "    if verify_events.shape[0] > 0:\n",
    "        for _, ve in verify_events.iterrows():\n",
    "            if verify_fully_labels(ve):\n",
    "                verifieds[u] += 1\n",
    "                trips_labeled[u] += 1\n",
    "    if label_events.shape[0] > 0:\n",
    "        for _, le in label_events.iterrows():\n",
    "            if label_fully_labels(le): trips_labeled[u] += 1\n",
    "    taps_avoided[u] = OLD_TAPS*trips_labeled[u]-taps[u]\n",
    "    taps_avoided_per_trip[u] = taps_avoided[u]/trips_labeled[u] if trips_labeled[u] != 0 else float(\"NaN\")\n",
    "    # print(f\"User tapped {taps[u]} times, avoided {taps_avoided[u]} taps to label {trips_labeled[u]} trips\")\n",
    "\n",
    "    if USING_ANALYSIS_VALIDATION_DB:\n",
    "        # Use pymongo to get the number of verify and label events for the current user\n",
    "        # Check that the numbers match those found above\n",
    "        # find returns a cursor object that has to be iterated through to use\n",
    "        ver_events_mongo = Stage_timeseries.find({    \n",
    "            \"$and\": [ {\"metadata.key\":\"stats/client_time\"},{\"user_id\": u},{\"data.name\": \"verify_trip\"}]\n",
    "            })\n",
    "\n",
    "        verify_count = 0\n",
    "        mongo_trips_verified = 0\n",
    "        mongo_trips_labeled = 0\n",
    "        for doc in ver_events_mongo:\n",
    "            verify_count += 1\n",
    "            event = doc['data']\n",
    "\n",
    "            # Reimplements verify_fully_labels\n",
    "            if not event['reading']['verifiable']: continue\n",
    "            user_input_m= json.loads(event['reading']['userInput'])  # m for mongo\n",
    "            final_inference_m = json.loads(event['reading']['finalInference'])  \n",
    "            if len(user_input_m) < len(LABEL_CATEGORIES) and len(set(user_input_m.keys()) | set(final_inference_m.keys())) == len(LABEL_CATEGORIES):\n",
    "                mongo_trips_verified += 1\n",
    "                mongo_trips_labeled += 1\n",
    "        \n",
    "        label_events_mongo = Stage_timeseries.find({\n",
    "            \"$and\": [ {\"metadata.key\":\"stats/client_time\"},{\"user_id\": u},{\"data.name\": \"select_label\"}]\n",
    "            })\n",
    "        \n",
    "        label_count = 0\n",
    "        for doc in label_events_mongo:\n",
    "            label_count += 1\n",
    "            event = doc['data']\n",
    "\n",
    "            # Reimplements label_fully_labels\n",
    "            user_input_m = json.loads(event[\"reading\"][\"userInput\"])\n",
    "            if len(user_input_m) == len(LABEL_CATEGORIES)-1 and event[\"reading\"][\"inputKey\"] not in event[\"reading\"][\"userInput\"]:\n",
    "                mongo_trips_labeled += 1\n",
    "\n",
    "        assert verify_count == len(verify_events)\n",
    "        assert label_count == len(label_events)\n",
    "        assert mongo_trips_verified == verifieds[u]\n",
    "        assert mongo_trips_labeled == trips_labeled[u]\n",
    "\n",
    "if USING_LABEL_TEST_DB:\n",
    "    assert sum(trips_labeled.values()) == 8\n",
    "    assert sum(verifieds.values()) == 6\n",
    "\n",
    "def print_tap_summary(users):\n",
    "    total_taps = sum(fu(taps, users).values())\n",
    "    total_taps_avoided = sum(fu(taps_avoided, users).values()) \n",
    "    total_trips_labeled = sum(fu(trips_labeled, users).values())\n",
    "    avoided_per_labeled = total_taps_avoided/total_trips_labeled\n",
    "    print(f\"Overal, users tapped {total_taps} times to label {total_trips_labeled} trips.\")\n",
    "    print(f\"Overall, {total_taps_avoided} taps were avoided, {avoided_per_labeled:.2f} per trip -- that's {avoided_per_labeled/OLD_TAPS:.2%} of taps\")\n",
    "    print(f\"We also saved {OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled, users).values())} taps across {sum(fu(g_high_confidence_n_after_unlabeled).values())} trips by not soliciting user input on very confident trips\")\n",
    "\n",
    "    total_taps_avoided_high = total_taps_avoided+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled, users).values())\n",
    "    total_trips_labeled_high = total_trips_labeled+sum(fu(g_high_confidence_n_after_unlabeled, users).values())\n",
    "    avoided_per_labeled_high = total_taps_avoided_high/total_trips_labeled_high\n",
    "    print(f\"If we also count the taps we avoided by not putting high-confidence inferences on the To Label screen:\")\n",
    "    print(f\"Overall, {total_taps_avoided_high} taps were avoided across {total_trips_labeled+sum(fu(g_high_confidence_n_after_unlabeled, users).values())} trips, {avoided_per_labeled_high:.2f} per trip -- that's {avoided_per_labeled_high/OLD_TAPS:.2%} of taps\")\n",
    "\n",
    "print_tap_summary(filtered_users)\n",
    "print(f\"number of verify,label events: {vevents_total},{levents_total}\")  # should be 495, 1022 for analysis-validation-db \n",
    "# (in mongo: db.Stage_timeseries.count({\"data.name\":\"verify_trip\"}),  db.Stage_timeseries.count({\"data.name\":\"select_label\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sense of the 'fully labels' functions\n",
    "event = stats['time'][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"verify_trip\")].iloc[30]\n",
    "user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "final_inference = json.loads(event[\"reading\"][\"finalInference\"])\n",
    "\n",
    "user_input\n",
    "#len(set(user_input.keys()) | set(final_inference.keys())) == 3\n",
    "    #final_inference = json.loads(event[\"reading\"][\"finalInference\"])\n",
    "    #return len(user_input) < len(LABEL_CATEGORIES) and len(set(user_input.keys()) | set(final_inference.keys())) == len(LABEL_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['time'][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"select_label\")].iloc[20]\n",
    "#user_input = json.loads(event[\"reading\"][\"userInput\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36725c58",
   "metadata": {},
   "source": [
    "### Fraction of users who used the verify button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566edd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each program, prints number of filtered users who used the verify button over total number of filtered users\n",
    "print(\"Users who have used the verify button at least once: \"+str({k: str(len([u for u in programs[k] if u in filtered_users and u in verifiers]))+\"/\"+str(len([u for u in programs[k] if u in filtered_users])) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e3b17",
   "metadata": {},
   "source": [
    "### Fraction of trips finalized using the verify button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a213f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of trips finalized using the verify button as a fraction of total number of user-confirmed trips:\\n\"+str({k: format_frac_percent(sum([verifieds[u] for u in programs[k] if u in filtered_users]), sum([trips_labeled[u] for u in programs[k] if u in filtered_users])) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c37d7f",
   "metadata": {},
   "source": [
    "### How much To Label is used vs. other tabs of Label vs. Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fa88f",
   "metadata": {},
   "source": [
    "First, let's (re)aquaint ourselves with what the instrumentation data can provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_instrumentation(u):\n",
    "    print(stats.keys())\n",
    "\n",
    "    nav_stats = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] != \"sync_launched\")].copy()\n",
    "    # print(nav_stats.shape)\n",
    "    # print(nav_stats.keys())\n",
    "    # print(nav_stats.head()[[\"name\",\"reading\",\"ts\"]])\n",
    "    # print(nav_stats[[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "\n",
    "    time_stats = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] != \"push_duration\") & (stats[\"time\"][\"name\"] != \"pull_duration\") & (stats[\"time\"][\"name\"] != \"sync_duration\")].copy()\n",
    "    # print(time_stats.shape)\n",
    "    # print(time_stats.keys())\n",
    "    # print(time_stats.head()[[\"name\",\"reading\",\"ts\"]])\n",
    "    # print(time_stats[[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "    # print(time_stats[time_stats[\"name\"] == \"label_tab_switch\"][[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "\n",
    "\n",
    "explore_instrumentation(filtered_users[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f15154",
   "metadata": {},
   "source": [
    "Based on the results of this brief investigation, it seems that it would be hard to measure time spent on To Label vs. the other screens because the stats I have don't seem to be accurately monitoring when the app stops being used. This should be revisited in the future, though -- there are probably some other stats elsewhere that could help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407e59f",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b28d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(logging.WARNING)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7431a",
   "metadata": {},
   "source": [
    "### Weekly labeling percentage over time for each program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 18s on an early-2015 MacBook Air\n",
    "# This cell rather clumsily calculates per-week labeling statistics for each user and each program.\n",
    "# TODO could use some major refactoring\n",
    "\n",
    "def filter_before_or_to_label(df, user):\n",
    "    # empty = (df[\"end_arrow\"] < user_after_start[user]) & (df[\"end_arrow\"] > user_after_start[user])\n",
    "    # (df[\"end_arrow\"] < user_after_start[user])\n",
    "    if user not in filter2_users: return df  # Everything is before update if you haven't installed the update! TODO filter2_users was not meant to be referred to so very globally\n",
    "\n",
    "    # Get the data before the after start for this user or where the expectation is to_label?\n",
    "    return df[(df[\"end_arrow\"] < user_after_start[user]) | (df[\"expectation\"].apply(lambda val: \"to_label\" in val and val[\"to_label\"]))]\n",
    "\n",
    "# This could almost certainly be done more efficiently, but it's not worth worrying about right now\n",
    "def count_weekly(users):\n",
    "    total_count_weekly = {}\n",
    "    labeled_count_weekly = {}\n",
    "    labeling_frac_weekly = {}\n",
    "    total_count_weekly_u = {}\n",
    "    labeled_count_weekly_u = {}\n",
    "    labeling_frac_weekly_u = {}\n",
    "    for u in users:\n",
    "        total_count_weekly_u[u] = {}\n",
    "        labeled_count_weekly_u[u] = {}\n",
    "        labeling_frac_weekly_u[u] = {}\n",
    "    for program in programs.keys():\n",
    "        total_count_weekly[program] = {}\n",
    "        labeled_count_weekly[program] = {}\n",
    "        labeling_frac_weekly[program] = {}\n",
    "        for week in weeks:\n",
    "            this_total_count = 0\n",
    "            this_labeled_count = 0\n",
    "            for u in users:\n",
    "                if u in programs[program]:\n",
    "                    if programs == \"stage\": print(u)\n",
    "                    total = filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week)\n",
    "                    total = filter_before_or_to_label(total, u)\n",
    "                    fully_labeled = total[(total[\"user_input\"].apply(len) == len(LABEL_CATEGORIES))]    \n",
    "                        # does this account for the cases where the user used the verify button? does user_input get populated after that?\n",
    "                    this_total_count += total.shape[0]\n",
    "                    this_labeled_count += fully_labeled.shape[0]\n",
    "\n",
    "                    total_count_weekly_u[u][week] = total.shape[0]\n",
    "                    labeled_count_weekly_u[u][week] = fully_labeled.shape[0]\n",
    "                    labeling_frac_weekly_u[u][week] = labeled_count_weekly_u[u][week]/total_count_weekly_u[u][week] if total_count_weekly_u[u][week] != 0 else float(\"nan\")\n",
    "            total_count_weekly[program][week] = this_total_count\n",
    "            labeled_count_weekly[program][week] = this_labeled_count\n",
    "            labeling_frac_weekly[program][week] = this_labeled_count/this_total_count if this_total_count != 0 else float(\"nan\")\n",
    "    return total_count_weekly, labeled_count_weekly, labeling_frac_weekly, total_count_weekly_u, labeled_count_weekly_u, labeling_frac_weekly_u\n",
    "\n",
    "total_count_weekly, labeled_count_weekly, labeling_frac_weekly, _, _, _ = count_weekly(filtered_users)\n",
    "total_count_weekly_ds1, labeled_count_weekly_ds1, labeling_frac_weekly_ds1, total_count_weekly_u, labeled_count_weekly_u, labeling_frac_weekly_u = count_weekly(server_filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b73c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient() \n",
    "db = client.Stage_database\n",
    "\n",
    "sat = db.Stage_analysis_timeseries  ## how do I get certain times?\n",
    "# maybe to get data from certain weeks I could reuse Gabriel's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c54707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat.find_one({\"metadata.key\":\"analysis/confirmed_trip\"})['data'].keys()  # we want end_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about this: figure out the weeks you need, then for each trip, if the end_arrow is in the week you want, add the object Id to a list\n",
    "weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff026f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some vail trips\n",
    "# Let's do week indices 16, 17, and 22\n",
    "for u in sorted(programs['only_after'])[:2]:\n",
    "    i = 0\n",
    "    have_started = False\n",
    "    for week in weeks:\n",
    "        total = filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week)\n",
    "        total = filter_before_or_to_label(total, u)\n",
    "        if (total.shape[0] > 0):\n",
    "            have_started = True\n",
    "            print(f'week index: {i} nrows: {total.shape[0]}')\n",
    "\n",
    "        # Save some trip object ids\n",
    "        if have_started:\n",
    "            print(total['_id'].iloc[0])\n",
    "            print(f\"length of user input for this trip: {total['user_input'].apply(len).iloc[0]}\")\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for a before_after site. I picked\n",
    "for u in sorted(programs['before_after'])[:2]:\n",
    "    i = 0\n",
    "    have_started = False\n",
    "    for week in weeks:\n",
    "        total = filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week)\n",
    "        total = filter_before_or_to_label(total, u)\n",
    "        if (total.shape[0] > 0):\n",
    "            have_started = True\n",
    "            print(f'week index: {i} nrows: {total.shape[0]}')\n",
    "\n",
    "        # Save some trip object ids\n",
    "        if have_started:\n",
    "            print(total['_id'].iloc[0])\n",
    "            print(f\"length of user input for this trip: {total['user_input'].apply(len).iloc[0]}\")\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a':[1,2,3],'b': [4,5,6]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc01860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of our users have installed the update at the end of a given week\n",
    "def count_users_weekly(users):\n",
    "    updated_users_weekly = {}\n",
    "    denom = {}\n",
    "    for program in programs:\n",
    "        updated_users_weekly[program] = {}\n",
    "        denom[program] = {}\n",
    "        for week in weeks:\n",
    "            updated_users_weekly[program][week] = 0\n",
    "            denom[program][week] = 0\n",
    "            for u in users:\n",
    "                if u in programs[program]:\n",
    "                    if filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week).shape[0] > 0:\n",
    "                        denom[program][week] += 1\n",
    "                        if u in user_after_start and user_after_start[u] < week[1]: updated_users_weekly[program][week] += 1\n",
    "        \n",
    "        for week in weeks:\n",
    "            n = updated_users_weekly[program][week]\n",
    "            # labeled = labeling_frac_weekly[program][week] #NaN-ify points where there were no labeled trips\n",
    "            updated_users_weekly[program][week] = n/denom[program][week] if denom[program][week] != 0 else float(\"NaN\")\n",
    "    return updated_users_weekly\n",
    "\n",
    "updated_users_weekly = count_users_weekly(filtered_users)\n",
    "updated_users_weekly_ds1 = count_users_weekly(server_filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of days users have used the app per week\n",
    "def count_days_per_week(users):\n",
    "    days_per_week_u = {u: {} for u in users}\n",
    "    for u in users:\n",
    "        time_stats = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] != \"push_duration\") & (stats[\"time\"][\"name\"] != \"pull_duration\") & (stats[\"time\"][\"name\"] != \"sync_duration\")]\n",
    "        nav_stats = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] != \"sync_launched\")].copy()\n",
    "        # print(pd.unique(nav_stats[\"name\"]))\n",
    "        switches = nav_stats[nav_stats[\"name\"] == \"opened_app\"].copy()\n",
    "        switches[\"ts_arrow\"] = switches[\"ts\"].apply(lambda ts: arrow.get(ts).to(MY_TZ))\n",
    "        for week in weeks:\n",
    "            these_switches = filter_between(switches, \"ts_arrow\", *week)\n",
    "            days_per_week_u[u][week] = pd.unique(these_switches[\"ts_arrow\"].apply(lambda ts: (ts-BEFORE_START).days)).shape[0]\n",
    "        # print(switches[\"ts_arrow\"])\n",
    "    return days_per_week_u\n",
    "\n",
    "days_per_week_u = count_days_per_week(server_filtered_users)\n",
    "# days_per_week_u = count_days_per_week([filtered_users[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5818370",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_labels = [week[0].datetime for week in weeks]  # f\"{week[0].month}/{week[0].day}\"\n",
    "\n",
    "def setup_weekly_axis(ax):\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m/%d\"))\n",
    "    ax.set_xticks(week_labels)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    plt.setp(ax.get_xticklabels(), visible=True)\n",
    "    ax.tick_params(labelbottom=True)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(15,8), gridspec_kw = {\"height_ratios\": [1, 3]}, sharex=True)\n",
    "for ax in axs:\n",
    "    setup_weekly_axis(ax)\n",
    "\n",
    "for program in programs_some:\n",
    "    n = len([u for u in programs[program] if u in filtered_users])\n",
    "    if n == 0: continue\n",
    "    label = f\"{program if program != 'ens' else 'ensemble'} (n={n})\"\n",
    "    lines = []\n",
    "    lines.append(axs[1].plot(week_labels, list(labeling_frac_weekly[program].values()), label=label, zorder = 1 if program == \"ens\" else 0)[0])\n",
    "    lines.append(axs[0].plot(week_labels, list(updated_users_weekly[program].values()), label=label, zorder = 1 if program == \"ens\" else 0)[0])\n",
    "    for line in lines:\n",
    "        if program == \"ens\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "\n",
    "axs[0].set_ylabel(\"% installed update\")\n",
    "axs[1].set_ylabel(\"% of expected trips labeled\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0892ca2",
   "metadata": {},
   "source": [
    "So that's not entirely encouraging, but it's also kind of hard to tell what's going on. Let's try something else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77d620",
   "metadata": {},
   "source": [
    "### Weekly labeling but it's Vail and Pueblo County vs. all the others\n",
    "(and we skip the first week and only do four weeks after that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07475b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the weeks we care about. TODO make this not dependent on starting date!!!\n",
    "first_start = 7\n",
    "first_end = 11\n",
    "second_start = 19\n",
    "second_end = 23\n",
    "for i in (first_start, first_end, second_start, second_end): print(weeks[i])\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(15,8), gridspec_kw = {\"height_ratios\": [1, 3]}, sharex=True)\n",
    "for ax in axs:\n",
    "    setup_weekly_axis(ax)\n",
    "\n",
    "for program in programs_all:\n",
    "    if program == \"ens\": continue\n",
    "    n = len([u for u in programs[program] if u in server_filtered_users])\n",
    "    if n == 0: continue\n",
    "\n",
    "    if program == \"before_after\": label = \"Before/after ensemble\"\n",
    "    elif program == \"only_after\": label = \"Only after ensemble\"\n",
    "    else: label = program\n",
    "    label += f\" n={n}\"\n",
    "\n",
    "    y1 = list(labeling_frac_weekly_ds1[program].values())\n",
    "    y2 = list(updated_users_weekly_ds1[program].values())\n",
    "    not_in_first = lambda x: x < first_start or x >= first_end\n",
    "    not_in_second = lambda x: x < second_start or x >= second_end\n",
    "    for i in range(len(weeks)):  # NaN out the data we don't care about\n",
    "        if program in [\"only_after\", *only_after_programs]:\n",
    "            if not_in_second(i): y1[i] = y2[i] = float(\"NaN\")\n",
    "        else:\n",
    "            if not_in_first(i) and not_in_second(i): y1[i] = y2[i] = float(\"NaN\")\n",
    "    lines = []\n",
    "    lines.append(axs[1].plot(week_labels, y1, label=label, zorder = 1 if program in (\"before_after\", \"only_after\") else 0)[0])\n",
    "    lines.append(axs[0].plot(week_labels, y2, label=label, zorder = 1 if program in (\"before_after\", \"only_after\") else 0)[0])\n",
    "    for line in lines:\n",
    "        if program == \"before_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "        elif program == \"only_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "            line.set_linestyle(\"dashed\")\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"% installed update\")\n",
    "axs[1].set_ylabel(\"% of expected trips labeled\")\n",
    "\n",
    "axs[1].legend(loc=\"center\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa3358",
   "metadata": {},
   "source": [
    "Let's do a second version of that with three sections:\n",
    "  1. Before/after group when they've just started (no update)\n",
    "  2. Before/after group just after they install the update (meaning calculate an offset for each user) (100% update)\n",
    "  3. Only after group when they've just started (100% update)\n",
    "\n",
    "and then I suppose we could have a fourth plot with the three ensembles from that.\n",
    "\n",
    "Then, let's use the same type of graph to figure out whether the update has any effect on labeling cadence. If so, that might be a sign of reduction (or increase) in burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26778ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_of_before_start = {}  # Index of week that user installed the app to begin with\n",
    "for u in server_filtered_users:\n",
    "    for i, week in enumerate(weeks):\n",
    "        if user_before_start[u] >= week[0] and user_before_start[u] < week[1]:\n",
    "            week_of_before_start[u] = i\n",
    "            break\n",
    "\n",
    "week_of_start = {}  # Index of week that user installed the update\n",
    "for u in user_after_start:\n",
    "    for i, week in enumerate(weeks):\n",
    "        if user_after_start[u] >= week[0] and user_after_start[u] < week[1]:\n",
    "            assert u not in week_of_start\n",
    "            week_of_start[u] = i\n",
    "# print(week_of_start)\n",
    "\n",
    "def weekly_average_per_user_offset(tseries, users, start_func, n_weeks, valid_func = lambda user, week: True):\n",
    "    augmented_weeks = weeks+[None]*n_weeks  # Account for overflow\n",
    "    per_user = []  # This could all be written as a massive comprehension; wouldn't that be fun.\n",
    "    for user in users:\n",
    "        start = start_func(user)\n",
    "        user_weeks = [augmented_weeks[i] if valid_func(user, i) else None for i in range(start, start+n_weeks)]\n",
    "        user_results = [tseries[user][week] if week != None else float(\"nan\") for week in user_weeks]\n",
    "        assert len(user_results) == n_weeks, len(user_results)\n",
    "        per_user.append(user_results)\n",
    "    return np.nanmean(per_user, axis=0)\n",
    "\n",
    "# print(weekly_average_per_user_offset(labeling_frac_weekly_u, filtered_users, lambda u: 5, 4))\n",
    "\n",
    "def style_lines(program, lines):\n",
    "    for line in lines:\n",
    "        if program == \"before_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "        elif program == \"only_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "            # line.set_linestyle(\"dashed\")\n",
    "\n",
    "def plot_labeling_breakdown_v2(tseries, y_percent):\n",
    "    labels = {program: \"ensemble\" if program == \"before_after\" else \"ensemble\" if program == \"only_after\" else program for program in programs_all}\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(22,8))\n",
    "    if y_percent: plt.subplots_adjust(wspace=0.3)\n",
    "    x = [1.5, 2.5, 3.5, 4.5]\n",
    "    axs[0].set_title(\"Before/after group at initial installation\")\n",
    "    axs[1].set_title(\"Before/after group at updating\")\n",
    "    axs[2].set_title(\"Only after group at installation\")\n",
    "    axs[3].set_title(\"Inter-scenario comparison\")\n",
    "    for ax in axs:\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "        ax.set_xlabel(\"Weeks after event\")\n",
    "        ax.set_xlim([1, 5])\n",
    "        if (y_percent):\n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.set_ylabel(\"% of expected trips labeled\")\n",
    "        else:\n",
    "            ax.set_ylim([0, 7])\n",
    "            ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "            ax.set_ylabel(\"Number of days per week users open app\")\n",
    "    \n",
    "    ensembles = []\n",
    "\n",
    "    # Actual plotting #1\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program in [\"ens\", \"stage\", \"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in server_filtered_users and u in tseries and u in week_of_before_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No before users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_before_start[u], 4, valid_func = lambda u, week: u not in week_of_start or week < week_of_start[u])\n",
    "        if program == \"before_after\": ensembles.append(y)\n",
    "        lines.append(axs[0].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "    \n",
    "    # Actual plotting #2\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program in [\"ens\", \"stage\", \"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in week_of_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No after users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_start[u], 4)\n",
    "        if program == \"before_after\": ensembles.append(y)\n",
    "        lines.append(axs[1].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "\n",
    "    # Actual plotting #3\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program not in [\"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in week_of_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No after users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_start[u], 4)\n",
    "        if program == \"only_after\": ensembles.append(y)\n",
    "        lines.append(axs[2].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "    \n",
    "    # Actual plotting #4\n",
    "    for i in range(len(ensembles)):\n",
    "        axs[3].plot(x, ensembles[i], label=axs[i].get_title(), linewidth=3)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.legend(loc = (\"lower left\" if y_percent else \"best\"))\n",
    "    \n",
    "plot_labeling_breakdown_v2(labeling_frac_weekly_u, True)\n",
    "plot_labeling_breakdown_v2(days_per_week_u, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b53911",
   "metadata": {},
   "source": [
    "Let's do a histogram of expectation confidences. To usefully display the data, we will have to constrain the y-axis such that some bars (e.g., the first) cannot fully display, so we annotate the graph with information about these bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96820736",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 100\n",
    "# y_step = 250\n",
    "# allowed_overflow = 1\n",
    "max_y = 1700  # TODO determine this algorithmically (for now, tune it manually per dataset)\n",
    "if TESTINGMODE: max_y = 20\n",
    "\n",
    "if len(all_high_confidence_n_after_unlabeled['ens']) >0:\n",
    "    all_confidences = complete_results[\"ens\"][\"all_confidences\"].copy()\n",
    "else:   # if there's nothing in ensemble, we are probably working with stage data only\n",
    "   all_confidences = complete_results[\"stage\"][\"all_confidences\"].copy()\n",
    "\n",
    "\n",
    "all_confidences.sort()\n",
    "# from collections import Counter\n",
    "# conf_count = Counter(all_confidences)\n",
    "# print(sorted(conf_count.values(), reverse=True)[:4])\n",
    "# max_y = sorted(conf_count.values(), reverse=True)[allowed_overflow]  # fails to take into account binning\n",
    "# print(max_y)\n",
    "# max_y = np.ceil(max_y/y_step)*y_step\n",
    "# print(max_y)\n",
    "\n",
    "bins = np.arange(0, 1+0.1/n_bins, 1/n_bins)\n",
    "assert len(bins) == n_bins+1\n",
    "\n",
    "range1 = [x for x in all_confidences if x <= LOW_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "range2 = [x for x in all_confidences if x > LOW_CONFIDENCE_THRESHOLD_PRODUCTION and x <= HIGH_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "range3 = [x for x in all_confidences if x > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,8))\n",
    "p1 = plt.hist(range1, bins=bins, label=f\"User input required, 1 or more red labels (total of {len(range1)} trips)\")\n",
    "p2 = plt.hist(range2, bins=bins, label=f\"User input required, all yellow labels (total of {len(range2)} trips)\")\n",
    "p3 = plt.hist(range3, bins=bins, label=f\"No user input required (total of {len(range3)} trips)\")\n",
    "ax.set_ylim([0, max_y])\n",
    "ax.set_xlim([0, 1])\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(250))\n",
    "if TESTINGMODE: ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "\n",
    "ax.set_title(\"Histogram of trip confidence, segmented by presentation to user\")\n",
    "ax.set_ylabel(\"Number of trips\")\n",
    "ax.set_xlabel(\"Confidence level of final inference\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Mark overflows on the graph\n",
    "for p in (p1, p2, p3):\n",
    "    for i, bar in enumerate(zip(*p)):\n",
    "        rect = bar[2]\n",
    "        height = int(rect.get_height())\n",
    "        if height > max_y:\n",
    "            ax.text(rect.get_x()+rect.get_width()*1.1, max_y*0.99, f\"⬆{height}\", ha=\"left\", va=\"top\")\n",
    "            print(f\"Overflow note: the bar for the region [{bins[i]:.1%}, {bins[i+1]:.1%}{']' if i == len(bins)-1 else ')'} contains {height} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e963c",
   "metadata": {},
   "source": [
    "Debugging break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d162f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was a bug in the confidence algorithm; I used this code to figure it out.\n",
    "def explore_confidences(u):\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    print(ct_df.shape)\n",
    "    # \"\"\"\n",
    "    for i, trip in ct_df.iterrows():\n",
    "        if i != 179: continue\n",
    "        inference = trip[\"inferred_labels\"]\n",
    "        if len(inference) > 0:\n",
    "            for option in inference:\n",
    "                print(f\"{len(option['labels'])}, {option['p']:.2f}\", end=\"; \")\n",
    "            print(i)\n",
    "\n",
    "        confidences = {}\n",
    "        for label_type in LABEL_CATEGORIES:\n",
    "            print(label_type)\n",
    "            counter = {}\n",
    "            for line in inference:\n",
    "                if label_type not in line[\"labels\"]: continue\n",
    "                val = line[\"labels\"][label_type]\n",
    "                if val not in counter: counter[val] = 0\n",
    "                counter[val] += line[\"p\"]\n",
    "            print(counter)\n",
    "            confidences[label_type] = max(counter.values())  # THIS WAS SUM BEFORE!!! THAT WAS THE PROBLEM!!!\n",
    "        print(\"CONFIDENCES:\")\n",
    "        print(confidences)\n",
    "        trip_confidence = min(confidences.values())\n",
    "        print(trip_confidence)\n",
    "    # \"\"\"\n",
    "    # print(ct_df.iloc[338][\"inferred_labels\"])\n",
    "    print(ct_df.iloc[179][\"inferred_labels\"])\n",
    "\n",
    "# explore_confidences(filtered_users[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4a4be",
   "metadata": {},
   "source": [
    "Let's do the weekly labeling thing but per-user, and let's line things up so everyone installs the update at the same point on the graph. We do this first as a scatterplot; then, we see if it might be clearer as a bar graph displaying three-week averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apologies for the rather awful code to follow. TODO at some point this should be cleaned up.\n",
    "aligned_labeling_frac_u = {}\n",
    "for u in week_of_start.keys():  # It's possible not everyone in filtered_users has a week_of_start within the weeks we're graphing, in which case drop that user\n",
    "    aligned_labeling_frac_u[u] = {}\n",
    "    for i, week in enumerate(weeks):\n",
    "        aligned_labeling_frac_u[u][i-week_of_start[u]] = labeling_frac_weekly_u[u][week]\n",
    "# aligned_labeling_frac_u = {1: {-8: 0, 0: 0}, 2: {-6: 0, 0: 0}, 3: {0: 0, 0: 0}, 4: {3: 0, 0: 0}, 5: {5: 0, 0: 0}, 6: {7: 0, 0: 0}}\n",
    "# min_offset = min([min(a.keys()) for a in aligned_labeling_frac_u.values()])\n",
    "# max_offset = max([max(a.keys()) for a in aligned_labeling_frac_u.values()])\n",
    "min_offset, max_offset = map(lambda f: (f([f(a.keys()) for a in aligned_labeling_frac_u.values()]) if len(aligned_labeling_frac_u) > 0 else float(\"nan\")), (min, max))\n",
    "min_offset, max_offset = (lambda x: (-x,x))(min(-min_offset, max_offset))  # fun fun\n",
    "x = np.array(range(min_offset, max_offset+1)) if len(aligned_labeling_frac_u) > 0 else []\n",
    "\n",
    "wk_u_y = {u: [aligned_labeling_frac_u[u][i] if i in aligned_labeling_frac_u[u] else float(\"nan\") for i in x] for u in week_of_start.keys()}\n",
    "wk_ens_u = [np.nanmean([wk_u_y[u][i] for u in week_of_start.keys()]) for i in range(len(x))]\n",
    "# print(wk_ens_u[x.index(3)])\n",
    "# print(wk_u_y[server_filtered_users[5]])\n",
    "# print([wk_u_y[u][x.index(3)] for u in week_of_start.keys()])\n",
    "\n",
    "def plot_weekly_per_user_line():\n",
    "    fig, ax = plt.subplots(1, figsize=(15,8))\n",
    "    ax.set_xlim([min_offset, max_offset])\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    for u in week_of_start.keys():\n",
    "        ax.scatter(x, wk_u_y[u])\n",
    "        ax.plot(x, wk_u_y[u], alpha=0.25)\n",
    "\n",
    "    line = ax.plot(x, wk_ens_u)[0]\n",
    "    line.set_color(\"black\")\n",
    "    line.set_linewidth(3)\n",
    "\n",
    "def plot_weekly_per_user_bar():\n",
    "    x2 = np.array([-9, -6, -3, 0, 3, 6, 9])\n",
    "    y2 = {u: [] for u in week_of_start.keys()}\n",
    "    for i in range(len(x2)-1):\n",
    "        xstart = list(x).index(x2[i])\n",
    "        xend = list(x).index(x2[i+1])  # If you are reading this code, my condolences\n",
    "        for u in week_of_start.keys():\n",
    "            # print(wk_u_y[u][xstart:xend])\n",
    "            y2[u].append(np.mean(wk_u_y[u][xstart:xend]))\n",
    "    x2 = x2[:-1]\n",
    "    # print(y2[server_filtered_users[5]])\n",
    "\n",
    "    width = 3/len(week_of_start.keys())*0.9\n",
    "    fig, ax = plt.subplots(1, figsize=(20,8))\n",
    "    ax.set_xlim([-9, 9])\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(3))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.xaxis.grid(True, color=\"black\", linewidth=2, linestyle=\"dotted\")\n",
    "\n",
    "    for i, u in enumerate(week_of_start.keys()):\n",
    "        ax.bar(x2+width*i, y2[u], width)\n",
    "\n",
    "    ax.plot(x, wk_ens_u, color=\"black\", linewidth=3)\n",
    "\n",
    "def plot_weekly_per_user():\n",
    "    if TESTINGMODE: return  # This just isn't going to work in TESTINGMODE\n",
    "    plot_weekly_per_user_line()\n",
    "    plot_weekly_per_user_bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fce40f",
   "metadata": {},
   "source": [
    "Let's construct an infographic visualizing how we eliminated taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stacked_bars(title, labels, scenarios, max_taps, width=6, hook=None):\n",
    "    user_burden = [max_taps-scenario[0]-scenario[1] for scenario in scenarios]\n",
    "    due_to_confirm = [scenario[0] for scenario in scenarios]\n",
    "    due_to_expectations = [scenario[1] for scenario in scenarios]\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(width,6))\n",
    "    p1 = ax.bar(labels, user_burden, bottom = list(map(lambda x: x[0]+x[1], zip(due_to_expectations, due_to_confirm))), label = \"User burden\")\n",
    "    p2 = ax.bar(labels, due_to_confirm, bottom = due_to_expectations, label = \"Eliminated due to confirm button\")\n",
    "    p3 = ax.bar(labels, due_to_expectations, label = \"Eliminated due to expectations\")\n",
    "    # Label user burden and all nonzero eliminations\n",
    "    rects_to_label = list(p1)+[r for i, r in enumerate(p2) if due_to_confirm[i] != 0]+[r for i, r in enumerate(p3) if due_to_expectations[i] != 0]\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2, rect.get_y()+rect.get_height()/2, f\"{height:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    ax.legend()  # loc=\"upper left\"\n",
    "    ax.set_ylabel(\"Average taps per user-labeled or confidently auto-labeled trip\")\n",
    "    ax.set_title(title)\n",
    "    if hook is not None: hook(ax)\n",
    "    print()\n",
    "\n",
    "#### added old taps to stacked_denom\n",
    "stacked_denom = OLD_TAPS*(sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "saved_due_to_confirm = sum(fu(taps_avoided).values())/stacked_denom  # Usually we take the denominator for this number to be only trips in To Label, but here it has to be all user- or confidently auto-labeled trips\n",
    "saved_due_to_expectations = (OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/stacked_denom\n",
    "np.testing.assert_almost_equal(saved_due_to_confirm+saved_due_to_expectations, ((sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(OLD_TAPS*(sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values())))))\n",
    "draw_stacked_bars(\"Reducing user burden without sacrificing data quality\", [\"Before\", \"After\"], [(0, 0), (saved_due_to_confirm, saved_due_to_expectations)], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e1bf3",
   "metadata": {},
   "source": [
    "## Numbers of Note\n",
    "Here is an attempt to put all the numbers I actually use in the paper in one place. All of these numbers should be merely restating what is above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c59b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tprint(label, value):  # Tabularly print\n",
    "    print(label.ljust(35, ' ')+\" \"+str(value).rjust(20, ' '))\n",
    "\n",
    "tprint(\"Size of Dataset 1\", len(server_filtered_users))\n",
    "tprint(\"Size of Dataset 2\", len(filtered_users))\n",
    "print()\n",
    "\n",
    "tprint(\"% need not label ensemble all\", format_frac_percent(*complete_results[\"ens\"][\"high\"][\"All\"]))\n",
    "tprint(\"% To Label all yellow ens all\", format_frac_percent(*complete_results[\"ens\"][\"mid\"][\"All\"]))\n",
    "tprint(\"% need not label ens after\", format_frac_percent(*complete_results[\"ens\"][\"high\"][\"After\"]))\n",
    "tprint(\"% To Label all yellow ens after\", format_frac_percent(*complete_results[\"ens\"][\"mid\"][\"After\"]))\n",
    "print()\n",
    "\n",
    "tprint(\"# taps after\", sum(fu(taps).values()))\n",
    "tprint(\"# trips labeled after\", sum(fu(trips_labeled).values()))\n",
    "tprint(\"# taps saved due to Confirm\", sum(fu(taps_avoided).values()))\n",
    "tprint(\"taps saved per trip due to Confirm\", f\"{sum(fu(taps_avoided).values())/sum(fu(trips_labeled).values()):.2f}\")\n",
    "tprint(\"% taps saved due to Confirm\", f\"{sum(fu(taps_avoided).values())/sum(fu(trips_labeled).values())/OLD_TAPS:.2%}\")\n",
    "tprint(\"# of users who used Confirm\", len([u for u in programs[\"ens\"] if u in filtered_users and u in verifiers]))\n",
    "tprint(\"% of trips finalized using Confirm\", format_frac_percent(sum([verifieds[u] for u in programs[\"ens\"] if u in filtered_users]), sum([trips_labeled[u] for u in programs[\"ens\"] if u in filtered_users])))\n",
    "print()\n",
    "\n",
    "tprint(\"# trips not in To Label\", sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "tprint(\"# taps saved due to To Label\", OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "tprint(\"# taps saved total\", sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "\n",
    "trips_relevant_total = sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values())\n",
    "tprint(\"# trips relevant total\", trips_relevant_total)  \n",
    "tprint(\"# taps saved per trip total\", f\"{(sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(trips_relevant_total):.2f}\")\n",
    "tprint(\"% taps saved per trip total\", f\"{(sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(trips_relevant_total)/OLD_TAPS:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42afbd51",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "Here's the plan:\n",
    "\n",
    "ASSUME as we have been doing that people don't misclick (i.e., once you label a label, you don't relabel it)\n",
    "\n",
    "Perhaps TODO test this assumption\n",
    "\n",
    "Limit consideration to labeled trips during the after period that were unlabeled when the inference algorithm ran on them.\n",
    "\n",
    "For each trip:\n",
    " 1. Calculate pre-update idealized number of taps (always 6)\n",
    " 2. Calculate post-update actual number of taps (verify_events+2*label_events as before)\n",
    " 3. Calculate post-update idealized number of taps if user had followed intended algorithm:\n",
    "    1. If all the yellow labels are correct, press verify button\n",
    "    2. Repeat substep 1 until no more correct yellow labels\n",
    "    3. Input true value for ~most certain~ first non-green label\n",
    "    4. Repeat substeps 1-3 until trip completely labeled\n",
    "    - Note that this is NOT the optimal algorithm -- the optimal algorithm would have people clicking the verify button if ANY of the yellow labels are correct, but we don't teach that you should do that\n",
    "\n",
    "We've already shown stacked-bars graphs of 1 vs. 2. Now, show graphs of 1 vs. 2 vs. 3. Is 3 sufficiently close to 2 that this is a useful approximation? If so, continue. If not, a much more complicated algorithm is needed.\n",
    "\n",
    "ASSUME that people follow the intended algorithm (see above). Now for a given low and high confidence, we can easily calculate a counterfactual stacked-bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labelstruct = [\n",
    "    {\"labels\": {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"shopping\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.15},\n",
    "    {\"labels\": {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"entertainment\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.05},\n",
    "    {\"labels\": {\"mode_confirm\": \"drove_alone\", \"purpose_confirm\": \"work\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.45},\n",
    "    {\"labels\": {\"mode_confirm\": \"shared_ride\", \"purpose_confirm\": \"work\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.35}\n",
    "]\n",
    "\n",
    "test_groundtruth = {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"shopping\", \"replaced_mode\": \"placeholder\"}\n",
    "\n",
    "# confidences = {}\n",
    "# for label_type in LABEL_CATEGORIES:\n",
    "#     counter = {}\n",
    "#     for line in inference:\n",
    "#         if label_type not in line[\"labels\"]: continue  # Seems we have some incomplete tuples!\n",
    "#         val = line[\"labels\"][label_type]\n",
    "#         if val not in counter: counter[val] = 0\n",
    "#         counter[val] += line[\"p\"]\n",
    "#     confidences[label_type] = max(counter.values()) if len(counter) > 0 else 0 # This needs to be max, not sum!!! A major bug in a previous version.\n",
    "# trip_confidence = min(confidences.values())\n",
    "\n",
    "# Basically a copypaste of the reimplementation above -- this is fiddly stuff we'd like to touch as little as possible\n",
    "# TODO refactor to eliminate code duplication, but only do this in the presence of testing data sufficient to ensure we don't mess it up\n",
    "\n",
    "# Labelstruct is the inferred probabilites for different label tuples for 1 trip.\n",
    "\n",
    "def sum_confidences(labelstruct):\n",
    "    confidences = {}\n",
    "    for label_type in LABEL_CATEGORIES:\n",
    "        counter = {}\n",
    "        for line in labelstruct:  # line is one of the sublists\n",
    "            if label_type not in line[\"labels\"]: continue   # eg mode_confirm not in the value list associated with \"labels\"\n",
    "            val = line[\"labels\"][label_type]    # eg \"walk\"\n",
    "            if val not in counter: counter[val] = 0\n",
    "            counter[val] += line[\"p\"]         # add the associated confidence to the sum for the current label value\n",
    "        confidences[label_type] = counter # place counter as the value for the current label_type\n",
    "    return confidences\n",
    "\n",
    "def best_confidences(labelstruct):\n",
    "    confidences = sum_confidences(labelstruct)\n",
    "    # Take the largest confidence labels\n",
    "    return {k: max(confidences[k].items(), key = lambda item: item[1]) if len(confidences[k]) > 0 else (None, 0) for k in confidences}\n",
    "    \n",
    "# Calculates the label categories and values we can display as yellow\n",
    "# Assumes we've already filtered out the non-viable options and renormalized\n",
    "def get_yellows(labelstruct, low_thresh):\n",
    "    confidences = best_confidences(labelstruct)\n",
    "    # print(\"cs\")\n",
    "    # print(confidences)\n",
    "    return {k: confidences[k][0] for k in confidences if confidences[k][1] > low_thresh}\n",
    "\n",
    "def next_green(labelstruct, established, ground_truth):\n",
    "    # Figure out the most probable thing for the user to next fill in\n",
    "    # confidences = sum_confidences(labelstruct)\n",
    "    # confidences = {k: confidences[k] for k in confidences if k not in established}  # Eliminate categories we've already greened\n",
    "    # choice = max(confidences.items(), key = lambda item: item[1][ground_truth[item[0]]] if len(item[1]) > 0 else 0)  # Pick the item whose probability matching the actual truth is highest\n",
    "    # return choice[0]\n",
    "\n",
    "    # Actually let's just say the user fills in the labels in order (this also seems plausible)\n",
    "    return next(filter(lambda category: category not in established, LABEL_CATEGORIES))\n",
    "\n",
    "def filter_and_renormalize(labelstruct, established, certainty):\n",
    "    for label_type in established:\n",
    "        # print(label_type)\n",
    "        labelstruct = list(filter(lambda row: row[\"labels\"][label_type] == established[label_type], labelstruct))\n",
    "    new_certainty = sum([row[\"p\"] for row in labelstruct])\n",
    "    for row in labelstruct: row[\"p\"] *= certainty/new_certainty\n",
    "    return labelstruct\n",
    "\n",
    "def is_above_high(labelstruct, high_thresh):\n",
    "    confidences = best_confidences(labelstruct)\n",
    "    confidences = {k: confidences[k][1] for k in confidences}  # store the best confidence value for each label category\n",
    "    trip_confidence = min(confidences.values())\n",
    "    return trip_confidence > high_thresh\n",
    "\n",
    "def intended_taps(labelstruct, ground_truth, low_thresh):\n",
    "    # print(ground_truth)\n",
    "    certainty = sum([row[\"p\"] for row in labelstruct])\n",
    "    established = {}\n",
    "    taps = 0\n",
    "    \n",
    "    while(len(established) < len(LABEL_CATEGORIES)):\n",
    "        # print(\"eold\")\n",
    "        # print(established)\n",
    "        candidates = get_yellows(labelstruct, low_thresh)\n",
    "        # print(\"c\")\n",
    "        # print(candidates)\n",
    "        # If all the yellow labels are correct, press the verify button and loop again\n",
    "        # (note the choice of all instead of any)\n",
    "        # print(\"g\")\n",
    "        # print(ground_truth)\n",
    "        if len(candidates) > 0 and all([candidates[k] == ground_truth[k] for k in candidates]):\n",
    "            taps += 1\n",
    "            established.update(candidates)\n",
    "        # Otherwise manually label the most confident\n",
    "        else:\n",
    "            taps += 2\n",
    "            selected = next_green(labelstruct, established, ground_truth)\n",
    "            established[selected] = ground_truth[selected]\n",
    "        # print(\"enew\")\n",
    "        # print(established)\n",
    "        labelstruct = filter_and_renormalize(labelstruct, established, certainty)\n",
    "        # print(\"ren\")\n",
    "        # print(labelstruct)\n",
    "        # print()\n",
    "    return taps\n",
    "\n",
    "assert intended_taps(test_labelstruct, test_groundtruth, LOW_CONFIDENCE_THRESHOLD_PRODUCTION) == 3\n",
    "# It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9448e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# This get's 'apply'ed to the ct_df dataframe\n",
    "def is_saved_due_to_expectations(high_thresh):\n",
    "    return lambda row: int(is_above_high(row[\"inferred_labels\"], high_thresh))  # found it? multiply by 6?\n",
    "\n",
    "def is_saved_due_to_confirm(low_thresh, high_thresh):\n",
    "    return lambda row: 0 if is_above_high(row[\"inferred_labels\"], high_thresh) \\\n",
    "        else 6 - intended_taps(row[\"inferred_labels\"], row[\"user_input\"], low_thresh)   ##### not right\n",
    "\n",
    "def get_eliminations(low_thresh, high_thresh):\n",
    "    print(f\"{low_thresh},{high_thresh}\")\n",
    "    u_due_to_expectations = {}\n",
    "    u_due_to_confirm = {}\n",
    "    u_n_trips = {}\n",
    "    for user in filtered_users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        \n",
    "        # Filter to get only unlabeled trips or trips that were labeled after the inference.\n",
    "        ct_df = filter_unlabeled(ct_df)\n",
    "        ct_df = ct_df[ct_df[\"user_input\"].apply(len) != 0]\n",
    "        # print(ct_df[\"user_input\"])\n",
    "        \n",
    "        # Make new columns that have a 1 if the algorithm saved the user a label\n",
    "        ct_df[\"due_to_expectations\"] = ct_df.apply(is_saved_due_to_expectations(high_thresh), axis=1)\n",
    "        ct_df[\"due_to_confirm\"] = ct_df.apply(is_saved_due_to_confirm(low_thresh, high_thresh), axis=1)\n",
    "        \n",
    "        # per user taps saved due to expectations?\n",
    "        u_due_to_expectations[user] = ct_df[\"due_to_expectations\"].sum()  \n",
    "        \n",
    "        u_due_to_confirm[user] = ct_df[\"due_to_confirm\"].sum()\n",
    "        u_n_trips[user] = ct_df.shape[0]\n",
    "    eliminations = sum(u_due_to_confirm.values())/sum(u_n_trips.values()), sum(u_due_to_expectations.values())/sum(u_n_trips.values())\n",
    "\n",
    "\n",
    "    # THE STACKED BARS EXPECTS SAVED DUE TO CONFIRM FIRST!!!! BUT ELIMINATIONS HAS SAVED DUE TO EXPECTATIONS FIRST!!\n",
    "\n",
    "\n",
    "    return eliminations\n",
    "        \n",
    "t1 = time.time()\n",
    "(idealized_saved_due_to_confirm, idealized_saved_due_to_expectations) = \\\n",
    "    get_eliminations(LOW_CONFIDENCE_THRESHOLD_PRODUCTION, HIGH_CONFIDENCE_THRESHOLD_PRODUCTION)\n",
    "print(time.time()-t1)\n",
    "draw_stacked_bars(\"Graph of 1 vs. 2 vs. 3 as described above\", \\\n",
    "                  [\"1 = Before\", \"2 = After\", \"3 = Idealized\"], \\\n",
    "                  [(0, 0), (saved_due_to_confirm, saved_due_to_expectations), \\\n",
    "                   (idealized_saved_due_to_confirm, idealized_saved_due_to_expectations)], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c7278",
   "metadata": {},
   "source": [
    "Now we can actually do the sensitivity analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    (LOW_CONFIDENCE_THRESHOLD_PRODUCTION, HIGH_CONFIDENCE_THRESHOLD_PRODUCTION),\n",
    "    \n",
    "    (0.05, 0.89),\n",
    "    (0.33, 0.89),\n",
    "    (0.40, 0.89),\n",
    "    (0.60, 0.89),\n",
    "    \n",
    "    (0.25, 0.75),\n",
    "    (0.25, 0.80),\n",
    "    (0.25, 0.95),\n",
    "    (0.25, 0.99) #.95\n",
    "]\n",
    "labels = list(map(lambda scenario: f\"({scenario[0]:.2f}, {scenario[1]:.2f})\", scenarios))\n",
    "labels[0] = \"curr=\"+labels[0]\n",
    "draw_stacked_bars(\"Sensitivity analysis!\", labels, \\\n",
    "                  list(map(lambda t: get_eliminations(*t), scenarios)), \\\n",
    "                  6, width=12, \\\n",
    "                  hook = lambda ax: ax.set_xlabel(\"Scenario: (lower threshold, upper threshold)\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416d85f",
   "metadata": {},
   "source": [
    "The above graphs look rather strange to me, but that might be because I'm not working with the full dataset. This analysis should be re-run with the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8c23b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('emission-private-eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
