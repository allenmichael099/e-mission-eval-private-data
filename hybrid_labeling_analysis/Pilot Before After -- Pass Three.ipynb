{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb176e71",
   "metadata": {},
   "source": [
    "# Pilot Before After -- Pass Three\n",
    "This is the same as Pass Two except with an additional \"sensitivity analysis\" section at the end. Also, possible minor modifications to increase compatibility with different types of database dump.\n",
    "\n",
    "## Pass Two\n",
    "This reimplements the elements of `Pilot Before After -- Pass One` that we actually use in a more streamlined manner and also does some pilot-specific analysis. This is the main analysis code used to generate the results in the paper with the working title \"A configurable approach to requesting user input and validation of low-confidence trip label inferences.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a48a04",
   "metadata": {},
   "source": [
    "## Usage\n",
    "To use, starting with a database dump tarball for each of the pilot programs plus stage:\n",
    "\n",
    "  1. Expand each tarball into its own folder\n",
    "  2. Make a new empty folder and start `mongod`, using the `--dbpath` option to tell it to store the database in that empty folder\n",
    "  3. `mongorestore` each of the dumps, one at a time, verifying that there are no weird errors. I did this in ascending order of dump size except ending with stage.\n",
    "  4. Ideally you would have \"0 document(s) failed to restore\" for all of the `mongorestore`s, but I consistently get `1400` documents failing to restore for `fc` and `1402` for `cc`.\n",
    "  5. Set the proper environment variables so this notebook can find the `emission` scripts (I do this for myself with a slightly hacky `sys.path.insert` below)\n",
    "  6. Run the notebook top to bottom. It should take on the order of 20 minutes (benchmarked on a 2015 MacBook Air) and run without errors. It takes massive (a few gigabytes) amounts of RAM, both in the `python` process and on the part of `mongod`.\n",
    "  7. The notebook is structured to load everything it needs from the database in the first few cells and then does not rely on the database again, so if you are tweaking later analyses and want to recover some RAM you can terminate the `mongod` process.\n",
    "\n",
    "tl;dr: this relies on a rather specific database configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e21ab",
   "metadata": {},
   "source": [
    "## Desired Output\n",
    "\n",
    "### All the useful information we want to keep from the previous notebook\n",
    "Dataset 1 -- no \"after\" condition necessary:\n",
    " * Number of participants\n",
    " * Unlabeled trips that users need not interact with at all\n",
    " * Trips that would be in To Label with no red labels\n",
    "\n",
    "Dataset 2 -- yes \"after\" condition necessary:\n",
    " * Number of participants\n",
    " * Frequency of app opens\n",
    " * Taps actually avoided by verify button\n",
    " * Taps actually avoided by hiding high confidence trips\n",
    " * Overall taps avoided (total, per trip, percentage of taps)\n",
    " * Fraction of users who used the verify button\n",
    " * Fraction of trips finalized using the verify button\n",
    "\n",
    "### New features\n",
    " * Graphs of weekly labeling percentage and number of days per week the app was used over time\n",
    " * Comparisons of pilot programs that started before the update was released to pilot programs that started with the update already installed\n",
    " * Histogram of expectation confidences, segmented by how they are presented to the user\n",
    " * Weekly labeling visualizations at per-user granularity\n",
    " * Visualization of how we save taps\n",
    " * Summary of all the numbers used in the draft paper\n",
    "\n",
    "### Still to do\n",
    " * What happens if we were to change the confidence thresholds? Can we save users more taps? This should be explored by figuring out how much of the current tapping is correcting the algorithm vs. filling in red labels vs. simply not using the new features -- which can be done with the \"select_label\" instrumentation event. If we lower the low threshold, we would expect the number of taps used to fill in red labels to decrease, but not the other two categories of taps.\n",
    " * Try to measure the time spent on each of the screens from which it is possible to label (To Label, All Unlabeled, Diary, etc.) -- this might be difficult, but it would be useful both to do a comparison between screens and also to see if this update changes the amount of time people spend labeling.\n",
    " * Maybe some more per-program breakdowns would be useful?\n",
    " * Statistical tests might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e972e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTINGMODE = True\n",
    "\n",
    "# Declarations -- we declare variables here so that we don't accidentally clear them later quite as much\n",
    "EXCLUDE_UUIDS = []\n",
    "stats = {}\n",
    "user_info = {}\n",
    "confirmed_trip_df_map = {}\n",
    "user_before_start = {}  # When the \"before\" period starts for each user\n",
    "user_after_start = {}  # When the \"after\" period starts for each user\n",
    "filter1_users = []  # Users with enough total trips\n",
    "filter2_users = []  # Users that have installed the update\n",
    "filter3_users = []  # Users with enough before trips\n",
    "filter4_users = [] # Users with enough after trips\n",
    "server_filtered_users = []\n",
    "filtered_users = []\n",
    "match_histogram = {}\n",
    "g_high_confidence_n_after_unlabeled = None  # Hack to give old code access to this later. TODO: rework this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0cc68",
   "metadata": {},
   "source": [
    "## Choices\n",
    " * Let \"before\" be from June 1 until the user installed the update\n",
    " * Let \"after\" be from when the user installed the update until the most recent data available (as of writing, October 18)\n",
    " * Require 30 total trips for inclusion in Dataset 1\n",
    " * Require 10 trips after the switch for inclusion in Dataset 2\n",
    " * For looking at frequency of app opens, analyze the entire Dataset 2 and then look at those who have opened the app at least 5 times before the switch and 5 times after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_TRIPS_TOTAL = 30  # Must be at least 1 to prevent division by zero\n",
    "if TESTINGMODE: REQUIRED_TRIPS_TOTAL = 1\n",
    "REQUIRED_TRIPS_BEFORE = 0  # Changed from 10 on 2021-12-18 -- this is a significant change in methodology!\n",
    "REQUIRED_TRIPS_AFTER = 10  # Changed from 0 on 2021-12-18\n",
    "REQUIRED_OPENS_TOTAL = 0\n",
    "REQUIRED_OPENS_BEFORE = 0  # Changed from 5 on 2021-12-18 -- this is a significant change in methodology!\n",
    "REQUIRED_OPENS_AFTER = 5\n",
    "import arrow\n",
    "MY_TZ = \"America/Denver\"  # Timezone we use when that information is absent (TODO this can be inferred from other data structures)\n",
    "BEFORE_START = arrow.get(\"2021-06-01T00:00-06:00\")\n",
    "AFTER_END = arrow.get(\"2021-11-15T09:00-08:00\")\n",
    "weeks = list(arrow.Arrow.span_range(\"week\", BEFORE_START, AFTER_END))[:-1]  # The weeks we care about when doing weekly analyses\n",
    "from uuid import UUID  # This part is for if you want to manually exclude certain users\n",
    "# EXCLUDE_UUIDS = [UUID(s) for s in input(\"Enter UUIDs to exclude, separated by spaces: \").split(\" \") if len(s) > 0]\n",
    "print(EXCLUDE_UUIDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0672aa",
   "metadata": {},
   "source": [
    "## Settings\n",
    "The below settings correctly mirror the production configuration; however, **the confidence thresholds are different for stage**. This must be kept in mind when doing analysis of stage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CATEGORIES = [\"mode_confirm\", \"purpose_confirm\", \"replaced_mode\"]\n",
    "HIGH_CONFIDENCE_THRESHOLD_PRODUCTION = 0.89  # Confidence we need to not put a trip in To Label\n",
    "# if TESTINGMODE: HIGH_CONFIDENCE_THRESHOLD_PRODUCTION = 0.80\n",
    "LOW_CONFIDENCE_THRESHOLD_PRODUCTION = 0.25  # confidenceThreshold from the config file\n",
    "OLD_TAPS = 2*len(LABEL_CATEGORIES)  # Number of taps each trip required to fully label under the old UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1922ca0",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc0a85",
   "metadata": {},
   "source": [
    "### Imports, aliases, logging, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../e-mission-server\")  # Works for my configuration; might be different for you\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as estag\n",
    "import emission.storage.timeseries.timequery as estt\n",
    "from statistics import mean\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Various ways to play with how large a firehose of information you want\n",
    "_default_log_level = logging.DEBUG\n",
    "def set_log_level(level):\n",
    "    logging.getLogger().setLevel(level)\n",
    "def reset_log_level():\n",
    "    global _default_log_level\n",
    "    logging.getLogger().setLevel(_default_log_level)\n",
    "set_log_level(logging.WARNING)\n",
    "\n",
    "agts = estag.AggregateTimeSeries()\n",
    "\n",
    "db_keys = {\n",
    "    \"time\": \"stats/client_time\",\n",
    "    \"error\": \"stats/client_error\",\n",
    "    \"nav\": \"stats/client_nav_event\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a883920",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_between = lambda dataset, key, start, end: dataset[(dataset[key] >= start) & (dataset[key] <= end)]\n",
    "fu = lambda d, users=filtered_users: {k: d[k] for k in d if k in users}  # Filter users\n",
    "\n",
    "def filter_update(new, old, reason):\n",
    "    print(f\"Excluded {len(old)-len(new)} users, left with {len(new)}: {reason}\")\n",
    "\n",
    "format_frac_percent = lambda num, denom: f\"{num}/{denom}={(num/denom if denom != 0 else float('NaN')):.2%}\"\n",
    "\n",
    "delta2days = lambda d: d.days+d.seconds/86400\n",
    "\n",
    "format_arrow_comma = lambda a, b, c: f\"{a:.2f}->{b:.2f}, {c}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14984b38",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89d7d9",
   "metadata": {},
   "source": [
    "### Load stats and user databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d906eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 6m and 10m on an early-2015 MacBook Air\n",
    "for key in db_keys:\n",
    "    print(f'Adding \"{db_keys[key]}\" to stats as \"{key}\"')\n",
    "    stats[key] = agts.get_data_df(db_keys[key])\n",
    "    print(f\"-> Done; found {stats[key].shape[0]} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "programs_all = {\"ens\": [], \"before_after\": [], \"only_after\": []}  # This will be a dict where keys are all pilot programs plus \"ens\" for \"ensemble, no stage\" and \"stage\" for stage; and values are the users corresponding to each\n",
    "# Late in this process, I added \"before_after\" and \"only_after\": \"only_after\" is an ensemble for the programs where users were given the app with the update already installed, and \"before_afer\" is an ensemble for the remaining programs\n",
    "only_after_programs = [\"vail\", \"pc\"]\n",
    "for u in edb.get_uuid_db().find():\n",
    "    program = u[\"user_email\"].split(\"_\")[0]\n",
    "    uuid = u[\"uuid\"]\n",
    "    u[\"program\"] = program\n",
    "    if program not in programs_all.keys(): programs_all[program] = []\n",
    "    if program != \"stage\":\n",
    "        programs_all[\"ens\"].append(uuid)\n",
    "        if program in only_after_programs:\n",
    "            programs_all[\"only_after\"].append(uuid)\n",
    "        else:\n",
    "            programs_all[\"before_after\"].append(uuid)\n",
    "    programs_all[u[\"program\"]].append(uuid)\n",
    "    user_info[uuid] = u\n",
    "print(\"Programs all: \"+str({k: len(programs_all[k]) for k in programs_all}))\n",
    "\n",
    "# Ignore the small ensembles in certain cases\n",
    "programs_some = programs_all.copy()\n",
    "programs_some.pop(\"before_after\")\n",
    "programs_some.pop(\"only_after\")\n",
    "print(\"Programs some: \"+str({k: len(programs_some[k]) for k in programs_some}))\n",
    "\n",
    "programs = programs_all  # For backwards compatibility\n",
    "# The upside to this way of doing ensembles is it's really easy. The downside is a given user's data is calculated as many times as that user appears in the list of programs -- so with our current ensembles, we're doing most calculations three times when we only really need to do them once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1cadd",
   "metadata": {},
   "source": [
    "### Get Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 3m10 on an early-2015 MacBook Air\n",
    "set_log_level(logging.INFO)\n",
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "print(f\"Working with {len(all_users)} initial users\")\n",
    "\n",
    "filter0_users = [u for u in all_users if u not in EXCLUDE_UUIDS]  # Users that we don't explicitly exclude\n",
    "filter_update(filter0_users, all_users, \"presence on exclusion list\")\n",
    "\n",
    "if TESTINGMODE: gooduser = filter0_users[1]\n",
    "\n",
    "for u in filter0_users:\n",
    "    ts = esta.TimeSeries.get_time_series(u if not TESTINGMODE else gooduser)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "    confirmed_trip_df_map[u] = ct_df\n",
    "    if ct_df.shape[0] >= REQUIRED_TRIPS_TOTAL: filter1_users.append(u)\n",
    "filter_update(filter1_users, filter0_users, \"not enough total trips\")\n",
    "\n",
    "# To find a user's UUID based on the end date of their first trip:\n",
    "# for u in filter1_users:\n",
    "#     ct_df = confirmed_trip_df_map[u].copy()\n",
    "#     ct_df.sort_values(\"end_ts\", ascending=True, inplace=True)\n",
    "#     print(u)\n",
    "#     print(arrow.get(ct_df.iloc[0][\"end_ts\"]).to(\"America/Chicago\"))\n",
    "#     print()\n",
    "\n",
    "for u in filter1_users:\n",
    "    # Convert timestamps to more usable values; find per-user starting points\n",
    "    # I used to do this later in the process, but it turns out end_arrow and user_before_start are useful for server_confirmed_users too\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    if \"end_ts\" in ct_df:\n",
    "        ct_df[\"end_arrow\"] = ct_df[\"end_ts\"].apply(arrow.get)\n",
    "        ct_df.sort_values(\"end_arrow\", ascending=True, inplace=True)\n",
    "    else:\n",
    "        print(\"end_ts not in dataframe for \"+str(u))\n",
    "    if \"metadata_write_ts\" in ct_df:\n",
    "        ct_df[\"write_arrow\"] = ct_df[\"metadata_write_ts\"].apply(arrow.get)\n",
    "    else:\n",
    "        print(\"metadata_write_ts not in dataframe for \"+str(u))\n",
    "    this_before_start = max(ct_df.iloc[0][\"end_arrow\"], BEFORE_START)\n",
    "    user_before_start[u] = this_before_start\n",
    "\n",
    "for user in filter1_users:\n",
    "    if user not in server_filtered_users:\n",
    "        server_filtered_users.append(user)\n",
    "print(f\"For metrics that don't need user interaction, working with {len(server_filtered_users)} filtered users\")\n",
    "\n",
    "reset_log_level()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f2d28",
   "metadata": {},
   "source": [
    "### Get Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034dadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 30s and 50s on an early-2015 MacBook Air\n",
    "for u in filter1_users:\n",
    "    lts = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"label_tab_switch\")]\n",
    "    if len(lts) > 0:\n",
    "        filter2_users.append(u)\n",
    "        lts = lts.copy()\n",
    "        lts.sort_values(\"ts\", ascending=True, inplace=True)\n",
    "        this_after_start = arrow.get(lts.iloc[0][\"ts\"])\n",
    "        user_after_start[u] = this_after_start\n",
    "filter_update(filter2_users, filter1_users, \"have not installed the update\")\n",
    "\n",
    "for u in filter2_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    n_before = filter_between(ct_df, \"end_arrow\", user_before_start[u], user_after_start[u]).shape[0]\n",
    "    if n_before >= REQUIRED_TRIPS_BEFORE: filter3_users.append(u)\n",
    "filter_update(filter3_users, filter2_users, \"not enough before trips\")\n",
    "\n",
    "for u in filter3_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    n_after = ct_df[(ct_df[\"end_arrow\"] >= user_after_start[u])].shape[0]\n",
    "    if n_after >= REQUIRED_TRIPS_AFTER: filter4_users.append(u)\n",
    "filter_update(filter4_users, filter3_users, \"not enough after trips\")\n",
    "\n",
    "for user in filter4_users:\n",
    "    if user not in filtered_users:\n",
    "        filtered_users.append(user)\n",
    "print(f\"For metrics that do need user interaction, working with {len(filtered_users)} filtered users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716bed7",
   "metadata": {},
   "source": [
    "## Results from Dataset 1!\n",
    "To review, we want:\n",
    " * Number of participants\n",
    " * Unlabeled trips that users need not interact with at all\n",
    " * Trips that would be in To Label with no red labels\n",
    "\n",
    "and breakdowns of (all of? some of?) the above for each individual pilot program region (\"program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113548d",
   "metadata": {},
   "source": [
    "### Number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b67b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUMBER OF USERS IN DATASET 1: {len(server_filtered_users)}\")\n",
    "print(\"Breakdown by program: \"+str({k: len([u for u in programs[k] if u in server_filtered_users]) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c999c9c",
   "metadata": {},
   "source": [
    "### Unlabeled trips that users need not interact with at all\n",
    "Note: for this one, we calculate a stat across All of time, one for Before, and one for After. Currently, Before includes all of Dataset 1 (and obviously After only includes Dataset 2). TODO: figure out whether it might be a better comparison to report only Dataset 2 for Before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 30s and 50s on an early-2015 MacBook Air\n",
    "# Load the user confirmation data from the database\n",
    "manuals = {label: agts.get_data_df(\"manual/\"+label) for label in LABEL_CATEGORIES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seems to work but is way too slow; see below for the faster way\n",
    "\"\"\"\n",
    "set_log_level(logging.WARNING)\n",
    "import emission.storage.decorations.trip_queries as esdt\n",
    "trip_to_manuals = {}  # Dictionary by user of (dictionary by trip ID of (dictionary by label type of ()))\n",
    "\n",
    "malleable = lambda: type('', (), {})  # An object we can do anything with\n",
    "\n",
    "print(sum([confirmed_trip_df_map[u].shape[0] for u in all_users]))\n",
    "print(sum([confirmed_trip_df_map[u].shape[1] for u in all_users]))\n",
    "\n",
    "for u in all_users:\n",
    "    print(u)\n",
    "    trip_to_manuals[u] = {}\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    print(confirmed_trip_df_map[u].shape)\n",
    "    for i, trip in confirmed_trip_df_map[u].iterrows():\n",
    "        print(trip.keys())\n",
    "        break\n",
    "        print(i, end=\" \")\n",
    "        trip_to_manuals[u][trip._id] = {}\n",
    "        for label in manuals:\n",
    "            # ui = esdt.get_user_input_for_trip(\"analysis/confirmed_trip\", u, trip._id, \"manual/\"+label)\n",
    "            # trip_obj = ts.get_entry_from_id(\"analysis/confirmed_trip\", trip._id)\n",
    "            trip_obj = malleable()\n",
    "            trip_obj.data = trip\n",
    "            trip_obj.metadata = malleable()\n",
    "            trip_obj.metadata.time_zone = trip.start_local_dt_timezone\n",
    "            ui = esdt.get_user_input_for_trip_object(ts, trip_obj, \"manual/\"+label)\n",
    "            trip_to_manuals[u][trip._id][label] = ui\n",
    "        \n",
    "        \n",
    "        # for i, row in manuals[label].iterrows():\n",
    "        #     if (i % 10 == 0): print(i, end=\" \")\n",
    "        #     ui = malleable()\n",
    "        #     ui.data = row\n",
    "        #     ui.metadata = malleable()\n",
    "        #     ui.metadata.time_zone = row.start_local_dt_timezone\n",
    "        #     trip = esdt.get_trip_for_user_input_obj(ts, ui)\n",
    "        #     if trip is None: continue\n",
    "        #     if trip.get_id() not in trip_to_manuals: trip_to_manuals[u][trip.get_id()] = {}\n",
    "        #     trip_to_manuals[u][trip.get_id()][label] = ui\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 5m27 on an early-2015 MacBook Air\n",
    "# Calculate which trips were manually labeled before the inference algorithm ran. These trips are part of the \"train\" dataset, so we need to exclude them from the \"test\" dataset.\n",
    "# To do this, we must match confirmed trip entries with entries from the user input database.\n",
    "# TODO My matcher is a rather blunt tool, and it misses a lot of matches. Can we get something with success rates approaching that of esdt.get_user_input_for_trip_object without sacrificing so much time?\n",
    "import time\n",
    "def filter_time_permissive(df, trip, threshold=15):\n",
    "    start = trip[\"start_ts\"]\n",
    "    end = trip[\"end_ts\"]\n",
    "    before_g = df[\"start_ts\"] >= start-threshold\n",
    "    before_l = df[\"start_ts\"] <= start+threshold\n",
    "    after_g = df[\"end_ts\"] >= end-threshold\n",
    "    after_l = df[\"end_ts\"] <= end+threshold\n",
    "    result = df[before_g & before_l & after_g & after_l]\n",
    "    return result\n",
    "\n",
    "def get_write_time(df, trip):\n",
    "    if len(trip[\"user_input\"]) == 0: return float(\"NaN\")\n",
    "    candidates = filter_time_permissive(df, trip)\n",
    "    if len(candidates) not in match_histogram: match_histogram[len(candidates)] = 0\n",
    "    match_histogram[len(candidates)] += 1\n",
    "    write_times = candidates[\"metadata_write_ts\"].values\n",
    "    return min(write_times) if len(write_times) > 0 else float(\"-inf\") # If we can't find a match, assume the worst\n",
    "\n",
    "def get_write_times(trip):\n",
    "    times = [get_write_time(df, trip) for df in manuals.values()]\n",
    "    return min(times)\n",
    "\n",
    "def explore_matching(user):\n",
    "    print(user)\n",
    "    ct_df = confirmed_trip_df_map[user].copy()\n",
    "    print(ct_df.shape)\n",
    "    to_match = manuals[\"mode_confirm\"]\n",
    "    print(to_match.keys())\n",
    "\n",
    "    t1 = time.time()\n",
    "    for i, trip in ct_df.iterrows():\n",
    "        if len(trip[\"user_input\"]) == 0: continue\n",
    "        t_start = trip[\"start_ts\"]\n",
    "        t_end = trip[\"end_ts\"]\n",
    "        # print(t_start)\n",
    "        THRESHOLD = 60\n",
    "        filtered = filter_time_permissive(to_match, t_start, t_end)\n",
    "        if (filtered.shape[0] != 1):\n",
    "            print(filtered[\"metadata_write_ts\"].values)\n",
    "            print(filtered.shape[0])\n",
    "    print(time.time()-t1)\n",
    "\n",
    "# explore_matching(filtered_users[1])\n",
    "\n",
    "def do_matching(users):\n",
    "    for user in users:\n",
    "        # print(user)\n",
    "        ct_df = confirmed_trip_df_map[user]\n",
    "        # print(ct_df.keys())\n",
    "        # print(ct_df.shape)\n",
    "        ct_df[\"label_write_time\"] = ct_df.apply(lambda trip: get_write_times(trip), axis=1)\n",
    "\n",
    "do_matching(server_filtered_users)\n",
    "print(sorted(match_histogram.items()))  # We want as many items as possible to have exactly one match. Zero matches means we will be forced to exclude the trip, and multiple matches means we must take the most pessimistic match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bac4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 19s on an early-2015 MacBook Air\n",
    "\n",
    "# Previously, we only operated on trips that were *actually* unlabeled. Now, we operate on trips that were unlabeled at the time of expectation generation.\n",
    "def filter_unlabeled(df):\n",
    "    # return df[df[\"user_input\"].apply(len) == 0]\n",
    "    # Check first for label_write_time is NaN and then for label_write_time after expectation generation\n",
    "    return df[(df[\"label_write_time\"] != df[\"label_write_time\"]) | (df[\"label_write_time\"] > df[\"metadata_write_ts\"])]\n",
    "\n",
    "def is_unlabeled(trip):\n",
    "    # return len(trip[\"user_input\"]) == 0\n",
    "    # print(\"Labeled at: \"+str(trip[\"label_write_time\"]))\n",
    "    # print(\"Inferred at: \"+str(trip[\"metadata_write_ts\"]))\n",
    "    return (trip[\"label_write_time\"] != trip[\"label_write_time\"]) | (trip[\"label_write_time\"] > trip[\"metadata_write_ts\"])\n",
    "\n",
    "def high_stats(users):\n",
    "    global g_high_confidence_n_after_unlabeled  # see above :(\n",
    "    \n",
    "    total_trip_n = {}\n",
    "    total_trip_n_after = {}\n",
    "    total_trip_n_unlabeled = {}\n",
    "    total_trip_n_after_unlabeled = {}\n",
    "    high_confidence_n = {}  # Trips with inferences so confident they don't need to go in To Label\n",
    "    high_confidence_n_after = {}\n",
    "    high_confidence_n_unlabeled = {}\n",
    "    high_confidence_n_after_unlabeled = {}\n",
    "    mid_confidence_n = {}  # Trips that need to go in To Label but have no red labels\n",
    "    mid_confidence_n_after = {}\n",
    "    mid_confidence_n_unlabeled = {}\n",
    "    mid_confidence_n_after_unlabeled = {}\n",
    "    high_confidence_frac = {}\n",
    "    mid_confidence_frac = {}\n",
    "    mid_confidence_any = {}  # Trips that need to go in To Label but have at least one yellow label\n",
    "    mid_confidence_any_after = {}\n",
    "    all_confidences = []\n",
    "\n",
    "    for u in users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        total_trip_n[u] = ct_df.shape[0]\n",
    "        high_confidence_n[u] = 0\n",
    "        mid_confidence_n[u] = 0\n",
    "\n",
    "        total_trip_n_unlabeled[u] = filter_unlabeled(ct_df).shape[0]\n",
    "        high_confidence_n_unlabeled[u] = 0\n",
    "        mid_confidence_n_unlabeled[u] = 0\n",
    "\n",
    "        mid_confidence_any[u] = 0\n",
    "        mid_confidence_any_after[u] = 0\n",
    "\n",
    "        if u in filtered_users:\n",
    "            high_confidence_n_after[u] = 0\n",
    "            high_confidence_n_after_unlabeled[u] = 0\n",
    "            mid_confidence_n_after[u] = 0\n",
    "            mid_confidence_n_after_unlabeled[u] = 0\n",
    "            this_after_start = user_after_start[u]\n",
    "            trips_after = ct_df[(ct_df[\"end_arrow\"] >= this_after_start)]\n",
    "            total_trip_n_after[u] = trips_after.shape[0]\n",
    "            total_trip_n_after_unlabeled[u] = filter_unlabeled(trips_after).shape[0]\n",
    "            ids = []\n",
    "            for _, trip in trips_after.iterrows():\n",
    "                ids.append(trip[\"_id\"])\n",
    "\n",
    "        for _, trip in ct_df.iterrows():\n",
    "            inference = trip[\"inferred_labels\"]\n",
    "            # Here goes a quick and partial reimplementation of the on-the-fly (client-side) inference algorithm\n",
    "            confidences = {}\n",
    "            for label_type in LABEL_CATEGORIES:\n",
    "                counter = {}\n",
    "                for line in inference:\n",
    "                    if label_type not in line[\"labels\"]: continue  # Seems we have some incomplete tuples!\n",
    "                    val = line[\"labels\"][label_type]\n",
    "                    if val not in counter: counter[val] = 0\n",
    "                    counter[val] += line[\"p\"]\n",
    "                confidences[label_type] = max(counter.values()) if len(counter) > 0 else 0 # This needs to be max, not sum!!! A major bug in a previous version.\n",
    "            trip_confidence = min(confidences.values())\n",
    "            all_confidences.append(trip_confidence)\n",
    "            # if (trip_confidence >= 0.01 and trip_confidence <= 0.99): print(trip_confidence)\n",
    "            if trip_confidence > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                high_confidence_n[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                    high_confidence_n_after[u] += 1\n",
    "                    if is_unlabeled(trip): high_confidence_n_after_unlabeled[u] += 1\n",
    "                if is_unlabeled(trip): high_confidence_n_unlabeled[u] += 1\n",
    "            if trip_confidence > LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                mid_confidence_n[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                    mid_confidence_n_after[u] += 1\n",
    "                    if is_unlabeled(trip): mid_confidence_n_after_unlabeled[u] += 1\n",
    "                if is_unlabeled(trip): mid_confidence_n_unlabeled[u] += 1\n",
    "            if max(confidences.values()) > LOW_CONFIDENCE_THRESHOLD_PRODUCTION and trip_confidence <= LOW_CONFIDENCE_THRESHOLD_PRODUCTION:\n",
    "                mid_confidence_any[u] += 1\n",
    "                if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                        mid_confidence_any_after[u] += 1\n",
    "        high_confidence_frac[u] = high_confidence_n[u] / total_trip_n[u]\n",
    "        in_to_label = total_trip_n[u]-high_confidence_n[u]\n",
    "        mid_confidence_frac[u] = (mid_confidence_n[u]-high_confidence_n[u]) / in_to_label if in_to_label != 0 else float(\"NaN\")\n",
    "        \n",
    "    results = {\"high\": {\n",
    "                \"All\": (sum(high_confidence_n_unlabeled.values()), sum(total_trip_n_unlabeled.values())),\n",
    "                \"Before\": (sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_unlabeled.values())-sum(total_trip_n_after_unlabeled.values())),\n",
    "                \"After\": (sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_after_unlabeled.values()))\n",
    "               },\n",
    "               \"mid\": {  # For mid confidence, we subtract the high confidence counts from both numerator and denominator to only capture what's going on in To Label\n",
    "                \"All\": (sum(mid_confidence_n_unlabeled.values())-sum(high_confidence_n_unlabeled.values()), sum(total_trip_n_unlabeled.values())-sum(high_confidence_n_unlabeled.values())),\n",
    "                \"Before\": ((sum(mid_confidence_n_unlabeled.values())-sum(mid_confidence_n_after_unlabeled.values()))-(sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values())), (sum(total_trip_n_unlabeled.values())-sum(total_trip_n_after_unlabeled.values()))-(sum(high_confidence_n_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()))),\n",
    "                \"After\": (sum(mid_confidence_n_after_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()), sum(total_trip_n_after_unlabeled.values())-sum(high_confidence_n_after_unlabeled.values()))\n",
    "               },\n",
    "               \"mid_any\": {\n",
    "                   \"All\": sum(mid_confidence_any.values()),\n",
    "                   \"Before\": sum(mid_confidence_any.values())-sum(mid_confidence_any_after.values()),\n",
    "                   \"After\": sum(mid_confidence_any_after.values())\n",
    "               },\n",
    "               \"all_confidences\": all_confidences}\n",
    "    if g_high_confidence_n_after_unlabeled is None: g_high_confidence_n_after_unlabeled = high_confidence_n_after_unlabeled\n",
    "    return results\n",
    "\n",
    "complete_results = {k: high_stats([u for u in programs[k] if u in server_filtered_users]) for k in programs}\n",
    "print(\"Considering only unlabeled data, we calculate the average percentage of trips users do not need to interact at all with:\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"high\"]:\n",
    "        print(f\"\\t{s}: {format_frac_percent(*complete_results[k]['high'][s])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4360c28",
   "metadata": {},
   "source": [
    "### Trips that would be in To Label with no red labels\n",
    "Same note as above applies here.\n",
    "\n",
    "Numerator is number of trips in To Label with no red labels, denominator is number of trips in To Label at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considering only unlabeled data, we calculate the average percentage of trips that would appear in To Label with no red labels:\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"mid\"]:\n",
    "        print(f\"\\t{s}: {format_frac_percent(*complete_results[k]['mid'][s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09364baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considering only unlabeled data, we calculate the number of trips appearing in To Label with some red labels but with at least yellow labels, just to reassure ourselves that it is a lot:\")\n",
    "for k in complete_results:\n",
    "    print(f\"{k} ({len([u for u in programs[k] if u in server_filtered_users])} users for \\\"all\\\" and \\\"before\\\"; {len([u for u in programs[k] if u in filtered_users])} for \\\"after\\\"):\")\n",
    "    for s in complete_results[k][\"mid_any\"]:\n",
    "        print(f\"\\t{s}: {complete_results[k]['mid_any'][s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1103528",
   "metadata": {},
   "source": [
    "## Results from Dataset 2!\n",
    "To review, we want:\n",
    " * Number of participants\n",
    " * Frequency of app opens\n",
    " * Taps actually avoided by verify button\n",
    " * Taps actually avoided by hiding high confidence trips\n",
    " * Overall taps avoided (total, per trip, percentage of taps)\n",
    " * Fraction of users who used the verify button\n",
    " * Fraction of trips finalized using the verify button\n",
    " * How much To Label is used vs. other tabs of Label vs. Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8bff7",
   "metadata": {},
   "source": [
    "### Number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NUMBER OF USERS IN DATASET 2: {len(filtered_users)}\")\n",
    "print(\"Breakdown by program: \"+str({k: len([u for u in programs[k] if u in filtered_users]) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56b3a3",
   "metadata": {},
   "source": [
    "## Frequency of app opens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequencies(users):\n",
    "    days_before = {}\n",
    "    days_after = {}\n",
    "    opens_before = {}\n",
    "    opens_after = {}\n",
    "    opens_per_day_before = {}\n",
    "    opens_per_day_after = {}\n",
    "    \n",
    "    for u in users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        this_before_start = user_before_start[u]\n",
    "        this_before_end = user_after_start[u]\n",
    "        this_after_start = user_after_start[u]\n",
    "        this_after_end = AFTER_END\n",
    "\n",
    "        days_before[u] = delta2days(this_before_end-this_before_start)\n",
    "        days_after[u] = delta2days(this_after_end-this_after_start)\n",
    "\n",
    "        opens = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] == \"opened_app\")].copy()\n",
    "        opens[\"ts_arrow\"] = opens[\"ts\"].apply(arrow.get)\n",
    "        opens_before[u] = filter_between(opens, \"ts_arrow\", this_before_start, this_before_end).shape[0]\n",
    "        opens_after[u] = filter_between(opens, \"ts_arrow\", this_after_start, this_after_end).shape[0]\n",
    "\n",
    "        opens_per_day_before[u] = opens_before[u]/days_before[u]\n",
    "        opens_per_day_after[u] = opens_after[u]/days_after[u]\n",
    "    \n",
    "    print(\"Everybody in Dataset 2:\")\n",
    "    output_results(users, opens_per_day_before, opens_per_day_after, opens_before, opens_after)\n",
    "    \n",
    "    opens_filtered_users = [u for u in users if opens_before[u] >= REQUIRED_OPENS_BEFORE and opens_after[u] >= REQUIRED_OPENS_AFTER and opens_before[u]+opens_after[u] >= REQUIRED_OPENS_TOTAL]\n",
    "    print(f\"\\nOnly those with >={REQUIRED_OPENS_TOTAL} opens total, >={REQUIRED_OPENS_BEFORE} opens Before, >={REQUIRED_OPENS_AFTER} opens After:\")\n",
    "    output_results(opens_filtered_users, opens_per_day_before, opens_per_day_after, opens_before, opens_after)\n",
    "    \n",
    "def output_results(users, opens_per_day_before, opens_per_day_after, opens_before, opens_after, do_breakdown=True):\n",
    "    opens_per_day_before, opens_per_day_after, opens_before, opens_after = map(lambda d: {k: d[k] for k in d if k in users}, [opens_per_day_before, opens_per_day_after, opens_before, opens_after])\n",
    "    n_users = len(users)\n",
    "    print(\"App opens per day before->after, total opens before+after:\")\n",
    "    print(\"SUM:\")\n",
    "    print(format_arrow_comma(sum(opens_per_day_before.values()), sum(opens_per_day_after.values()), sum(opens_before.values())+sum(opens_after.values())))\n",
    "    print(\"AVERAGE:\")\n",
    "    if n_users > 0:\n",
    "        print(format_arrow_comma(sum(opens_per_day_before.values())/n_users, sum(opens_per_day_after.values())/n_users, (sum(opens_before.values())+sum(opens_after.values()))/n_users))\n",
    "    else: print(\"N/A\")\n",
    "    if not do_breakdown: return\n",
    "    print(\"User breakdown:\")\n",
    "    if n_users > 0:\n",
    "        for u in users:\n",
    "            print(format_arrow_comma(opens_per_day_before[u], opens_per_day_after[u], opens_before[u]+opens_after[u]))\n",
    "    else: print(\"N/A\")\n",
    "    \n",
    "compute_frequencies(filtered_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743c010",
   "metadata": {},
   "source": [
    "### Taps actually avoided by verify button\n",
    "### Taps actually avoided by hiding high confidence trips\n",
    "### Overall taps avoided (total, per trip, percentage of taps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa312c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at between 10s and 30s on an early-2015 MacBook Air\n",
    "# Whether or not a press of the verify button fully labels a trip\n",
    "def verify_fully_labels(event):\n",
    "    if not event[\"reading\"][\"verifiable\"]: return False  # Forgot about this case until working with real pilot program data...\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "    final_inference = json.loads(event[\"reading\"][\"finalInference\"])\n",
    "    return len(user_input) < len(LABEL_CATEGORIES) and len(set(user_input.keys()) | set(final_inference.keys())) == len(LABEL_CATEGORIES)\n",
    "\n",
    "# Whether or not a given label dropdown menu selection fully labels a trip\n",
    "def label_fully_labels(event):\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "    return len(user_input) == len(LABEL_CATEGORIES)-1 and event[\"reading\"][\"inputKey\"] not in event[\"reading\"][\"userInput\"]\n",
    "\n",
    "verifieds = {}\n",
    "taps = {}\n",
    "trips_labeled = {}\n",
    "taps_avoided = {}\n",
    "taps_avoided_per_trip = {}\n",
    "verifiers = set()\n",
    "\n",
    "for u in filtered_users:\n",
    "    trips_labeled[u] = 0\n",
    "    verifieds[u] = 0\n",
    "    # TODO: maybe only consider unlabeled-at-time-of-inference-generation trips here?\n",
    "    verify_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"verify_trip\")]\n",
    "    if len(verify_events) > 0: verifiers.add(u)\n",
    "    label_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == (u if not TESTINGMODE else gooduser)) & (stats[\"time\"][\"name\"] == \"select_label\")]\n",
    "    taps[u] = len(verify_events)+2*len(label_events)\n",
    "    # The testing user seems to have an unusually high number of mistaps. When crunching real data, we will let this count against taps saved,\n",
    "    # but to have useful data to debug the sensitivity analysis that appears later, let's artificially assume that 1 in 10 label_events is a mistap and ignore those.\n",
    "    if TESTINGMODE: taps[u] = len(verify_events)+(1-0.10)*(2*len(label_events))\n",
    "    if verify_events.shape[0] > 0:\n",
    "        for _, ve in verify_events.iterrows():\n",
    "            if verify_fully_labels(ve):\n",
    "                verifieds[u] += 1\n",
    "                trips_labeled[u] += 1\n",
    "    if label_events.shape[0] > 0:\n",
    "        for _, le in label_events.iterrows():\n",
    "            if label_fully_labels(le): trips_labeled[u] += 1\n",
    "    taps_avoided[u] = OLD_TAPS*trips_labeled[u]-taps[u]\n",
    "    taps_avoided_per_trip[u] = taps_avoided[u]/trips_labeled[u] if trips_labeled[u] != 0 else float(\"NaN\")\n",
    "    # print(f\"User tapped {taps[u]} times, avoided {taps_avoided[u]} taps to label {trips_labeled[u]} trips\")\n",
    "\n",
    "def print_tap_summary(users):\n",
    "    total_taps = sum(fu(taps, users).values())\n",
    "    total_taps_avoided = sum(fu(taps_avoided, users).values()) \n",
    "    total_trips_labeled = sum(fu(trips_labeled, users).values())\n",
    "    avoided_per_labeled = total_taps_avoided/total_trips_labeled\n",
    "    print(f\"Overal, users tapped {total_taps} times to label {total_trips_labeled} trips.\")\n",
    "    print(f\"Overall, {total_taps_avoided} taps were avoided, {avoided_per_labeled:.2f} per trip -- that's {avoided_per_labeled/OLD_TAPS:.2%} of taps\")\n",
    "    print(f\"We also saved {OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled, users).values())} taps across {sum(fu(g_high_confidence_n_after_unlabeled).values())} trips by not soliciting user input on very confident trips\")\n",
    "\n",
    "    total_taps_avoided_high = total_taps_avoided+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled, users).values())\n",
    "    total_trips_labeled_high = total_trips_labeled+sum(fu(g_high_confidence_n_after_unlabeled, users).values())\n",
    "    avoided_per_labeled_high = total_taps_avoided_high/total_trips_labeled_high\n",
    "    print(f\"If we also count the taps we avoided by not putting high-confidence inferences on the To Label screen:\")\n",
    "    print(f\"Overall, {total_taps_avoided_high} taps were avoided across {total_trips_labeled+sum(fu(g_high_confidence_n_after_unlabeled, users).values())} trips, {avoided_per_labeled_high:.2f} per trip -- that's {avoided_per_labeled_high/OLD_TAPS:.2%} of taps\")\n",
    "\n",
    "print_tap_summary(filtered_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d094b76",
   "metadata": {},
   "source": [
    "### Fraction of users who used the verify button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5296be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Users who have used the verify button at least once: \"+str({k: str(len([u for u in programs[k] if u in filtered_users and u in verifiers]))+\"/\"+str(len([u for u in programs[k] if u in filtered_users])) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9527f7",
   "metadata": {},
   "source": [
    "### Fraction of trips finalized using the verify button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29157fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of trips finalized using the verify button as a fraction of total number of user-confirmed trips:\\n\"+str({k: format_frac_percent(sum([verifieds[u] for u in programs[k] if u in filtered_users]), sum([trips_labeled[u] for u in programs[k] if u in filtered_users])) for k in programs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8c0f9",
   "metadata": {},
   "source": [
    "### How much To Label is used vs. other tabs of Label vs. Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d099c2b",
   "metadata": {},
   "source": [
    "First, let's (re)aquaint ourselves with what the instrumentation data can provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_instrumentation(u):\n",
    "    print(stats.keys())\n",
    "\n",
    "    nav_stats = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] != \"sync_launched\")].copy()\n",
    "    # print(nav_stats.shape)\n",
    "    # print(nav_stats.keys())\n",
    "    # print(nav_stats.head()[[\"name\",\"reading\",\"ts\"]])\n",
    "    # print(nav_stats[[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "\n",
    "    time_stats = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] != \"push_duration\") & (stats[\"time\"][\"name\"] != \"pull_duration\") & (stats[\"time\"][\"name\"] != \"sync_duration\")].copy()\n",
    "    # print(time_stats.shape)\n",
    "    # print(time_stats.keys())\n",
    "    # print(time_stats.head()[[\"name\",\"reading\",\"ts\"]])\n",
    "    # print(time_stats[[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "    # print(time_stats[time_stats[\"name\"] == \"label_tab_switch\"][[\"name\",\"reading\",\"ts\"]].to_csv())\n",
    "\n",
    "\n",
    "explore_instrumentation(filtered_users[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903c437",
   "metadata": {},
   "source": [
    "Based on the results of this brief investigation, it seems that it would be hard to measure time spent on To Label vs. the other screens because the stats I have don't seem to be accurately monitoring when the app stops being used. This should be revisited in the future, though -- there are probably some other stats elsewhere that could help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d5eb0",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level(logging.WARNING)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1142e30",
   "metadata": {},
   "source": [
    "### Weekly labeling percentage over time for each program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while -- clocked at 18s on an early-2015 MacBook Air\n",
    "# This cell rather clumsily calculates per-week labeling statistics for each user and each program.\n",
    "# TODO could use some major refactoring\n",
    "\n",
    "def filter_before_or_to_label(df, user):\n",
    "    # empty = (df[\"end_arrow\"] < user_after_start[user]) & (df[\"end_arrow\"] > user_after_start[user])\n",
    "    # (df[\"end_arrow\"] < user_after_start[user])\n",
    "    if user not in filter2_users: return df  # Everything is before update if you haven't installed the update! TODO filter2_users was not meant to be referred to so very globally\n",
    "    return df[(df[\"end_arrow\"] < user_after_start[user]) | (df[\"expectation\"].apply(lambda val: \"to_label\" in val and val[\"to_label\"]))]\n",
    "\n",
    "# This could almost certainly be done more efficiently, but it's not worth worrying about right now\n",
    "def count_weekly(users):\n",
    "    total_count_weekly = {}\n",
    "    labeled_count_weekly = {}\n",
    "    labeling_frac_weekly = {}\n",
    "    total_count_weekly_u = {}\n",
    "    labeled_count_weekly_u = {}\n",
    "    labeling_frac_weekly_u = {}\n",
    "    for u in users:\n",
    "            total_count_weekly_u[u] = {}\n",
    "            labeled_count_weekly_u[u] = {}\n",
    "            labeling_frac_weekly_u[u] = {}\n",
    "    for program in programs.keys():\n",
    "        total_count_weekly[program] = {}\n",
    "        labeled_count_weekly[program] = {}\n",
    "        labeling_frac_weekly[program] = {}\n",
    "        for week in weeks:\n",
    "            this_total_count = 0\n",
    "            this_labeled_count = 0\n",
    "            for u in users:\n",
    "                if u in programs[program]:\n",
    "                    if programs == \"stage\": print(u)\n",
    "                    total = filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week)\n",
    "                    total = filter_before_or_to_label(total, u)\n",
    "                    fully_labeled = total[(total[\"user_input\"].apply(len) == len(LABEL_CATEGORIES))]\n",
    "                    this_total_count += total.shape[0]\n",
    "                    this_labeled_count += fully_labeled.shape[0]\n",
    "\n",
    "                    total_count_weekly_u[u][week] = total.shape[0]\n",
    "                    labeled_count_weekly_u[u][week] = fully_labeled.shape[0]\n",
    "                    labeling_frac_weekly_u[u][week] = labeled_count_weekly_u[u][week]/total_count_weekly_u[u][week] if total_count_weekly_u[u][week] != 0 else float(\"nan\")\n",
    "            total_count_weekly[program][week] = this_total_count\n",
    "            labeled_count_weekly[program][week] = this_labeled_count\n",
    "            labeling_frac_weekly[program][week] = this_labeled_count/this_total_count if this_total_count != 0 else float(\"nan\")\n",
    "    return total_count_weekly, labeled_count_weekly, labeling_frac_weekly, total_count_weekly_u, labeled_count_weekly_u, labeling_frac_weekly_u\n",
    "\n",
    "total_count_weekly, labeled_count_weekly, labeling_frac_weekly, _, _, _ = count_weekly(filtered_users)\n",
    "total_count_weekly_ds1, labeled_count_weekly_ds1, labeling_frac_weekly_ds1, total_count_weekly_u, labeled_count_weekly_u, labeling_frac_weekly_u = count_weekly(server_filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many of our users have installed the update at the end of a given week\n",
    "def count_users_weekly(users):\n",
    "    updated_users_weekly = {}\n",
    "    denom = {}\n",
    "    for program in programs:\n",
    "        updated_users_weekly[program] = {}\n",
    "        denom[program] = {}\n",
    "        for week in weeks:\n",
    "            updated_users_weekly[program][week] = 0\n",
    "            denom[program][week] = 0\n",
    "            for u in users:\n",
    "                if u in programs[program]:\n",
    "                    if filter_between(confirmed_trip_df_map[u], \"end_arrow\", *week).shape[0] > 0:\n",
    "                        denom[program][week] += 1\n",
    "                        if u in user_after_start and user_after_start[u] < week[1]: updated_users_weekly[program][week] += 1\n",
    "        \n",
    "        for week in weeks:\n",
    "            n = updated_users_weekly[program][week]\n",
    "            # labeled = labeling_frac_weekly[program][week] #NaN-ify points where there were no labeled trips\n",
    "            updated_users_weekly[program][week] = n/denom[program][week] if denom[program][week] != 0 else float(\"NaN\")\n",
    "    return updated_users_weekly\n",
    "\n",
    "updated_users_weekly = count_users_weekly(filtered_users)\n",
    "updated_users_weekly_ds1 = count_users_weekly(server_filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3741d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of days users have used the app per week\n",
    "def count_days_per_week(users):\n",
    "    days_per_week_u = {u: {} for u in users}\n",
    "    for u in users:\n",
    "        time_stats = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] != \"push_duration\") & (stats[\"time\"][\"name\"] != \"pull_duration\") & (stats[\"time\"][\"name\"] != \"sync_duration\")]\n",
    "        nav_stats = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] != \"sync_launched\")].copy()\n",
    "        # print(pd.unique(nav_stats[\"name\"]))\n",
    "        switches = nav_stats[nav_stats[\"name\"] == \"opened_app\"].copy()\n",
    "        switches[\"ts_arrow\"] = switches[\"ts\"].apply(lambda ts: arrow.get(ts).to(MY_TZ))\n",
    "        for week in weeks:\n",
    "            these_switches = filter_between(switches, \"ts_arrow\", *week)\n",
    "            days_per_week_u[u][week] = pd.unique(these_switches[\"ts_arrow\"].apply(lambda ts: (ts-BEFORE_START).days)).shape[0]\n",
    "        # print(switches[\"ts_arrow\"])\n",
    "    return days_per_week_u\n",
    "\n",
    "days_per_week_u = count_days_per_week(server_filtered_users)\n",
    "# days_per_week_u = count_days_per_week([filtered_users[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341210b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_labels = [week[0].datetime for week in weeks]  # f\"{week[0].month}/{week[0].day}\"\n",
    "\n",
    "def setup_weekly_axis(ax):\n",
    "    ax.xaxis_date()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m/%d\"))\n",
    "    ax.set_xticks(week_labels)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    plt.setp(ax.get_xticklabels(), visible=True)\n",
    "    ax.tick_params(labelbottom=True)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(15,8), gridspec_kw = {\"height_ratios\": [1, 3]}, sharex=True)\n",
    "for ax in axs:\n",
    "    setup_weekly_axis(ax)\n",
    "\n",
    "for program in programs_some:\n",
    "    n = len([u for u in programs[program] if u in filtered_users])\n",
    "    if n == 0: continue\n",
    "    label = f\"{program if program != 'ens' else 'ensemble'} (n={n})\"\n",
    "    lines = []\n",
    "    lines.append(axs[1].plot(week_labels, list(labeling_frac_weekly[program].values()), label=label, zorder = 1 if program == \"ens\" else 0)[0])\n",
    "    lines.append(axs[0].plot(week_labels, list(updated_users_weekly[program].values()), label=label, zorder = 1 if program == \"ens\" else 0)[0])\n",
    "    for line in lines:\n",
    "        if program == \"ens\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "\n",
    "axs[0].set_ylabel(\"% installed update\")\n",
    "axs[1].set_ylabel(\"% of expected trips labeled\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f171db4",
   "metadata": {},
   "source": [
    "So that's not entirely encouraging, but it's also kind of hard to tell what's going on. Let's try something else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8e76a",
   "metadata": {},
   "source": [
    "### Weekly labeling but it's Vail and Pueblo County vs. all the others\n",
    "(and we skip the first week and only do four weeks after that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the weeks we care about. TODO make this not dependent on starting date!!!\n",
    "first_start = 7\n",
    "first_end = 11\n",
    "second_start = 19\n",
    "second_end = 23\n",
    "for i in (first_start, first_end, second_start, second_end): print(weeks[i])\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(15,8), gridspec_kw = {\"height_ratios\": [1, 3]}, sharex=True)\n",
    "for ax in axs:\n",
    "    setup_weekly_axis(ax)\n",
    "\n",
    "for program in programs_all:\n",
    "    if program == \"ens\": continue\n",
    "    n = len([u for u in programs[program] if u in server_filtered_users])\n",
    "    if n == 0: continue\n",
    "\n",
    "    if program == \"before_after\": label = \"Before/after ensemble\"\n",
    "    elif program == \"only_after\": label = \"Only after ensemble\"\n",
    "    else: label = program\n",
    "    label += f\" n={n}\"\n",
    "\n",
    "    y1 = list(labeling_frac_weekly_ds1[program].values())\n",
    "    y2 = list(updated_users_weekly_ds1[program].values())\n",
    "    not_in_first = lambda x: x < first_start or x >= first_end\n",
    "    not_in_second = lambda x: x < second_start or x >= second_end\n",
    "    for i in range(len(weeks)):  # NaN out the data we don't care about\n",
    "        if program in [\"only_after\", *only_after_programs]:\n",
    "            if not_in_second(i): y1[i] = y2[i] = float(\"NaN\")\n",
    "        else:\n",
    "            if not_in_first(i) and not_in_second(i): y1[i] = y2[i] = float(\"NaN\")\n",
    "    lines = []\n",
    "    lines.append(axs[1].plot(week_labels, y1, label=label, zorder = 1 if program in (\"before_after\", \"only_after\") else 0)[0])\n",
    "    lines.append(axs[0].plot(week_labels, y2, label=label, zorder = 1 if program in (\"before_after\", \"only_after\") else 0)[0])\n",
    "    for line in lines:\n",
    "        if program == \"before_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "        elif program == \"only_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "            line.set_linestyle(\"dashed\")\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"% installed update\")\n",
    "axs[1].set_ylabel(\"% of expected trips labeled\")\n",
    "\n",
    "axs[1].legend(loc=\"center\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152ea84",
   "metadata": {},
   "source": [
    "Let's do a second version of that with three sections:\n",
    "  1. Before/after group when they've just started (no update)\n",
    "  2. Before/after group just after they install the update (meaning calculate an offset for each user) (100% update)\n",
    "  3. Only after group when they've just started (100% update)\n",
    "\n",
    "and then I suppose we could have a fourth plot with the three ensembles from that.\n",
    "\n",
    "Then, let's use the same type of graph to figure out whether the update has any effect on labeling cadence. If so, that might be a sign of reduction (or increase) in burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_of_before_start = {}  # Index of week that user installed the app to begin with\n",
    "for u in server_filtered_users:\n",
    "    for i, week in enumerate(weeks):\n",
    "        if user_before_start[u] >= week[0] and user_before_start[u] < week[1]:\n",
    "            week_of_before_start[u] = i\n",
    "            break\n",
    "\n",
    "week_of_start = {}  # Index of week that user installed the update\n",
    "for u in user_after_start:\n",
    "    for i, week in enumerate(weeks):\n",
    "        if user_after_start[u] >= week[0] and user_after_start[u] < week[1]:\n",
    "            assert u not in week_of_start\n",
    "            week_of_start[u] = i\n",
    "# print(week_of_start)\n",
    "\n",
    "def weekly_average_per_user_offset(tseries, users, start_func, n_weeks, valid_func = lambda user, week: True):\n",
    "    augmented_weeks = weeks+[None]*n_weeks  # Account for overflow\n",
    "    per_user = []  # This could all be written as a massive comprehension; wouldn't that be fun.\n",
    "    for user in users:\n",
    "        start = start_func(user)\n",
    "        user_weeks = [augmented_weeks[i] if valid_func(user, i) else None for i in range(start, start+n_weeks)]\n",
    "        user_results = [tseries[user][week] if week != None else float(\"nan\") for week in user_weeks]\n",
    "        assert len(user_results) == n_weeks, len(user_results)\n",
    "        per_user.append(user_results)\n",
    "    return np.nanmean(per_user, axis=0)\n",
    "\n",
    "# print(weekly_average_per_user_offset(labeling_frac_weekly_u, filtered_users, lambda u: 5, 4))\n",
    "\n",
    "def style_lines(program, lines):\n",
    "    for line in lines:\n",
    "        if program == \"before_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "        elif program == \"only_after\":\n",
    "            line.set_color(\"black\")\n",
    "            line.set_linewidth(3)\n",
    "            # line.set_linestyle(\"dashed\")\n",
    "\n",
    "def plot_labeling_breakdown_v2(tseries, y_percent):\n",
    "    labels = {program: \"ensemble\" if program == \"before_after\" else \"ensemble\" if program == \"only_after\" else program for program in programs_all}\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(22,8))\n",
    "    if y_percent: plt.subplots_adjust(wspace=0.3)\n",
    "    x = [1.5, 2.5, 3.5, 4.5]\n",
    "    axs[0].set_title(\"Before/after group at initial installation\")\n",
    "    axs[1].set_title(\"Before/after group at updating\")\n",
    "    axs[2].set_title(\"Only after group at installation\")\n",
    "    axs[3].set_title(\"Inter-scenario comparison\")\n",
    "    for ax in axs:\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "        ax.set_xlabel(\"Weeks after event\")\n",
    "        ax.set_xlim([1, 5])\n",
    "        if (y_percent):\n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.set_ylabel(\"% of expected trips labeled\")\n",
    "        else:\n",
    "            ax.set_ylim([0, 7])\n",
    "            ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "            ax.set_ylabel(\"Number of days per week users open app\")\n",
    "    \n",
    "    ensembles = []\n",
    "\n",
    "    # Actual plotting #1\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program in [\"ens\", \"stage\", \"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in server_filtered_users and u in tseries and u in week_of_before_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No before users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_before_start[u], 4, valid_func = lambda u, week: u not in week_of_start or week < week_of_start[u])\n",
    "        if program == \"before_after\": ensembles.append(y)\n",
    "        lines.append(axs[0].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "    \n",
    "    # Actual plotting #2\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program in [\"ens\", \"stage\", \"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in week_of_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No after users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_start[u], 4)\n",
    "        if program == \"before_after\": ensembles.append(y)\n",
    "        lines.append(axs[1].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "\n",
    "    # Actual plotting #3\n",
    "    lines = []\n",
    "    for program in programs_all:\n",
    "        if program not in [\"only_after\", *only_after_programs]: continue\n",
    "        users = [u for u in programs[program] if u in week_of_start]\n",
    "        if len(users) == 0:\n",
    "            print(f\"No after users for program: {program}\")\n",
    "            continue\n",
    "        y = weekly_average_per_user_offset(tseries, users, lambda u: week_of_start[u], 4)\n",
    "        if program == \"only_after\": ensembles.append(y)\n",
    "        lines.append(axs[2].plot(x, y, label=labels[program]+f\" n={len(users)}\", zorder = 1 if program == \"before_after\" else 0)[0])\n",
    "        style_lines(program, lines)\n",
    "    \n",
    "    # Actual plotting #4\n",
    "    for i in range(len(ensembles)):\n",
    "        axs[3].plot(x, ensembles[i], label=axs[i].get_title(), linewidth=3)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.legend(loc = (\"lower left\" if y_percent else \"best\"))\n",
    "    \n",
    "plot_labeling_breakdown_v2(labeling_frac_weekly_u, True)\n",
    "plot_labeling_breakdown_v2(days_per_week_u, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc35c0",
   "metadata": {},
   "source": [
    "Let's do a histogram of expectation confidences. To usefully display the data, we will have to constrain the y-axis such that some bars (e.g., the first) cannot fully display, so we annotate the graph with information about these bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 100\n",
    "# y_step = 250\n",
    "# allowed_overflow = 1\n",
    "max_y = 1700  # TODO determine this algorithmically (for now, tune it manually per dataset)\n",
    "if TESTINGMODE: max_y = 20\n",
    "\n",
    "all_confidences = complete_results[\"ens\"][\"all_confidences\"].copy()\n",
    "all_confidences.sort()\n",
    "# from collections import Counter\n",
    "# conf_count = Counter(all_confidences)\n",
    "# print(sorted(conf_count.values(), reverse=True)[:4])\n",
    "# max_y = sorted(conf_count.values(), reverse=True)[allowed_overflow]  # fails to take into account binning\n",
    "# print(max_y)\n",
    "# max_y = np.ceil(max_y/y_step)*y_step\n",
    "# print(max_y)\n",
    "\n",
    "bins = np.arange(0, 1+0.1/n_bins, 1/n_bins)\n",
    "assert len(bins) == n_bins+1\n",
    "\n",
    "range1 = [x for x in all_confidences if x <= LOW_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "range2 = [x for x in all_confidences if x > LOW_CONFIDENCE_THRESHOLD_PRODUCTION and x <= HIGH_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "range3 = [x for x in all_confidences if x > HIGH_CONFIDENCE_THRESHOLD_PRODUCTION]\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15,8))\n",
    "p1 = plt.hist(range1, bins=bins, label=f\"User input required, 1 or more red labels (total of {len(range1)} trips)\")\n",
    "p2 = plt.hist(range2, bins=bins, label=f\"User input required, all yellow labels (total of {len(range2)} trips)\")\n",
    "p3 = plt.hist(range3, bins=bins, label=f\"No user input required (total of {len(range3)} trips)\")\n",
    "ax.set_ylim([0, max_y])\n",
    "ax.set_xlim([0, 1])\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(250))\n",
    "if TESTINGMODE: ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "\n",
    "ax.set_title(\"Histogram of trip confidence, segmented by presentation to user\")\n",
    "ax.set_ylabel(\"Number of trips\")\n",
    "ax.set_xlabel(\"Confidence level of final inference\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Mark overflows on the graph\n",
    "for p in (p1, p2, p3):\n",
    "    for i, bar in enumerate(zip(*p)):\n",
    "        rect = bar[2]\n",
    "        height = int(rect.get_height())\n",
    "        if height > max_y:\n",
    "            ax.text(rect.get_x()+rect.get_width()*1.1, max_y*0.99, f\"{height}\", ha=\"left\", va=\"top\")\n",
    "            print(f\"Overflow note: the bar for the region [{bins[i]:.1%}, {bins[i+1]:.1%}{']' if i == len(bins)-1 else ')'} contains {height} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48021b3",
   "metadata": {},
   "source": [
    "Debugging break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was a bug in the confidence algorithm; I used this code to figure it out.\n",
    "def explore_confidences(u):\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    print(ct_df.shape)\n",
    "    # \"\"\"\n",
    "    for i, trip in ct_df.iterrows():\n",
    "        if i != 179: continue\n",
    "        inference = trip[\"inferred_labels\"]\n",
    "        if len(inference) > 0:\n",
    "            for option in inference:\n",
    "                print(f\"{len(option['labels'])}, {option['p']:.2f}\", end=\"; \")\n",
    "            print(i)\n",
    "\n",
    "        confidences = {}\n",
    "        for label_type in LABEL_CATEGORIES:\n",
    "            print(label_type)\n",
    "            counter = {}\n",
    "            for line in inference:\n",
    "                if label_type not in line[\"labels\"]: continue\n",
    "                val = line[\"labels\"][label_type]\n",
    "                if val not in counter: counter[val] = 0\n",
    "                counter[val] += line[\"p\"]\n",
    "            print(counter)\n",
    "            confidences[label_type] = max(counter.values())  # THIS WAS SUM BEFORE!!! THAT WAS THE PROBLEM!!!\n",
    "        print(\"CONFIDENCES:\")\n",
    "        print(confidences)\n",
    "        trip_confidence = min(confidences.values())\n",
    "        print(trip_confidence)\n",
    "    # \"\"\"\n",
    "    # print(ct_df.iloc[338][\"inferred_labels\"])\n",
    "    print(ct_df.iloc[179][\"inferred_labels\"])\n",
    "\n",
    "# explore_confidences(filtered_users[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b203b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a94c8db",
   "metadata": {},
   "source": [
    "Let's do the weekly labeling thing but per-user, and let's line things up so everyone installs the update at the same point on the graph. We do this first as a scatterplot; then, we see if it might be clearer as a bar graph displaying three-week averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51383fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apologies for the rather awful code to follow. TODO at some point this should be cleaned up.\n",
    "aligned_labeling_frac_u = {}\n",
    "for u in week_of_start.keys():  # It's possible not everyone in filtered_users has a week_of_start within the weeks we're graphing, in which case drop that user\n",
    "    aligned_labeling_frac_u[u] = {}\n",
    "    for i, week in enumerate(weeks):\n",
    "        aligned_labeling_frac_u[u][i-week_of_start[u]] = labeling_frac_weekly_u[u][week]\n",
    "# aligned_labeling_frac_u = {1: {-8: 0, 0: 0}, 2: {-6: 0, 0: 0}, 3: {0: 0, 0: 0}, 4: {3: 0, 0: 0}, 5: {5: 0, 0: 0}, 6: {7: 0, 0: 0}}\n",
    "# min_offset = min([min(a.keys()) for a in aligned_labeling_frac_u.values()])\n",
    "# max_offset = max([max(a.keys()) for a in aligned_labeling_frac_u.values()])\n",
    "min_offset, max_offset = map(lambda f: (f([f(a.keys()) for a in aligned_labeling_frac_u.values()]) if len(aligned_labeling_frac_u) > 0 else float(\"nan\")), (min, max))\n",
    "min_offset, max_offset = (lambda x: (-x,x))(min(-min_offset, max_offset))  # fun fun\n",
    "x = np.array(range(min_offset, max_offset+1)) if len(aligned_labeling_frac_u) > 0 else []\n",
    "\n",
    "wk_u_y = {u: [aligned_labeling_frac_u[u][i] if i in aligned_labeling_frac_u[u] else float(\"nan\") for i in x] for u in week_of_start.keys()}\n",
    "wk_ens_u = [np.nanmean([wk_u_y[u][i] for u in week_of_start.keys()]) for i in range(len(x))]\n",
    "# print(wk_ens_u[x.index(3)])\n",
    "# print(wk_u_y[server_filtered_users[5]])\n",
    "# print([wk_u_y[u][x.index(3)] for u in week_of_start.keys()])\n",
    "\n",
    "def plot_weekly_per_user_line():\n",
    "    fig, ax = plt.subplots(1, figsize=(15,8))\n",
    "    ax.set_xlim([min_offset, max_offset])\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    for u in week_of_start.keys():\n",
    "        ax.scatter(x, wk_u_y[u])\n",
    "        ax.plot(x, wk_u_y[u], alpha=0.25)\n",
    "\n",
    "    line = ax.plot(x, wk_ens_u)[0]\n",
    "    line.set_color(\"black\")\n",
    "    line.set_linewidth(3)\n",
    "\n",
    "def plot_weekly_per_user_bar():\n",
    "    x2 = np.array([-9, -6, -3, 0, 3, 6, 9])\n",
    "    y2 = {u: [] for u in week_of_start.keys()}\n",
    "    for i in range(len(x2)-1):\n",
    "        xstart = list(x).index(x2[i])\n",
    "        xend = list(x).index(x2[i+1])  # If you are reading this code, my condolences\n",
    "        for u in week_of_start.keys():\n",
    "            # print(wk_u_y[u][xstart:xend])\n",
    "            y2[u].append(np.mean(wk_u_y[u][xstart:xend]))\n",
    "    x2 = x2[:-1]\n",
    "    # print(y2[server_filtered_users[5]])\n",
    "\n",
    "    width = 3/len(week_of_start.keys())*0.9\n",
    "    fig, ax = plt.subplots(1, figsize=(20,8))\n",
    "    ax.set_xlim([-9, 9])\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(3))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.xaxis.grid(True, color=\"black\", linewidth=2, linestyle=\"dotted\")\n",
    "\n",
    "    for i, u in enumerate(week_of_start.keys()):\n",
    "        ax.bar(x2+width*i, y2[u], width)\n",
    "\n",
    "    ax.plot(x, wk_ens_u, color=\"black\", linewidth=3)\n",
    "\n",
    "def plot_weekly_per_user():\n",
    "    if TESTINGMODE: return  # This just isn't going to work in TESTINGMODE\n",
    "    plot_weekly_per_user_line()\n",
    "    plot_weekly_per_user_bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8928f",
   "metadata": {},
   "source": [
    "Let's construct an infographic visualizing how we eliminated taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stacked_bars(title, labels, scenarios, max_taps, width=6, hook=None):\n",
    "    user_burden = [max_taps-scenario[0]-scenario[1] for scenario in scenarios]\n",
    "    due_to_confirm = [scenario[0] for scenario in scenarios]\n",
    "    due_to_expectations = [scenario[1] for scenario in scenarios]\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(width,6))\n",
    "    p1 = ax.bar(labels, user_burden, bottom = list(map(lambda x: x[0]+x[1], zip(due_to_expectations, due_to_confirm))), label = \"User burden\")\n",
    "    p2 = ax.bar(labels, due_to_confirm, bottom = due_to_expectations, label = \"Eliminated due to confirm button\")\n",
    "    p3 = ax.bar(labels, due_to_expectations, label = \"Eliminated due to expectations\")\n",
    "    # Label user burden and all nonzero eliminations\n",
    "    rects_to_label = list(p1)+[r for i, r in enumerate(p2) if due_to_confirm[i] != 0]+[r for i, r in enumerate(p3) if due_to_expectations[i] != 0]\n",
    "    for rect in rects_to_label:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2, rect.get_y()+rect.get_height()/2, f\"{height:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    ax.legend()  # loc=\"upper left\"\n",
    "    ax.set_ylabel(\"Average taps per user-labeled or confidently auto-labeled trip\")\n",
    "    ax.set_title(title)\n",
    "    if hook is not None: hook(ax)\n",
    "    print()\n",
    "\n",
    "stacked_denom = (sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "saved_due_to_confirm = sum(fu(taps_avoided).values())/stacked_denom  # Usually we take the denominator for this number to be only trips in To Label, but here it has to be all user- or confidently auto-labeled trips\n",
    "saved_due_to_expectations = (OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/stacked_denom\n",
    "np.testing.assert_almost_equal(saved_due_to_confirm+saved_due_to_expectations, ((sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values()))))\n",
    "draw_stacked_bars(\"Reducing user burden without sacrificing data quality\", [\"Before\", \"After\"], [(0, 0), (saved_due_to_confirm, saved_due_to_expectations)], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4ad1f",
   "metadata": {},
   "source": [
    "## Numbers of Note\n",
    "Here is an attempt to put all the numbers I actually use in the paper in one place. All of these numbers should be merely restating what is above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5859ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tprint(label, value):  # Tabularly print\n",
    "    print(label.ljust(35, ' ')+\" \"+str(value).rjust(20, ' '))\n",
    "\n",
    "tprint(\"Size of Dataset 1\", len(server_filtered_users))\n",
    "tprint(\"Size of Dataset 2\", len(filtered_users))\n",
    "print()\n",
    "\n",
    "tprint(\"% need not label ensemble all\", format_frac_percent(*complete_results[\"ens\"][\"high\"][\"All\"]))\n",
    "tprint(\"% To Label all yellow ens all\", format_frac_percent(*complete_results[\"ens\"][\"mid\"][\"All\"]))\n",
    "tprint(\"% need not label ens after\", format_frac_percent(*complete_results[\"ens\"][\"high\"][\"After\"]))\n",
    "tprint(\"% To Label all yellow ens after\", format_frac_percent(*complete_results[\"ens\"][\"mid\"][\"After\"]))\n",
    "print()\n",
    "\n",
    "tprint(\"# taps after\", sum(fu(taps).values()))\n",
    "tprint(\"# trips labeled after\", sum(fu(trips_labeled).values()))\n",
    "tprint(\"# taps saved due to Confirm\", sum(fu(taps_avoided).values()))\n",
    "tprint(\"taps saved per trip due to Confirm\", f\"{sum(fu(taps_avoided).values())/sum(fu(trips_labeled).values()):.2f}\")\n",
    "tprint(\"% taps saved due to Confirm\", f\"{sum(fu(taps_avoided).values())/sum(fu(trips_labeled).values())/OLD_TAPS:.2%}\")\n",
    "tprint(\"# of users who used Confirm\", len([u for u in programs[\"ens\"] if u in filtered_users and u in verifiers]))\n",
    "tprint(\"% of trips finalized using Confirm\", format_frac_percent(sum([verifieds[u] for u in programs[\"ens\"] if u in filtered_users]), sum([trips_labeled[u] for u in programs[\"ens\"] if u in filtered_users])))\n",
    "print()\n",
    "\n",
    "tprint(\"# trips not in To Label\", sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "tprint(\"# taps saved due to To Label\", OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "tprint(\"# taps saved total\", sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))\n",
    "tprint(\"# trips relevant total\", sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values()))  # Sorry for all the copypasting\n",
    "tprint(\"# taps saved per trip total\", f\"{(sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values())):.2f}\")\n",
    "tprint(\"% taps saved per trip total\", f\"{(sum(fu(taps_avoided).values())+OLD_TAPS*sum(fu(g_high_confidence_n_after_unlabeled).values()))/(sum(fu(trips_labeled).values())+sum(fu(g_high_confidence_n_after_unlabeled).values()))/OLD_TAPS:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209c292",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae41d2a",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "Here's the plan:\n",
    "\n",
    "ASSUME as we have been doing that people don't misclick (i.e., once you label a label, you don't relabel it)\n",
    "\n",
    "Perhaps TODO test this assumption\n",
    "\n",
    "Limit consideration to labeled trips during the after period that were unlabeled when the inference algorithm ran on them.\n",
    "\n",
    "For each trip:\n",
    " 1. Calculate pre-update idealized number of taps (always 6)\n",
    " 2. Calculate post-update actual number of taps (verify_events+2*label_events as before)\n",
    " 3. Calculate post-update idealized number of taps if user had followed intended algorithm:\n",
    "    1. If all the yellow labels are correct, press verify button\n",
    "    2. Repeat substep 1 until no more correct yellow labels\n",
    "    3. Input true value for ~most certain~ first non-green label\n",
    "    4. Repeat substeps 1-3 until trip completely labeled\n",
    "    - Note that this is NOT the optimal algorithm -- the optimal algorithm would have people clicking the verify button if ANY of the yellow labels are correct, but we don't teach that you should do that\n",
    "\n",
    "We've already shown stacked-bars graphs of 1 vs. 2. Now, show graphs of 1 vs. 2 vs. 3. Is 3 sufficiently close to 2 that this is a useful approximation? If so, continue. If not, a much more complicated algorithm is needed.\n",
    "\n",
    "ASSUME that people follow the intended algorithm (see above). Now for a given low and high confidence, we can easily calculate a counterfactual stacked-bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labelstruct = [\n",
    "    {\"labels\": {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"shopping\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.15},\n",
    "    {\"labels\": {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"entertainment\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.05},\n",
    "    {\"labels\": {\"mode_confirm\": \"drove_alone\", \"purpose_confirm\": \"work\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.45},\n",
    "    {\"labels\": {\"mode_confirm\": \"shared_ride\", \"purpose_confirm\": \"work\", \"replaced_mode\": \"placeholder\"}, \"p\": 0.35}\n",
    "]\n",
    "\n",
    "test_groundtruth = {\"mode_confirm\": \"walk\", \"purpose_confirm\": \"shopping\", \"replaced_mode\": \"placeholder\"}\n",
    "\n",
    "# confidences = {}\n",
    "# for label_type in LABEL_CATEGORIES:\n",
    "#     counter = {}\n",
    "#     for line in inference:\n",
    "#         if label_type not in line[\"labels\"]: continue  # Seems we have some incomplete tuples!\n",
    "#         val = line[\"labels\"][label_type]\n",
    "#         if val not in counter: counter[val] = 0\n",
    "#         counter[val] += line[\"p\"]\n",
    "#     confidences[label_type] = max(counter.values()) if len(counter) > 0 else 0 # This needs to be max, not sum!!! A major bug in a previous version.\n",
    "# trip_confidence = min(confidences.values())\n",
    "\n",
    "# Basically a copypaste of the reimplementation above -- this is fiddly stuff we'd like to touch as little as possible\n",
    "# TODO refactor to eliminate code duplication, but only do this in the presence of testing data sufficient to ensure we don't mess it up\n",
    "def sum_confidences(labelstruct):\n",
    "    confidences = {}\n",
    "    for label_type in LABEL_CATEGORIES:\n",
    "        counter = {}\n",
    "        for line in labelstruct:\n",
    "            if label_type not in line[\"labels\"]: continue\n",
    "            val = line[\"labels\"][label_type]\n",
    "            if val not in counter: counter[val] = 0\n",
    "            counter[val] += line[\"p\"]\n",
    "        confidences[label_type] = counter\n",
    "    return confidences\n",
    "def best_confidences(labelstruct):\n",
    "    confidences = sum_confidences(labelstruct)\n",
    "    return {k: max(confidences[k].items(), key = lambda item: item[1]) if len(confidences[k]) > 0 else (None, 0) for k in confidences}\n",
    "\n",
    "# Calculates the label categories and values we can display as yellow\n",
    "# Assumes we've already filtered out the non-viable options and renormalized\n",
    "def get_yellows(labelstruct, low_thresh):\n",
    "    confidences = best_confidences(labelstruct)\n",
    "    # print(\"cs\")\n",
    "    # print(confidences)\n",
    "    return {k: confidences[k][0] for k in confidences if confidences[k][1] > low_thresh}\n",
    "\n",
    "def next_green(labelstruct, established, ground_truth):\n",
    "    # Figure out the most probable thing for the user to next fill in\n",
    "    # confidences = sum_confidences(labelstruct)\n",
    "    # confidences = {k: confidences[k] for k in confidences if k not in established}  # Eliminate categories we've already greened\n",
    "    # choice = max(confidences.items(), key = lambda item: item[1][ground_truth[item[0]]] if len(item[1]) > 0 else 0)  # Pick the item whose probability matching the actual truth is highest\n",
    "    # return choice[0]\n",
    "\n",
    "    # Actually let's just say the user fills in the labels in order (this also seems plausible)\n",
    "    return next(filter(lambda category: category not in established, LABEL_CATEGORIES))\n",
    "\n",
    "def filter_and_renormalize(labelstruct, established, certainty):\n",
    "    for label_type in established:\n",
    "        # print(label_type)\n",
    "        labelstruct = list(filter(lambda row: row[\"labels\"][label_type] == established[label_type], labelstruct))\n",
    "    new_certainty = sum([row[\"p\"] for row in labelstruct])\n",
    "    for row in labelstruct: row[\"p\"] *= certainty/new_certainty\n",
    "    return labelstruct\n",
    "\n",
    "def is_above_high(labelstruct, high_thresh):\n",
    "    confidences = best_confidences(labelstruct)\n",
    "    confidences = {k: confidences[k][1] for k in confidences}\n",
    "    trip_confidence = min(confidences.values())\n",
    "    return trip_confidence > high_thresh\n",
    "\n",
    "def intended_taps(labelstruct, ground_truth, low_thresh):\n",
    "    # print(ground_truth)\n",
    "    certainty = sum([row[\"p\"] for row in labelstruct])\n",
    "    established = {}\n",
    "    taps = 0\n",
    "    while(len(established) < len(LABEL_CATEGORIES)):\n",
    "        # print(\"eold\")\n",
    "        # print(established)\n",
    "        candidates = get_yellows(labelstruct, low_thresh)\n",
    "        # print(\"c\")\n",
    "        # print(candidates)\n",
    "        # If all the yellow labels are correct, press the verify button and loop again\n",
    "        # (note the choice of all instead of any)\n",
    "        # print(\"g\")\n",
    "        # print(ground_truth)\n",
    "        if len(candidates) > 0 and all([candidates[k] == ground_truth[k] for k in candidates]):\n",
    "            taps += 1\n",
    "            established.update(candidates)\n",
    "        # Otherwise manually label the most confident\n",
    "        else:\n",
    "            taps += 2\n",
    "            selected = next_green(labelstruct, established, ground_truth)\n",
    "            established[selected] = ground_truth[selected]\n",
    "        # print(\"enew\")\n",
    "        # print(established)\n",
    "        labelstruct = filter_and_renormalize(labelstruct, established, certainty)\n",
    "        # print(\"ren\")\n",
    "        # print(labelstruct)\n",
    "        # print()\n",
    "    return taps\n",
    "\n",
    "assert intended_taps(test_labelstruct, test_groundtruth, LOW_CONFIDENCE_THRESHOLD_PRODUCTION) == 3\n",
    "# It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_saved_due_to_expectations(high_thresh):\n",
    "    return lambda row: int(is_above_high(row[\"inferred_labels\"], high_thresh))\n",
    "\n",
    "def is_saved_due_to_confirm(low_thresh, high_thresh):\n",
    "    return lambda row: 0 if is_above_high(row[\"inferred_labels\"], high_thresh) else 6-intended_taps(row[\"inferred_labels\"], row[\"user_input\"], low_thresh)\n",
    "\n",
    "def get_eliminations(low_thresh, high_thresh):\n",
    "    u_due_to_expectations = {}\n",
    "    u_due_to_confirm = {}\n",
    "    u_n_trips = {}\n",
    "    for user in filtered_users:\n",
    "        ct_df = confirmed_trip_df_map[u]\n",
    "        ct_df = filter_unlabeled(ct_df)\n",
    "        ct_df = ct_df[ct_df[\"user_input\"].apply(len) != 0]\n",
    "        # print(ct_df[\"user_input\"])\n",
    "        ct_df[\"due_to_expectations\"] = ct_df.apply(is_saved_due_to_expectations(high_thresh), axis=1)\n",
    "        ct_df[\"due_to_confirm\"] = ct_df.apply(is_saved_due_to_confirm(low_thresh, high_thresh), axis=1)\n",
    "        u_due_to_expectations[user] = ct_df[\"due_to_expectations\"].sum()\n",
    "        u_due_to_confirm[user] = ct_df[\"due_to_confirm\"].sum()\n",
    "        u_n_trips[user] = ct_df.shape[0]\n",
    "    eliminations = sum(u_due_to_expectations.values())/sum(u_n_trips.values()), sum(u_due_to_confirm.values())/sum(u_n_trips.values())\n",
    "    return eliminations\n",
    "        \n",
    "(idealized_saved_due_to_confirm, idealized_saved_due_to_expectations) = get_eliminations(LOW_CONFIDENCE_THRESHOLD_PRODUCTION, HIGH_CONFIDENCE_THRESHOLD_PRODUCTION)\n",
    "draw_stacked_bars(\"Graph of 1 vs. 2 vs. 3 as described above\", [\"1 = Before\", \"2 = After\", \"3 = Idealized\"], [(0, 0), (saved_due_to_confirm, saved_due_to_expectations), (idealized_saved_due_to_confirm, idealized_saved_due_to_expectations)], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2cab2",
   "metadata": {},
   "source": [
    "Now we can actually do the sensitivity analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    (LOW_CONFIDENCE_THRESHOLD_PRODUCTION, HIGH_CONFIDENCE_THRESHOLD_PRODUCTION),\n",
    "    \n",
    "    (0.05, 0.89),\n",
    "    (0.33, 0.89),\n",
    "    (0.40, 0.89),\n",
    "    (0.60, 0.89),\n",
    "    \n",
    "    (0.25, 0.75),\n",
    "    (0.25, 0.80),\n",
    "    (0.25, 0.95),\n",
    "    (0.25, 0.95)\n",
    "]\n",
    "labels = list(map(lambda scenario: f\"({scenario[0]:.2f}, {scenario[1]:.2f})\", scenarios))\n",
    "labels[0] = \"curr=\"+labels[0]\n",
    "draw_stacked_bars(\"Sensitivity analysis!\", labels, list(map(lambda t: get_eliminations(*t), scenarios)), 6, width=12, hook = lambda ax: ax.set_xlabel(\"Scenario: (lower threshold, upper threshold)\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da681417",
   "metadata": {},
   "source": [
    "The above graphs look rather strange to me, but that might be because I'm not working with the full dataset. This analysis should be re-run with the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb8179",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0cb092abdb45b3f790b3b120e72702300df2a5a5aff9091ddb915b48532d090"
  },
  "kernelspec": {
   "display_name": "iTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
