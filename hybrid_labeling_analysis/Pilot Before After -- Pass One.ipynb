{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b7bc6f",
   "metadata": {},
   "source": [
    "# Pilot Before After -- Pass One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64d7f6",
   "metadata": {},
   "source": [
    "This began as a copypaste of `Explore stage before vs after.ipynb` and remains a fairly faithful adaptation thereof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67df77",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9d89a",
   "metadata": {},
   "source": [
    "### Users to include in the study\n",
    "All except:\n",
    " * Users that don't correspond to real people\n",
    " * Users that don't have X confirmed trips both before and after\n",
    " * Users that don't have Y labeled trips both before and after?\n",
    " * Myself and possibly others who had very close contact with the updates (Shankari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5cfeb",
   "metadata": {},
   "source": [
    "### Time period\n",
    "Let \"**before**\" be from June 1 (to only capture summer patterns and exclude any old versions of the app and old behaviors) or whenever the user started using the app (measured by user's first `OPEN_APP` event), whichever is later, until July 19, a day before a meeting in which users may have been encouraged to label their trips more than usual.\n",
    "\n",
    "Let \"**after**\" be from whenever the user installed the update (measured by user's first `LABEL_TAB_SWITCH` event) until the new confidence threshold went into effect (let's say 3:20 PM MDT on August 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b609e7f",
   "metadata": {},
   "source": [
    "### Things to measure\n",
    " * Fraction of trips labeled before vs. after\n",
    " * Frequency of interaction with app before vs. after\n",
    " * Some metric representing the amount of work we saved the user\n",
    "   * Number of times \"verify\" was used\n",
    "   * Number of taps avoided\n",
    "   * Fraction of trips with no red labels\n",
    "   * Fraction of trips we didn't need to put in To Label\n",
    " * How much To Label is used vs. other tabs/screens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bb612",
   "metadata": {},
   "source": [
    "### Instrumentation that exists for both before and after\n",
    "~~strikethrough = irrelevant to us~~\n",
    " * `STATE_CHANGED`: Not sure, guessing when a page is switched using the buttons at the bottom of the screen?\n",
    " * ~~`BUTTON_FORCE_SYNC`: When a force sync is activated~~\n",
    " * `OPENED_APP`: When the app is opened\n",
    " * `CHECKED_DIARY`: When the Diary page is opened\n",
    " * `EXPANDED_TRIP`: When a map for a trip is opened?\n",
    " * `NOTIFICATION_OPEN`: When user clicks on a push notification?\n",
    " * ~~`METRICS_TIME`: Time spent on the metrics page~~\n",
    " * ~~`BEAR_TIME`: Time spent on the bear page~~\n",
    " * `DIARY_TIME`: Time spent on the Diary page, plus maybe something about the detail view?\n",
    " * `CHECKED_INF_SCROLL`: When we open the Label page\n",
    " * `INF_SCROLL_TIME`: Time spent on the Label page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf674f5",
   "metadata": {},
   "source": [
    "### Instrumentation that exists for after but not before\n",
    " * `VERIFY_TRIP`: When user presses the confirm button and what the labels look like when they do\n",
    " * `LABEL_TAB_SWITCH`: When user switches between filter tabs on the Label page\n",
    " * `SELECT_LABEL`: When user manually selects a label and what the labels look like when they do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6445f43",
   "metadata": {},
   "source": [
    "### How to measure things\n",
    " * Fraction of trips labeled before vs. after\n",
    "   * We can do this just using confirmed trip data\n",
    "   * If we do that, it will be based on trip time, rather than labeling time\n",
    "   * That might not actually be okay, given that since our \"after\" period is so short, people might very well be labeling \"before\" trips during the \"after\" period\n",
    "   * Maybe let's skip this metric for now\n",
    " * Frequency of interaction with app before vs. after\n",
    "   * Measure frequency of `OPENED_APP`\n",
    " * Number of times \"verify\" was used\n",
    "   * Measure frequency of `VERIFY_TRIP`\n",
    " * Fraction of trips with no red labels\n",
    "   * Just use confirmed trip data\n",
    " * Fraction of trips we didn't need to put in To Label\n",
    "   * Just use confirmed trip data plus expectations\n",
    " * Number of taps avoided per trip\n",
    "   * Let `taps` be the sum of the occurrences of `VERIFY_TRIP` and the occurrences of `SELECT_LABEL`\n",
    "   * Let `trips_labeled` be the sum of the occurrences of `VERIFY_TRIP` for which none of the labels were red, the occurrences of `SELECT_LABEL` for which all labels except the one being selected were green, and the number of trips we didn't need to put in To Label\n",
    "   * Final value is `6-taps/trips_labeled`\n",
    " * How much To Label is used vs. other tabs/screens\n",
    "   * Count the time spent on each of the screens using `LABEL_TAB_SWITCH` and `INF_SCROLL_TIME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f2ff4",
   "metadata": {},
   "source": [
    "### Given limited time to write analysis code, how to prioritize each of the metrics\n",
    " 1. Frequency of interaction with app before vs. after\n",
    "   * Probably not going to be the most useful metric, especially given how much we have influenced this, but it's a nice easy one to take as a \"warmup\"\n",
    " 2. Fraction of trips we didn't need to put in To Label\n",
    "   * Good context for the rest is to come\n",
    " 3. Fraction of trips with no red labels\n",
    "   * Good context for the rest is to come\n",
    " 4. Number of taps avoided per trip\n",
    "   * Hard to calculate, but guaranteed to show that we saved the users some work -- just nice to know exactly how much\n",
    " 5. Number of times \"verify\" was used\n",
    "   * Easy to calculate and nice to know\n",
    " 6. How much To Label is used vs. other tabs/screens\n",
    "   * This can be skipped if necessary -- for now, qualitative feedback probably fills this niche better anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85993b77",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ec89d",
   "metadata": {},
   "source": [
    "### HARDCODED THINGS THAT SHOULD BE FETCHED PROGRAMMATICALLY IF WHEN I REVISIT THIS IT ISN'T 2AM\n",
    "Please don't forget about this and botch future analyses by using old constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CATEGORIES = [\"mode_confirm\", \"purpose_confirm\", \"replaced_mode\"]\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.95  # Confidence we need to not put a trip in To Label\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.5  # confidenceThreshold from the config file (or from last time I thought hardcoding was a good idea...)\n",
    "# Note that this will become more complicated if future analyses are done under a regime with multiple collection modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6de124",
   "metadata": {},
   "source": [
    "(end ugly hardcoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f20e7",
   "metadata": {},
   "source": [
    "### Defining the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f25bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REQUIRED_TRIPS_TOTAL = 1 # previously 30\n",
    "REQUIRED_TRIPS_BEFORE = 0 # previously 10\n",
    "REQUIRED_TRIPS_AFTER = 0\n",
    "import arrow\n",
    "BEFORE_START = arrow.get(\"2021-06-01T00:00-06:00\")\n",
    "BEFORE_END = arrow.get(\"2021-07-20T00:00-06:00\")\n",
    "AFTER_END = arrow.get(\"2021-08-11T15:20-06:00\")\n",
    "from uuid import UUID\n",
    "EXCLUDE_UUIDS = [] # EXCLUDE_UUIDS = [UUID(s) for s in input(\"Enter UUIDs to exclude, separated by spaces: \").split(\" \") if len(s) > 0]\n",
    "print(EXCLUDE_UUIDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6226a",
   "metadata": {},
   "source": [
    "### Relevant instrumentation keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_keys = {\n",
    "    \"time\": \"stats/client_time\",\n",
    "    \"error\": \"stats/client_error\",\n",
    "    \"nav\": \"stats/client_nav_event\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d90a7",
   "metadata": {},
   "source": [
    "### Do not truncate dataframes to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60334fe5",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "_default_log_level = logging.DEBUG\n",
    "def set_log_level(level):\n",
    "    logging.getLogger().setLevel(level)\n",
    "def reset_log_level():\n",
    "    global _default_log_level\n",
    "    logging.getLogger().setLevel(_default_log_level)\n",
    "reset_log_level()\n",
    "# If we wanted to be even more elaborate we could push and pop from a stack of logging levels..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42bf52e",
   "metadata": {},
   "source": [
    "### Imports and aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as estag\n",
    "agts = estag.AggregateTimeSeries()\n",
    "import emission.storage.timeseries.timequery as estt\n",
    "from statistics import mean\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f52af",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16be2fb",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_between = lambda dataset, key, start, end: dataset[(dataset[key] >= start) & (dataset[key] <= end)]\n",
    "def print_div_zero(f):\n",
    "    try:\n",
    "        print(f())\n",
    "    except ZeroDivisionError:\n",
    "        print(\"[denominator is zero]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770c012",
   "metadata": {},
   "source": [
    "### Get the instrumentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much slower if I provide total_time_query to get_data_df.\n",
    "# total_time_query = estt.TimeQuery(\"data.ts\", BEFORE_START.timestamp, AFTER_END.timestamp)\n",
    "stats = {}\n",
    "for key in db_keys:\n",
    "    print(f'Adding \"{db_keys[key]}\" to stats as \"{key}\"')\n",
    "    stats[key] = agts.get_data_df(db_keys[key])\n",
    "    print(f\"-> Done; found {stats[key].shape[0]} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17220f5a",
   "metadata": {},
   "source": [
    "### Get the trip data and filter a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fee75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_update(new, old, reason):\n",
    "    print(f\"Excluded {len(old)-len(new)} users, left with {len(new)}: {reason}\")\n",
    "\n",
    "set_log_level(logging.INFO)\n",
    "all_users = esta.TimeSeries.get_uuid_list()\n",
    "confirmed_trip_df_map = {}\n",
    "print(f\"Working with {len(all_users)} initial users\")\n",
    "\n",
    "filter0_users = [u for u in all_users if u not in EXCLUDE_UUIDS]  # Users that we don't explicitly exclude\n",
    "filter_update(filter0_users, all_users, \"presence on exclusion list\")\n",
    "\n",
    "filter1_users = []  # Users with enough total trips\n",
    "for u in filter0_users:\n",
    "    ts = esta.TimeSeries.get_time_series(u)\n",
    "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
    "    confirmed_trip_df_map[u] = ct_df\n",
    "    if ct_df.shape[0] >= REQUIRED_TRIPS_TOTAL: filter1_users.append(u)\n",
    "filter_update(filter1_users, filter0_users, \"not enough total trips\")\n",
    "\n",
    "# To find a user's UUID based on the end date of their first trip:\n",
    "# for u in filter1_users:\n",
    "#     ct_df = confirmed_trip_df_map[u].copy()\n",
    "#     ct_df.sort_values(\"end_ts\", ascending=True, inplace=True)\n",
    "#     print(u)\n",
    "#     print(arrow.get(ct_df.iloc[0][\"end_ts\"]).to(\"America/Chicago\"))\n",
    "#     print()\n",
    "\n",
    "server_filtered_users = filter1_users\n",
    "print(f\"For metrics that don't need user interaction, working with {len(server_filtered_users)} filtered users\")\n",
    "\n",
    "reset_log_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_stats_existence(name, dataset=filter1_users):\n",
    "    print(f\"Exploring {name}\")\n",
    "    m = 0\n",
    "    for u in dataset:\n",
    "        for k in db_keys:\n",
    "            n = len(stats[k][(stats[k][\"user_id\"] == u) & (stats[k][\"name\"] == name)])\n",
    "            if n > 0:\n",
    "                print(f\"{k}: {n}; \", end=\"\", flush=True)\n",
    "                m += 1\n",
    "    print(f\"\\nTotal: {m} out of {len(dataset)}\")\n",
    "explore_stats_existence(\"opened_app\")\n",
    "explore_stats_existence(\"label_tab_switch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter2_users = []  # Users with enough before trips\n",
    "user_before_start = {}\n",
    "for u in filter1_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    ct_df[\"end_arrow\"] = ct_df[\"end_ts\"].apply(arrow.get)\n",
    "    ct_df[\"write_arrow\"] = ct_df[\"metadata_write_ts\"].apply(arrow.get)\n",
    "    ct_df.sort_values(\"end_arrow\", ascending=True, inplace=True)\n",
    "    this_before_start = max(ct_df.iloc[0][\"end_arrow\"], BEFORE_START)\n",
    "    n_before = filter_between(ct_df, \"end_arrow\", this_before_start, BEFORE_END).shape[0]\n",
    "    user_before_start[u] = this_before_start\n",
    "    if n_before >= REQUIRED_TRIPS_BEFORE: filter2_users.append(u)\n",
    "filter_update(filter2_users, filter1_users, \"not enough before trips\")\n",
    "\n",
    "filter3_users = []  # Users that have installed the update\n",
    "for u in filter2_users:\n",
    "    lts = len(stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"label_tab_switch\")])  # (or haven't used the update enough to generate any of these events, which is highly unlikely if they do have the update)\n",
    "    if lts > 0: filter3_users.append(u)\n",
    "filter_update(filter3_users, filter2_users, \"have not installed the update\")\n",
    "\n",
    "filter4_users = [] # Users with enough after trips\n",
    "user_after_start = {}\n",
    "for u in filter3_users:\n",
    "    lts = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"label_tab_switch\")].copy()\n",
    "    lts.sort_values(\"ts\", ascending=True, inplace=True)\n",
    "    this_after_start = arrow.get(lts.iloc[0][\"ts\"])\n",
    "    user_after_start[u] = this_after_start\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    n_after = filter_between(ct_df, \"end_arrow\", this_after_start, AFTER_END).shape[0]\n",
    "    if n_after >= REQUIRED_TRIPS_AFTER: filter4_users.append(u)\n",
    "filter_update(filter4_users, filter3_users, \"not enough after trips\")\n",
    "\n",
    "filtered_users = filter4_users\n",
    "print(f\"For metrics that do need user interaction, working with {len(filtered_users)} filtered users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2fb54",
   "metadata": {},
   "source": [
    "## Analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbc5cb",
   "metadata": {},
   "source": [
    "### 1. Frequency of interaction with app before vs. after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_before = {}\n",
    "days_after = {}\n",
    "opens_before = {}\n",
    "opens_after = {}\n",
    "opens_per_day_before = {}\n",
    "opens_per_day_after = {}\n",
    "\n",
    "delta2days = lambda d: d.days+d.seconds/86400\n",
    "\n",
    "for u in filtered_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    this_before_start = user_before_start[u]\n",
    "    this_before_end = BEFORE_END\n",
    "    this_after_start = user_after_start[u]\n",
    "    this_after_end = AFTER_END\n",
    "    \n",
    "    days_before[u] = delta2days(this_before_end-this_before_start)\n",
    "    days_after[u] = delta2days(this_after_end-this_after_start)\n",
    "    \n",
    "    opens = stats[\"nav\"][(stats[\"nav\"][\"user_id\"] == u) & (stats[\"nav\"][\"name\"] == \"opened_app\")].copy()\n",
    "    opens[\"ts_arrow\"] = opens[\"ts\"].apply(arrow.get)\n",
    "    opens_before[u] = filter_between(opens, \"ts_arrow\", this_before_start, this_before_end).shape[0]\n",
    "    opens_after[u] = filter_between(opens, \"ts_arrow\", this_after_start, this_after_end).shape[0]\n",
    "    \n",
    "    opens_per_day_before[u] = opens_before[u]/days_before[u]\n",
    "    opens_per_day_after[u] = opens_after[u]/days_after[u]\n",
    "\n",
    "for u in filtered_users:\n",
    "    print(f\"App opens per day before->after, opens before+after: {opens_per_day_before[u]:.2f}->{opens_per_day_after[u]:.2f}, {opens_before[u]+opens_after[u]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65076d38",
   "metadata": {},
   "source": [
    "Well, with the `stage_2021-08-07` dump dataset, I don't think that tells us anything at all conclusive. Let's move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4092d",
   "metadata": {},
   "source": [
    "### 2. Fraction of trips we didn't need to put in To Label\n",
    "This one is good because we don't need the users to have done anything, even install the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e87255",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trip_n = {}\n",
    "total_trip_n_after = {}\n",
    "total_trip_n_unlabeled = {}\n",
    "total_trip_n_after_unlabeled = {}\n",
    "high_confidence_n = {}  # Trips with inferences so confident they don't need to go in To Label\n",
    "high_confidence_n_after = {}\n",
    "high_confidence_n_unlabeled = {}\n",
    "high_confidence_n_after_unlabeled = {}\n",
    "mid_confidence_n = {}  # Trips that need to go in To Label but have no red labels\n",
    "high_confidence_frac = {}\n",
    "mid_confidence_frac = {}\n",
    "\n",
    "for u in server_filtered_users:\n",
    "    ct_df = confirmed_trip_df_map[u]\n",
    "    total_trip_n[u] = ct_df.shape[0]\n",
    "    high_confidence_n[u] = 0\n",
    "    mid_confidence_n[u] = 0\n",
    "    \n",
    "    total_trip_n_unlabeled[u] = ct_df[ct_df[\"user_input\"].apply(len) == 0].shape[0]\n",
    "    high_confidence_n_unlabeled[u] = 0\n",
    "    \n",
    "    if u in filtered_users:\n",
    "        high_confidence_n_after[u] = 0\n",
    "        high_confidence_n_after_unlabeled[u] = 0\n",
    "        this_after_start = user_after_start[u]\n",
    "        this_after_end = AFTER_END\n",
    "        trips_after = filter_between(ct_df, \"end_arrow\", this_after_start, this_after_end)\n",
    "        total_trip_n_after[u] = trips_after.shape[0]\n",
    "        total_trip_n_after_unlabeled[u] = trips_after[trips_after[\"user_input\"].apply(len) == 0].shape[0]\n",
    "        ids = []\n",
    "        for _, trip in trips_after.iterrows():\n",
    "            ids.append(trip[\"_id\"])\n",
    "    \n",
    "    for _, trip in ct_df.iterrows():\n",
    "        inference = trip[\"inferred_labels\"]\n",
    "        # Here goes a quick and partial reimplementation of the on-the-fly (client-side) inference algorithm\n",
    "        confidences = {}\n",
    "        for label_type in LABEL_CATEGORIES:\n",
    "            counter = {}\n",
    "            for line in inference:\n",
    "                if label_type not in line[\"labels\"]: continue  # Seems we have some incomplete tuples!\n",
    "                val = line[\"labels\"][label_type]\n",
    "                if val not in counter: counter[val] = 0\n",
    "                counter[val] += line[\"p\"]\n",
    "            confidences[label_type] = sum(counter.values())\n",
    "        trip_confidence = min(confidences.values())\n",
    "        # if (trip_confidence >= 0.01 and trip_confidence <= 0.99): print(trip_confidence)\n",
    "        if trip_confidence > HIGH_CONFIDENCE_THRESHOLD:\n",
    "            high_confidence_n[u] += 1\n",
    "            if u in filtered_users and trip[\"_id\"] in ids:\n",
    "                high_confidence_n_after[u] += 1\n",
    "                if len(trip[\"user_input\"]) == 0: high_confidence_n_after_unlabeled[u] += 1\n",
    "            if len(trip[\"user_input\"]) == 0: high_confidence_n_unlabeled[u] += 1\n",
    "        if trip_confidence > LOW_CONFIDENCE_THRESHOLD:\n",
    "            mid_confidence_n[u] += 1\n",
    "    high_confidence_frac[u] = high_confidence_n[u] / total_trip_n[u]\n",
    "    in_to_label = total_trip_n[u]-high_confidence_n[u]\n",
    "    mid_confidence_frac[u] = (mid_confidence_n[u]-high_confidence_n[u]) / in_to_label if in_to_label != 0 else float(\"NaN\")\n",
    "\n",
    "for u in server_filtered_users:\n",
    "    print(f\"{high_confidence_frac[u]:.2%}\")\n",
    "print(f\"Average percentage of trips users do not need to interact at all with: {sum(high_confidence_n.values())}/{sum(total_trip_n.values())}={sum(high_confidence_n.values())/sum(total_trip_n.values()):.2%}\")\n",
    "print(\"Considering only those in the \\\"after\\\" period:\")\n",
    "print_div_zero(lambda: f\"Average percentage of trips users do not need to interact at all with: {sum(high_confidence_n_after.values())}/{sum(total_trip_n_after.values())}={sum(high_confidence_n_after.values())/sum(total_trip_n_after.values()):.2%}\")\n",
    "print(\"Considering all unlabeled:\")\n",
    "print(f\"Average percentage of trips users do not need to interact at all with: {sum(high_confidence_n_unlabeled.values())}/{sum(total_trip_n_unlabeled.values())}={sum(high_confidence_n_unlabeled.values())/sum(total_trip_n_unlabeled.values()):.2%}\")\n",
    "print(\"Considering only those both unlabeled and in the \\\"after\\\" period:\")\n",
    "print_div_zero(lambda: f\"Average percentage of trips users do not need to interact at all with: {sum(high_confidence_n_after_unlabeled.values())}/{sum(total_trip_n_after_unlabeled.values())}={sum(high_confidence_n_after_unlabeled.values())/sum(total_trip_n_after_unlabeled.values()):.2%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e326386",
   "metadata": {},
   "source": [
    "### 3. Fraction of trips with no red labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dfe5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for u in server_filtered_users:\n",
    "    print(f\"{mid_confidence_frac[u]:.2%}\")\n",
    "print(f\"Average percentage of trips in To Label with no red labels: {mean(filter(lambda v: v == v, mid_confidence_frac.values())):.2%}\")  # The filter excludes NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8baa1",
   "metadata": {},
   "source": [
    "### 4. Number of taps avoided per trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_TAPS = 2*len(LABEL_CATEGORIES)  # Number of taps each trip requires to fully label under the old UI\n",
    "\n",
    "verifieds = {}\n",
    "taps = {}\n",
    "trips_labeled = {}\n",
    "taps_avoided = {}\n",
    "taps_avoided_per_trip = {}\n",
    "\n",
    "# Whether or not a press of the verify button fully labels a trip\n",
    "def verify_fully_labels(event):\n",
    "    if not event[\"reading\"][\"verifiable\"]: return False  # Forgot about this case until working with real pilot program data...\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "    final_inference = json.loads(event[\"reading\"][\"finalInference\"])\n",
    "    return len(user_input) < len(LABEL_CATEGORIES) and len(set(user_input.keys()) | set(final_inference.keys())) == len(LABEL_CATEGORIES)\n",
    "\n",
    "# Whether or not a given label dropdown menu selection fully labels a trip\n",
    "def label_fully_labels(event):\n",
    "    user_input = json.loads(event[\"reading\"][\"userInput\"])\n",
    "    return len(user_input) == len(LABEL_CATEGORIES)-1 and event[\"reading\"][\"inputKey\"] not in event[\"reading\"][\"userInput\"]\n",
    "\n",
    "for u in filtered_users:\n",
    "    trips_labeled[u] = 0\n",
    "    verifieds[u] = 0\n",
    "    verify_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"verify_trip\")]\n",
    "    label_events = stats[\"time\"][(stats[\"time\"][\"user_id\"] == u) & (stats[\"time\"][\"name\"] == \"select_label\")]\n",
    "    taps[u] = len(verify_events)+2*len(label_events)\n",
    "    if verify_events.shape[0] > 0:\n",
    "        for _, ve in verify_events.iterrows():  # Not entirely sure why apply() wasn't working for this, don't have time to figure out\n",
    "            if verify_fully_labels(ve):\n",
    "                verifieds[u] += 1\n",
    "                trips_labeled[u] += 1\n",
    "    if label_events.shape[0] > 0:\n",
    "        for _, le in label_events.iterrows():\n",
    "            if label_fully_labels(le): trips_labeled[u] += 1\n",
    "    taps_avoided[u] = OLD_TAPS*trips_labeled[u]-taps[u]\n",
    "    taps_avoided_per_trip[u] = taps_avoided[u]/trips_labeled[u] if trips_labeled[u] != 0 else float(\"NaN\")\n",
    "\n",
    "print(f\"We assume that under the old design, users must tap {OLD_TAPS} times to label each trip -- two taps per label category\")\n",
    "for u in filtered_users:\n",
    "    print(f\"User tapped {taps[u]} times, avoided {taps_avoided[u]} taps to label {trips_labeled[u]} trips\")\n",
    "\n",
    "total_taps_avoided = sum(taps_avoided.values()) \n",
    "total_trips_labeled = sum(trips_labeled.values())\n",
    "avoided_per_labeled = total_taps_avoided/total_trips_labeled\n",
    "print(f\"Overall, {total_taps_avoided} taps were avoided, {avoided_per_labeled:.2f} per trip -- that's {avoided_per_labeled/OLD_TAPS:.2%} of taps\")\n",
    "\n",
    "# The following stats seem very impressive but aren't really fair because there are so many unlabeled trips:\n",
    "# total_taps_avoided_high = total_taps_avoided+OLD_TAPS*sum(high_confidence_n.values())\n",
    "# total_trips_labeled_high = total_trips_labeled+sum(high_confidence_n.values())\n",
    "# avoided_per_labeled_high = total_taps_avoided_high/total_trips_labeled_high\n",
    "# print(f\"If we also count the taps we avoided by not putting high-confidence inferences on the To Label screen:\")\n",
    "# print(f\"Overall, {total_taps_avoided_high} taps were avoided, {avoided_per_labeled_high:.2f} per trip -- that's {avoided_per_labeled_high/OLD_TAPS:.2%} of taps\")\n",
    "\n",
    "# Here's a fairer way of saying it:\n",
    "print(f\"We also saved {OLD_TAPS*sum(high_confidence_n_after_unlabeled.values())} taps across {sum(high_confidence_n_after_unlabeled.values())} trips by not soliciting user input on very confident trips\")\n",
    "\n",
    "total_taps_avoided_high = total_taps_avoided+OLD_TAPS*sum(high_confidence_n_after_unlabeled.values())\n",
    "total_trips_labeled_high = total_trips_labeled+sum(high_confidence_n_after_unlabeled.values())\n",
    "avoided_per_labeled_high = total_taps_avoided_high/total_trips_labeled_high\n",
    "print(f\"If we also count the taps we avoided by not putting high-confidence inferences on the To Label screen:\")\n",
    "print(f\"Overall, {total_taps_avoided_high} taps were avoided across {sum(trips_labeled.values())+sum(high_confidence_n_after_unlabeled.values())} trips, {avoided_per_labeled_high:.2f} per trip -- that's {avoided_per_labeled_high/OLD_TAPS:.2%} of taps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74eaa3c",
   "metadata": {},
   "source": [
    "### 5. Number of times \"verify\" was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5956d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in filtered_users:\n",
    "    print(verifieds[u])\n",
    "print(f\"Overall, {len([v for v in verifieds.values() if v > 0])} users used the verify button for a total of {sum(verifieds.values())} uses\")\n",
    "print(f\"Total number of trips labeled: {sum(trips_labeled.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c3422",
   "metadata": {},
   "source": [
    "### 6. How much To Label is used vs. other tabs/screens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc67e6e",
   "metadata": {},
   "source": [
    "I'm not going to implement this one for now. I don't think we will get any meaningful results from it given the extremely limited sample we're working with, so for now qualitative feedback answers the \"Was the To Label tab a good idea?\" question better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d613d6f",
   "metadata": {},
   "source": [
    "## Lingering questions\n",
    "### Is this all... correct?\n",
    "I wrote this all in one evening, very quickly. It should be tested more extensively.\n",
    "\n",
    "### Train vs. test\n",
    "Currently, I sometimes use unlabeled trips as a \"test\" dataset to remove the effect of the algorithm \"cheating\" by predicting data it trained on. Might there be a better way to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc3b34",
   "metadata": {},
   "source": [
    "## A note on multiple programs\n",
    "Using `mongod --dbpath`, it is possible to create many different databases. If one database is created for each of a handful of programs restored using `mongorestore` and these results are compared to results from an aggregate database created by `mongorestore`-ing all the dumps to the same database, certain statistics (e.g., total number of trips) turn out to be the same. Thus, it seems to be feasible to analyze multiple programs using only one aggregate database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5562f58",
   "metadata": {},
   "source": [
    "## Making sense of data from multiple programs\n",
    "In the final analysis, we will want to aggregate data from multiple pilot programs but also to be able to analyze each one individually. `mongorestore`-ing all the dumps into one database works well for the ensemble; for now, let's just make sure we can figure out which program each user in that ensemble came from so we can disaggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = {}\n",
    "for u in edb.get_uuid_db().find():\n",
    "    u[\"program\"] = u[\"user_email\"].split(\"_\")[0]\n",
    "    user_info[u[\"uuid\"]] = u\n",
    "    \n",
    "\n",
    "for u in server_filtered_users:\n",
    "    print(u)\n",
    "    print(user_info[u][\"program\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbe821",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988e26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
