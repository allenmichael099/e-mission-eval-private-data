{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try spatial covariance and compare it with other variance calculation methods\n",
    "Looks at how many standard deviations the sensed value is from the user labeled value.\n",
    "Calculate variance by summing trip level variances and including a spatial covariance term based on trip clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uuid import UUID\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/mallen2/alternate_branches/gis-with-build-label-model/e-mission-server')\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.core.wrapper.user as ecwu\n",
    "\n",
    "import confusion_matrix_handling as cm_handling\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "import get_EC\n",
    "import helper_functions as hf\n",
    "\n",
    "import sklearn.model_selection as skm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "\n",
    "# import folium\n",
    "\n",
    "# For fetching the trip clusters\n",
    "import emission.analysis.modelling.trip_model.model_storage as eamtm\n",
    "import emission.analysis.modelling.trip_model.model_type as eamum\n",
    "\n",
    "METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
    "ECAR_PROPORTION = 0 #0.01 #~1% of cars on the road are electric.\n",
    "DROVE_ALONE_TO_SHARED_RIDE_RATIO = 1\n",
    "\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv') # r stands for raw string, only matters if the path is on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already ran store_expanded_labeled_trips.ipynb already and want to save time vs running the cell below\n",
    "%store -r expanded_labeled_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import database_related_functions as drf  # all the emission server functions for this notebook are in here.\n",
    "user_list, os_map, uuid_program_map = drf.get_participants_programs_and_operating_systems()\n",
    "#print(len(user_list), len(os_map), len(uuid_program_map))\n",
    "\n",
    "# Takes ~ 1 min 45 s to 2 min 45 s on Macbook Pro for all ceo data up to May 2022.\n",
    "expanded_labeled_trips = drf.get_expanded_labeled_trips(user_list)\n",
    "expanded_labeled_trips['os'] = expanded_labeled_trips.user_id.map(os_map)\n",
    "expanded_labeled_trips['program'] = expanded_labeled_trips['user_id'].map(uuid_program_map)\n",
    "\n",
    "expanded_labeled_trips = expanded_labeled_trips.drop(labels = ['source', 'end_fmt_time', 'end_loc', 'raw_trip',\n",
    "    'start_fmt_time', 'start_loc','start_local_dt_year', 'start_local_dt_month', 'start_local_dt_day',\n",
    "    'start_local_dt_hour', 'start_local_dt_minute', 'start_local_dt_second',\n",
    "    'start_local_dt_weekday', 'start_local_dt_timezone',\n",
    "    'end_local_dt_year', 'end_local_dt_month', 'end_local_dt_day',\n",
    "    'end_local_dt_hour', 'end_local_dt_minute', 'end_local_dt_second',\n",
    "    'end_local_dt_weekday', 'end_local_dt_timezone'], axis = 1)\n",
    "\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "\n",
    "# Group together the prepilot participants\n",
    "prepilot_list = ['84Q9SsrH','cwZazZLJ','CudLAeg8','sxxcLqbK','Q8T7QTXK','5KEGHHuf','e9MaNVU7','7c797MRD','rhBZukxY','k36cxmfA','FmxVf8u6','F3jxHLSW']\n",
    "expanded_labeled_trips['program'] = expanded_labeled_trips.program.replace(prepilot_list, \"prepilot\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_labeled_trips = hf.drop_unwanted_trips(expanded_labeled_trips, drop_not_a_trip=False)\n",
    "# If you want to double check whether you included not a trip: 'not_a_trip' in expanded_labeled_trips.mode_confirm.unique()\n",
    "\n",
    "expanded_labeled_trips = hf.get_primary_modes(expanded_labeled_trips,energy_dict,MODE_MAPPING_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each trip to a cluster. Takes 13 minutes on all ceo up to May.\n",
    "# First you need to find the clusters with build_label_model.py.\n",
    "# (the script in e-mission-server that does the greedy similarity binning)\n",
    "# source .../e-mission-py.bash  bin/build_label_model.py -a   (-a is for all users)\n",
    "\n",
    "# start with the dataframe of the trips that we are interested in.\n",
    "############## Make sure that this is a precomputation if you decide to use spatial covariance.###############\n",
    "\n",
    "#expanded_labeled_trips = expanded_labeled_trips.set_index('_id')\n",
    "expanded_labeled_trips['cluster_id'] = ['0']*len(expanded_labeled_trips)\n",
    "expanded_labeled_trips['trip_neighbors'] = [['0']]*len(expanded_labeled_trips)\n",
    "expanded_labeled_trips['cluster_size'] = ['0']*len(expanded_labeled_trips)\n",
    "\n",
    "for user_id in expanded_labeled_trips.user_id.unique():\n",
    "    # get the cluster for this user\n",
    "    user_trip_clusters = eamtm.load_model(user_id, eamum.ModelType.GREEDY_SIMILARITY_BINNING, eamtm.ModelStorage.DOCUMENT_DATABASE)\n",
    "    if user_trip_clusters is not None:\n",
    "        for cluster_number in user_trip_clusters.keys():\n",
    "            trips_in_cluster = user_trip_clusters[cluster_number]['trip_ids']  # want trips such that they are in the cluster and in the dataframe.\n",
    "            relevant_trip_indices = expanded_labeled_trips[expanded_labeled_trips['_id'].isin(trips_in_cluster)].index\n",
    "\n",
    "            trips_in_cluster_and_df = expanded_labeled_trips.loc[relevant_trip_indices]['_id']\n",
    "\n",
    "            # Set the cluster number for all trips in the current cluster.\n",
    "\n",
    "            for idx, trip_id in zip(relevant_trip_indices,trips_in_cluster):\n",
    "                expanded_labeled_trips.loc[idx,'cluster_id'] = str(user_id) + '_' + cluster_number\n",
    "\n",
    "                expanded_labeled_trips.loc[idx,'cluster_size'] = len(trips_in_cluster_and_df)\n",
    "\n",
    "                # set a trip's neighbors as all cluster members excluding itself.\n",
    "                expanded_labeled_trips.at[idx,'trip_neighbors'] = set(trips_in_cluster_and_df).difference({trip_id})\n",
    "    else:\n",
    "        user_index = expanded_labeled_trips[expanded_labeled_trips.user_id == user_id].index\n",
    "        expanded_labeled_trips.loc[user_index, ['cluster_id', 'cluster_size']] = '1',1\n",
    "\n",
    "        #expanded_labeled_trips.loc[relevant_trip_indices,'cluster_id'] = str(user_id) + '_' + cluster_number # cluster number is a string\n",
    "\n",
    "print(\"Histogram of cluster size:\")\n",
    "expanded_labeled_trips.cluster_size.hist(bins=30); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra packages I installed but ultimately didn't use. # shapely; geopandas; pysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time if you want to run this notebook again:\n",
    "#all_ceo_with_clusters_expanded_labeled_trips = expanded_labeled_trips.copy()\n",
    "#%store all_ceo_with_clusters_expanded_labeled_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r all_ceo_with_clusters_expanded_labeled_trips\n",
    "expanded_labeled_trips = all_ceo_with_clusters_expanded_labeled_trips.copy()\n",
    "del all_ceo_with_clusters_expanded_labeled_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at how many trips are in clusters rather than just one time trips.\n",
    "number_of_clustered_trips = len(expanded_labeled_trips[expanded_labeled_trips.cluster_size > 1])\n",
    "number_of_single_trips = len(expanded_labeled_trips)-number_of_clustered_trips\n",
    "\n",
    "print(number_of_clustered_trips, number_of_single_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dist_MCS_df = pd.read_csv(\"unit_distance_MCS.csv\").set_index(\"moment\")\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrices and then the EI moments from those.\n",
    "android_confusion = pd.read_csv(\"android_confusion.csv\").set_index('gt_mode')\n",
    "ios_confusion = pd.read_csv(\"ios_confusion.csv\").set_index('gt_mode')\n",
    "\n",
    "android_confusion = cm_handling.collapse_confusion_matrix(android_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "ios_confusion = cm_handling.collapse_confusion_matrix(ios_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "\n",
    "sensed_car_EI = hf.find_sensed_car_energy_intensity(energy_dict, ECAR_PROPORTION, DROVE_ALONE_TO_SHARED_RIDE_RATIO)\n",
    "energy_dict.update({\"Car, sensed\": sensed_car_EI})\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "EI_length_cov = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you forget this step, the error for expected may be different, \n",
    "# since you might be relying on a different saved version of the EI_moments_dataframe\n",
    "android_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(android_confusion,energy_dict)\n",
    "ios_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(ios_confusion,energy_dict)\n",
    "os_EI_moments_map = {'ios': ios_EI_moments_df, 'android': android_EI_moments_df}\n",
    "energy_consumption_df = get_EC.compute_all_EC_values(expanded_labeled_trips,unit_dist_MCS_df,energy_dict,android_EI_moments_df,ios_EI_moments_df, \\\n",
    "    EI_length_cov, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of finding a spatial covariance for vail.\n",
    "get_EC.spatial_autocov_based_on_clusters(energy_consumption_df[energy_consumption_df.program == 'vail'],'user_labeled', print_statistics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the spatial autocovariance specific to each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~1 minute on all ceo up to May 2022.\n",
    "user_spatial_cov_map = {}\n",
    "user_morans_I_map = {}\n",
    "for user in energy_consumption_df.user_id.unique():\n",
    "    user_df = energy_consumption_df[energy_consumption_df.user_id == user].copy()\n",
    "    if len(user_df) < 2: \n",
    "        user_spatial_cov_map[user] = 0\n",
    "        morans_I = 0\n",
    "    else:\n",
    "        user_spatial_cov_map[user], morans_I = get_EC.spatial_autocov_based_on_clusters(user_df, 'expected')\n",
    "    user_morans_I_map[user] = morans_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(user_morans_I_map.values(), bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking that I didn't include a trip in the list of its neighbors\n",
    "mistaken_extra_ids = []\n",
    "for i,ct in energy_consumption_df.iterrows():\n",
    "    if ct['_id'] in ct['trip_neighbors']:\n",
    "        print('trip should not be counted as one of its neighbors.')\n",
    "        mistaken_extra_ids.append(ct['_id'])\n",
    "\n",
    "# looks like a few slipped through somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we use 1 spatial covariance for all users? # Takes 1 min 15 s\n",
    "cov_sum = 0\n",
    "for cluster_id in energy_consumption_df.cluster_id.unique():\n",
    "    cluster_size = len(energy_consumption_df[energy_consumption_df.cluster_id == cluster_id])\n",
    "    if cluster_size > 1:\n",
    "        cov_sum += 15.504*(cluster_size**2 - cluster_size)  # I calculated a CEO dataset wide spatial cov of 15.504\n",
    "larger_sd = np.sqrt(energy_consumption_df.confusion_var.sum() + cov_sum)\n",
    "print(f\"aggregate standard deviation: {larger_sd}\")\n",
    "error = energy_consumption_df.expected.sum() - energy_consumption_df.user_labeled.sum()\n",
    "print(f\"Error for expected: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if Moran's I is maxed out for each participant, leading to just adding the variance for each user in each cluster?\n",
    "# Concretely, instead of using the autocovariance that I calculated for each user, I would just use the variance of expected EI estimates calculated for each user.\n",
    "from math import factorial\n",
    "def n_choose_r(n,r):\n",
    "    return factorial(n)/(factorial(r)*factorial(n-r))\n",
    "\n",
    "user_variance_map = {}\n",
    "for user in energy_consumption_df.user_id.unique():\n",
    "    user_df = energy_consumption_df[energy_consumption_df.user_id == user].copy()\n",
    "    user_variance_map[user] = np.var(user_df.expected)\n",
    "\n",
    "max_cov_sum = 0\n",
    "for user in energy_consumption_df.user_id.unique():\n",
    "    # Get the trips associated with this user.\n",
    "    user_df = energy_consumption_df[energy_consumption_df.user_id == user].copy()\n",
    "    for cluster_id in user_df.cluster_id.unique():\n",
    "        cluster_size = len(user_df[user_df.cluster_id == cluster_id])\n",
    "        if cluster_size > 1:\n",
    "            max_cov_sum += 2*user_variance_map[user]*n_choose_r(cluster_size,2)  # I later switched to n**2 - n.  2* n_choose_2 == n**2 - n. \n",
    "\n",
    "max_spatial_cov_sd = np.sqrt(energy_consumption_df.confusion_var.sum() + max_cov_sum)\n",
    "print(f\"aggregate standard deviation: {max_spatial_cov_sd}\")\n",
    "error = energy_consumption_df.expected.sum() - energy_consumption_df.user_labeled.sum()\n",
    "print(f\"Error for expected: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_sd = np.sqrt(get_EC.compute_variance_including_spatial_cov_for_trips_dataframe(energy_consumption_df,user_spatial_cov_map))\n",
    "print(f\"aggregate standard deviation: {larger_sd}\")\n",
    "error = energy_consumption_df.expected.sum() - energy_consumption_df.user_labeled.sum()\n",
    "print(f\"Error for expected: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_consumption_from_primary_mode_df = get_EC.compute_all_EC_values_from_primary_mode(expanded_labeled_trips, unit_dist_MCS_df, energy_dict, android_EI_moments_df,ios_EI_moments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_n_sd_map_agg_distance = hf.plot_estimates_with_sd_by_program(energy_consumption_df,os_EI_moments_map,unit_dist_MCS_df, variance_method='aggregate_section_distances', user_spatial_cov_map = user_spatial_cov_map)\n",
    "program_n_sd_map_agg_primary_mode_distance = hf.plot_estimates_with_sd_by_program(energy_consumption_from_primary_mode_df,os_EI_moments_map,unit_dist_MCS_df, variance_method='aggregate_primary_mode_distances', user_spatial_cov_map = user_spatial_cov_map)\n",
    "program_n_sd_map_spatial_cov = hf.plot_estimates_with_sd_by_program(energy_consumption_df,os_EI_moments_map,unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "program_n_sd_map_individual = hf.plot_estimates_with_sd_by_program(energy_consumption_df,os_EI_moments_map,unit_dist_MCS_df, variance_method='independent', user_spatial_cov_map = user_spatial_cov_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sd_df = pd.DataFrame([program_n_sd_map_agg_distance, program_n_sd_map_agg_primary_mode_distance, program_n_sd_map_spatial_cov,program_n_sd_map_individual])\n",
    "print(n_sd_df.rename(index = {0:'Aggregate Section Distance', 1: 'Aggregate Primary Mode Distance', 2:'Spatial Covariance', 3:'Independent Trips'}).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot estimates plus or minus one standard deviation for each program\n",
    "The left dots in each plot are for user labeled values.\n",
    "The right three dots in each plot are expected aka confusion based values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell plots the user labeled and expected aggregate energy consumptions on the left and right, respectively.\n",
    "# It finds aggregate variance by summing individual variances and adding a spatial covariance term.\n",
    "\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_df,os_EI_moments_map,unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Bayes update. \n",
    "### How does spatial covariance perform with different assumed mode distributions than that of MobilityNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_probs_prespecified = {\"Car, sensed\": 0.7, \"Pilot ebike\": 0.13}\n",
    "prior_probs = prior_probs_prespecified.copy()\n",
    "n_other_modes = len(android_confusion.index) - len(prior_probs_prespecified)\n",
    "probability_remaining = 1 - sum(prior_probs_prespecified.values())\n",
    "prior_probs.update({x: probability_remaining/n_other_modes for x in android_confusion.index if x not in prior_probs_prespecified.keys()})\n",
    "#prior_probs = {x: 1/len(android_confusion.index) for x in android_confusion.index} # if you want a uniform prior.\n",
    "\n",
    "android_EI_moments_with_Bayes_update_df = cm_handling.get_Bayesian_conditional_EI_expectation_and_variance(android_confusion,energy_dict, prior_probs)\n",
    "ios_EI_moments_with_Bayes_update_df = cm_handling.get_Bayesian_conditional_EI_expectation_and_variance(ios_confusion,energy_dict, prior_probs)\n",
    "os_EI_moments_with_Bayes_update_map = {'ios': ios_EI_moments_with_Bayes_update_df, 'android': android_EI_moments_with_Bayes_update_df}\n",
    "energy_consumption_with_Bayes_update_df = get_EC.compute_all_EC_values(expanded_labeled_trips,unit_dist_MCS_df,energy_dict,\\\n",
    "    android_EI_moments_with_Bayes_update_df,\\\n",
    "    ios_EI_moments_with_Bayes_update_df, \\\n",
    "    EI_length_cov, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a spatial autocovariance specific to each user. Takes ~1 minute on all ceo up to May 2022.\n",
    "user_spatial_cov_map = {}\n",
    "user_morans_I_map = {}\n",
    "for user in energy_consumption_with_Bayes_update_df.user_id.unique():\n",
    "    user_df = energy_consumption_with_Bayes_update_df[energy_consumption_with_Bayes_update_df.user_id == user].copy()\n",
    "    if len(user_df) < 2: \n",
    "        user_spatial_cov_map[user] = 0\n",
    "        morans_I = 0\n",
    "    else:\n",
    "        user_spatial_cov_map[user], morans_I = get_EC.spatial_autocov_based_on_clusters(user_df, 'expected')\n",
    "    user_morans_I_map[user] = morans_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_percent_errors = pd.DataFrame(hf.get_program_percent_error_map(energy_consumption_with_Bayes_update_df), index=[0]).round(2)\n",
    "print(f\"percent error for each program:\")\n",
    "program_percent_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with prior of 0.7 car, 0.13 ebike.\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with prior of 0.7 car, 0.13 ebike. didn't update spatial cov map from uniform version\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with uniform prior:\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with uniform prior and aggregate distance.\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='aggregate_distance', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with car 0.7, ebike 0.13 and aggregate distance.\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='aggregate_distance', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pcar 0.5, p ebike 0.13:\n",
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(energy_consumption_with_Bayes_update_df,os_EI_moments_with_Bayes_update_map, unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does spatial covariance perform for a mostly ebike dataset, using the mobilitynet implicit prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mostly_ebike_df(df):\n",
    "    df = df.copy()\n",
    "    all_ebike_trips = df[df.mode_confirm == 'pilot_ebike'].copy()\n",
    "    not_ebike_trips = df[df.mode_confirm != 'pilot_ebike'].copy()\n",
    "    n_trips_over_2 = int(np.floor(len(df)/2))\n",
    "    ebike_trip_list = np.random.choice(all_ebike_trips._id,len(df))\n",
    "    other_trip_list = np.random.choice(not_ebike_trips._id,n_trips_over_2)\n",
    "    half_ebike_trips = np.concatenate((ebike_trip_list, other_trip_list))\n",
    "\n",
    "    # Construct a dataframe the size of ceo with 50% of the trips being ebike\n",
    "    half_ebike_trips_idx = df[df._id.isin(half_ebike_trips)].index\n",
    "    return df.loc[half_ebike_trips_idx]\n",
    "\n",
    "\n",
    "mostly_ebike_df = hf.construct_mostly_ebike_df(energy_consumption_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_spatial_cov_map, _ = get_EC.get_user_spatial_cov_map(mostly_ebike_df, estimation_method='expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(mostly_ebike_df, os_EI_moments_map, unit_dist_MCS_df, variance_method='spatial_cov', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_n_sd_map = hf.plot_estimates_with_sd_by_program(mostly_ebike_df,os_EI_moments_map, unit_dist_MCS_df, variance_method='aggregate_distance', user_spatial_cov_map = user_spatial_cov_map)\n",
    "print(program_n_sd_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for cases where mode inference errors are repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by looking for ebike mispredicted as car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebike_but_predicted_as_car = energy_consumption_df[(energy_consumption_df.mode_confirm == 'pilot_ebike') & (energy_consumption_df.primary_mode == 'car')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = ebike_but_predicted_as_car.cluster_id.iloc[0]\n",
    "ebike_but_predicted_as_car[ebike_but_predicted_as_car.cluster_id == cluster_id][['mode_confirm','primary_mode','distance_miles','expected','user_labeled']]\n",
    "# even though these were the same mistake in the same cluster, the trips were different distances and so the energy consumption estimates were not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try looking at a larger cluster. Turns out that large cluster does not guarantee a lot of repeated ebike car mistakes.\n",
    "large_clusters = ebike_but_predicted_as_car[ebike_but_predicted_as_car.cluster_size > 50].cluster_id\n",
    "ebike_but_predicted_as_car[ebike_but_predicted_as_car.cluster_id == large_clusters.iloc[1]][['mode_confirm','primary_mode','distance_miles','expected','user_labeled']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's pick a cluster and look for common mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_clusters = energy_consumption_df[energy_consumption_df.cluster_size > 10].cluster_id\n",
    "energy_consumption_df[energy_consumption_df.cluster_id == large_clusters.iloc[0]][['mode_confirm','primary_mode','distance_miles','expected','predicted','user_labeled']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_clusters = energy_consumption_df[energy_consumption_df.cluster_size > 10].cluster_id\n",
    "energy_consumption_df[(energy_consumption_df.cluster_id == large_clusters.iloc[0]) & (energy_consumption_df.primary_mode == 'car')][['mode_confirm','primary_mode','distance_miles','expected','predicted','user_labeled']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of not a trips.\n",
    "large_clusters = energy_consumption_df[energy_consumption_df.cluster_size > 50].cluster_id\n",
    "cols_of_interest = ['mode_confirm','primary_mode','distance_miles','expected','predicted','user_labeled']\n",
    "energy_consumption_df[(energy_consumption_df.cluster_id == large_clusters.iloc[13000]) & (energy_consumption_df.primary_mode == 'no_sensed')][cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LISAs(df, col_of_interest):\n",
    "    '''\n",
    "    Outputs a Local Indicator of Spatial Association (LISA) for the col_of_interest. Specifically, it finds local Moran's I for each trip.\n",
    "    See https://geographicdata.science/book/notebooks/07_local_autocorrelation.html for details. \n",
    "    The weights are based on cluster membership. If a trip is in the neighborhood of another (aka in the same cluster),\n",
    "    the spatial weight is 1. Otherwise it is 0. Cluster membership is seen in the cluster_id or in the trip_neighbors columns.\n",
    "\n",
    "    df: a trips dataframe that already has assigned clusters for each trip id.\n",
    "    col_of_interest: a string label for the column to find spatial covariance from.\n",
    "        eg, use 'expected' for sensing expected energy consumption estimates.\n",
    "\n",
    "    Returns: a float representing the spatial autocovariance of the variable. \n",
    "        (I think) it should be between -1*v and 1*v, where v is the variance of the variable.\n",
    "    '''\n",
    "    n = len(df)\n",
    "    xbar = np.mean(df[col_of_interest])\n",
    "    var_x = np.var(df[col_of_interest])\n",
    "    cov_sum = 0\n",
    "    W_sum = 0 \n",
    "\n",
    "    local_I = []\n",
    "\n",
    "    for i,trip in df.iterrows():\n",
    "            #if trip['cluster_size'] == 1: \n",
    "            #    local_I.append(0)\n",
    "            #else:\n",
    "            #neighbor_list = list(trip['trip_neighbors']) \n",
    "            # sometimes neighbor list will have neighbors that are not in the timeframe of interest, but neighbors_df will only have the trips of interest.\n",
    "            neighbors_df = df[(df.cluster_id == trip['cluster_id']) & (df._id != trip['_id'])]#neighbors_df = energy_consumption_df[energy_consumption_df['_id'].isin(neighbor_list)].copy()\n",
    "\n",
    "            if len(neighbors_df) == 0:\n",
    "                local_I.append(0)\n",
    "            else:\n",
    "                # for each neighbor, multiply trip i's deviation from the mean by the neighbors deviation from the mean.\n",
    "                # then sum:  ( sum_{j=1:n} w_{ij} (x_i - xbar)(x_j - xbar) )\n",
    "                trip_i_deviation = trip[col_of_interest] - xbar   # scalar\n",
    "                neighbor_deviations = neighbors_df[col_of_interest] - xbar  # array\n",
    "                #print(trip_i_deviation)\n",
    "                #print(neighbor_deviations)\n",
    "\n",
    "                w = 1/len(neighbors_df) # normalize the weights\n",
    "                local_I.append(w*sum(trip_i_deviation*neighbor_deviations)/var_x)\n",
    "\n",
    "    return local_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_clusters = energy_consumption_df[energy_consumption_df.cluster_size > 50].cluster_id\n",
    "\n",
    "no_sensed_user = energy_consumption_df[(energy_consumption_df.cluster_id == large_clusters.iloc[13000])].user_id.iloc[0]\n",
    "trips = energy_consumption_df[energy_consumption_df.user_id == no_sensed_user].copy()\n",
    "LISAs_for_no_sensed_user = calculate_LISAs(trips,'expected')\n",
    "\n",
    "trips['local_I'] = LISAs_for_no_sensed_user\n",
    "trips[trips.cluster_id == large_clusters.iloc[13000]][cols_of_interest + ['cluster_id','local_I']] \n",
    "#trips[trips.local_I > 10].cluster_id\n",
    "\n",
    "#spatial_autocov_based_on_clusters(trips,'expected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.cluster_size.hist(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.local_I.hist(bins=40); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_I_expected = calculate_LISAs(energy_consumption_df[energy_consumption_df.program == 'vail'],'expected')\n",
    "local_I_predicted = calculate_LISAs(energy_consumption_df[energy_consumption_df.program == 'vail'],'predicted')\n",
    "local_I_user_labeled = calculate_LISAs(energy_consumption_df[energy_consumption_df.program == 'vail'],'user_labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at the distribution of local I values that we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "ax = seaborn.kdeplot(local_I_expected)\n",
    "seaborn.rugplot(local_I_expected, ax=ax) # adds bars for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.kdeplot(local_I_user_labeled)\n",
    "seaborn.rugplot(local_I_user_labeled, ax=ax) # adds bars for each observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check for the spatial autocorrelation with synthetic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: trip id, cluster id, primary mode, distance, energy consumption\n",
    "# make a dataframe.\n",
    "n_trips = 10**3\n",
    "n_work_trips = int(0.70*n_trips)\n",
    "n_grocery_trips = int(0.20*n_trips)\n",
    "n_friend_trips = int(0.10*n_trips)\n",
    "\n",
    "work_trip_length = 10 # miles\n",
    "grocery_trip_length = 5\n",
    "friend_trip_length = 20\n",
    "\n",
    "fake_trips_df = pd.DataFrame({\n",
    "    \"_id\": list(range(n_trips)),\n",
    "    \"cluster_id\": [\"0\"]*n_work_trips + [\"1\"]*n_grocery_trips + [\"2\"]*n_friend_trips,\n",
    "    \"primary_mode\": [\"car\"]*n_trips,\n",
    "    \"distance\": [work_trip_length]*n_work_trips + [grocery_trip_length]*n_grocery_trips + [friend_trip_length]*n_friend_trips\n",
    "})\n",
    "\n",
    "r = 1 \n",
    "car_load_factor = (r+1)/(r+0.5)\n",
    "drove_alone_EI = energy_dict[\"Gas Car, drove alone\"]\n",
    "energy_dict.update({\"Car, sensed\": drove_alone_EI/car_load_factor})\n",
    "\n",
    "fake_trips_df['predicted_EC'] = energy_dict['Car, sensed']*fake_trips_df['distance']\n",
    "fake_trips_df['expected_EC'] = 1.1*fake_trips_df['distance']\n",
    "\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'expected_EC', print_statistics=True)\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'predicted_EC', print_statistics=True)\n",
    "print(f\"variance for predicted EC: {np.var(fake_trips_df.predicted_EC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_trips_df['local_I'] = calculate_LISAs(fake_trips_df,'predicted_EC')\n",
    "fake_trips_df[fake_trips_df.cluster_id == \"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: trip id, cluster id, primary mode, distance, energy consumption\n",
    "# make a dataframe.\n",
    "n_trips = 10**3\n",
    "n_work_trips = int(0.50*n_trips)\n",
    "n_grocery_trips = int(0.30*n_trips)\n",
    "n_friend_trips = int(0.20*n_trips)\n",
    "\n",
    "work_trip_length = 10 # miles\n",
    "grocery_trip_length = 5\n",
    "friend_trip_length = 20\n",
    "\n",
    "fake_trips_df = pd.DataFrame({\n",
    "    \"_id\": list(range(n_trips)),\n",
    "    \"cluster_id\": [\"0\"]*n_work_trips + [\"1\"]*n_grocery_trips + [\"2\"]*n_friend_trips,\n",
    "    \"primary_mode\": [\"car\"]*n_trips,\n",
    "    \"distance\": [work_trip_length]*n_work_trips + [grocery_trip_length]*n_grocery_trips + [friend_trip_length]*n_friend_trips\n",
    "})\n",
    "\n",
    "fake_trips_df['predicted_EC'] = energy_dict['Car, sensed']*fake_trips_df['distance']\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'predicted_EC', print_statistics=True)\n",
    "print(f\"variance for predicted EC: {np.var(fake_trips_df.predicted_EC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: trip id, cluster id, primary mode, distance, energy consumption\n",
    "# make a dataframe.\n",
    "n_trips = 10**3\n",
    "\n",
    "n_work_trips = int(0.50*n_trips)\n",
    "n_grocery_trips = int(0.30*n_trips)\n",
    "n_friend_trips = int(0.10*n_trips)\n",
    "n_recreation_trips = int(0.10*n_trips)\n",
    "\n",
    "work_trip_length = 10 # miles\n",
    "grocery_trip_length = 5\n",
    "friend_trip_length = 20\n",
    "recreation_trip_length = 40\n",
    "\n",
    "fake_trips_df = pd.DataFrame({\n",
    "    \"_id\": list(range(n_trips)),\n",
    "    \"cluster_id\": [\"0\"]*n_work_trips + [\"1\"]*n_grocery_trips + [\"2\"]*n_friend_trips + [\"3\"]*n_recreation_trips,\n",
    "    \"primary_mode\": [\"Car, sensed\"]*n_work_trips + [\"Pilot ebike\"]*n_grocery_trips + [\"Car, sensed\"]*n_friend_trips + [\"Car, sensed\"]*n_recreation_trips,\n",
    "    \"distance\": [work_trip_length]*n_work_trips + [grocery_trip_length]*n_grocery_trips + [friend_trip_length]*n_friend_trips + [recreation_trip_length]*n_recreation_trips\n",
    "})\n",
    "\n",
    "fake_trips_df['predicted_EC'] = fake_trips_df.primary_mode.map(energy_dict)*fake_trips_df['distance']\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'predicted_EC', print_statistics=True)\n",
    "print(f\"variance for predicted EC: {np.var(fake_trips_df.predicted_EC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If highly homogeneous:\n",
    "# columns: trip id, cluster id, primary mode, distance, energy consumption\n",
    "# make a dataframe.\n",
    "n_trips = 10**3\n",
    "\n",
    "n_work_trips = int(0.90*n_trips)\n",
    "n_grocery_trips = int(0.05*n_trips)\n",
    "n_friend_trips = int(0.05*n_trips)\n",
    "n_recreation_trips = int(0*n_trips)\n",
    "\n",
    "work_trip_length = 10 # miles\n",
    "grocery_trip_length = 5\n",
    "friend_trip_length = 20\n",
    "recreation_trip_length = 40\n",
    "\n",
    "fake_trips_df = pd.DataFrame({\n",
    "    \"_id\": list(range(n_trips)),\n",
    "    \"cluster_id\": [\"0\"]*n_work_trips + [\"1\"]*n_grocery_trips + [\"2\"]*n_friend_trips + [\"3\"]*n_recreation_trips,\n",
    "    \"primary_mode\": [\"Car, sensed\"]*n_work_trips + [\"Pilot ebike\"]*n_grocery_trips + [\"Car, sensed\"]*n_friend_trips + [\"Car, sensed\"]*n_recreation_trips,\n",
    "    \"distance\": [work_trip_length]*n_work_trips + [grocery_trip_length]*n_grocery_trips + [friend_trip_length]*n_friend_trips + [recreation_trip_length]*n_recreation_trips\n",
    "})\n",
    "\n",
    "fake_trips_df['predicted_EC'] = fake_trips_df.primary_mode.map(energy_dict)*fake_trips_df['distance']\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'predicted_EC', print_statistics=True)\n",
    "print(f\"variance for predicted EC: {np.var(fake_trips_df.predicted_EC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns: trip id, cluster id, primary mode, distance, energy consumption\n",
    "# make a dataframe.\n",
    "n_trips = 10**3\n",
    "\n",
    "n_work_trips = int(0.30*n_trips)\n",
    "n_grocery_trips = int(0.50*n_trips)\n",
    "n_friend_trips = int(0.10*n_trips)\n",
    "n_recreation_trips = int(0.10*n_trips)\n",
    "\n",
    "work_trip_length = 10 # miles\n",
    "grocery_trip_length = 5\n",
    "friend_trip_length = 20\n",
    "recreation_trip_length = 40\n",
    "\n",
    "fake_trips_df = pd.DataFrame({\n",
    "    \"_id\": list(range(n_trips)),\n",
    "    \"cluster_id\": [\"0\"]*n_work_trips + [\"1\"]*n_grocery_trips + [\"2\"]*n_friend_trips + [\"3\"]*n_recreation_trips,\n",
    "    \"primary_mode\": [\"Car, sensed\"]*n_work_trips + [\"Pilot ebike\"]*n_grocery_trips + [\"Car, sensed\"]*n_friend_trips + [\"Car, sensed\"]*n_recreation_trips,\n",
    "    \"distance\": [work_trip_length]*n_work_trips + [grocery_trip_length]*n_grocery_trips + [friend_trip_length]*n_friend_trips + [recreation_trip_length]*n_recreation_trips\n",
    "})\n",
    "\n",
    "fake_trips_df['predicted_EC'] = fake_trips_df.primary_mode.map(energy_dict)*fake_trips_df['distance']\n",
    "spatial_autocov_based_on_clusters(fake_trips_df,'predicted_EC', print_statistics=True)\n",
    "print(f\"variance for predicted EC: {np.var(fake_trips_df.predicted_EC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_mode_confirms = ['drove_alone','shared_ride','walk','pilot_ebike','bus','bike','train','taxi','free_shuttle', 'not_a_trip']\n",
    "main_modes_df = expanded_labeled_trips[expanded_labeled_trips.mode_confirm.isin(main_mode_confirms)].copy()\n",
    "main_modes_df = main_modes_df[main_modes_df.mode_confirm.notna()]\n",
    "\n",
    "match_count = 0\n",
    "for _,ct in main_modes_df.iterrows():\n",
    "    if (ct['primary_mode'] == 'car') and (ct['mode_confirm'] in ['shared_ride', 'taxi']):\n",
    "        match_count += 1\n",
    "    elif (ct['primary_mode'] == 'bicycling') and (ct['mode_confirm'] == 'pilot_ebike'):\n",
    "        match_count += 1\n",
    "    elif (ct['primary_mode'] == 'bus') and (ct['mode_confirm'] == 'free_shuttle'):\n",
    "        match_count += 1\n",
    "    elif MODE_MAPPING_DICT[ct['primary_mode']] == MODE_MAPPING_DICT[ct['mode_confirm']]:\n",
    "        match_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('emission-private-eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
