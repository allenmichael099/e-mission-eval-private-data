{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uuid import UUID\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/mallen2/alternate_branches/eval-compatible-server/e-mission-server')\n",
    "\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.decorations.trip_queries as esdtq\n",
    "import emission.core.wrapper.user as ecwu\n",
    "\n",
    "import confusion_matrix_handling as cm_handling\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "import get_EC\n",
    "import helper_functions as hf\n",
    "\n",
    "import sklearn.model_selection as skm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "\n",
    "import scipy\n",
    "\n",
    "METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
    "\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv') # r stands for raw string, only matters if the path is on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run \"Store_expanded_labeled_trips.ipynb\" first.\n",
    "%store -r expanded_labeled_trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dist_MCS_df = pd.read_csv(\"unit_distance_MCS.csv\").set_index(\"moment\")\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop trips you want to exclude from analysis.\n",
    "expanded_labeled_trips = hf.drop_unwanted_trips(expanded_labeled_trips,drop_not_a_trip=False)\n",
    "\n",
    "# Find the primary mode - the sensed mode with the longest section for each trip.\n",
    "expanded_labeled_trips = hf.get_primary_modes(expanded_labeled_trips,energy_dict,MODE_MAPPING_DICT)\n",
    "\n",
    "print('Here are the number of labeled trips remaining in each program dataset:')\n",
    "expanded_labeled_trips.program.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrices and then the EI moments from those.\n",
    "android_confusion = pd.read_csv(\"android_confusion.csv\").set_index('gt_mode')\n",
    "ios_confusion = pd.read_csv(\"ios_confusion.csv\").set_index('gt_mode')\n",
    "\n",
    "android_confusion = cm_handling.collapse_confusion_matrix(android_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "ios_confusion = cm_handling.collapse_confusion_matrix(ios_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "\n",
    "# here I'm referring to car_load_factor the number that we divide the drove alone energy intensity by\n",
    "# for r = 1, car_load_factor is 4/3.\n",
    "r = 1\n",
    "car_load_factor = (r+1)/(r+0.5)     \n",
    "drove_alone_EI = energy_dict[\"Gas Car, drove alone\"]\n",
    "energy_dict.update({\"Gas Car, sensed\": drove_alone_EI/car_load_factor})\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "EI_length_cov = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the energy consumption for each trip before shuffling and splitting.\n",
    "android_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(android_confusion,energy_dict)\n",
    "ios_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(ios_confusion,energy_dict)\n",
    "os_EI_moments_map = {'ios': ios_EI_moments_df, 'android': android_EI_moments_df}\n",
    "energy_consumption_df = get_EC.compute_all_EC_values(expanded_labeled_trips,unit_dist_MCS_df,energy_dict,android_EI_moments_df,ios_EI_moments_df,\n",
    "    EI_length_cov, print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split All CEO into 7 fake programs and then use splits of those programs to separately estimate correlations.\n",
    "Show that the estimated correlation between percent error and dataset characteristics is roughly the same across fake programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split CEO dataset many times.\n",
    "# compute the factors for each split.\n",
    "# compute the error.\n",
    "# compute the correlation between each variable in IND_VAR with DEP_VAR\n",
    "IND_VAR = ['drove_alone_2_shared_ride', 'no_sensed_ratio', 'car_like_ratio', 'e_bike_ratio', 'not_a_trip_ratio',\n",
    "           \"car_like_as_not_car\", \"e_bike_as_car\", \"e_bike_as_not_car_bike\", \n",
    "           \"non_car_2_car_user_label\", \"mispredicted_as_walk\", \"mispredicted_as_car\", 'distance_miles']\n",
    "DEP_VAR = 'error_pct_for_confusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple crossvalidation splitters in sklearn but all of them split into one training and one test set at a time\n",
    "# If you get this error: \"object of type 'int' has no len()\", just lower the number of splits per round\n",
    "def get_set_splits(df, n_rounds = 50, n_splits_per_round=10):\n",
    "    '''\n",
    "    Splits data into n_rounds * n_splits_per_round sets.\n",
    "    n_splits_per_round controls the size of the resulting data subsets. \n",
    "    To get lots of datasets without shrinking the size too much, we use multiple rounds of splits.\n",
    "\n",
    "    Returns numpy array of arrays of data indices.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    from numpy.random import default_rng\n",
    "    large_size_splits = []\n",
    "    for round in range(n_rounds):\n",
    "        rng = default_rng()\n",
    "        trip_index = np.array(df.index.copy())\n",
    "        rng.shuffle(trip_index)\n",
    "        # print(energy_consumption_df.index, trip_index)\n",
    "        splits = np.array_split(trip_index, n_splits_per_round)\n",
    "        large_size_splits.append(splits)\n",
    "    large_size_splits = np.array(large_size_splits, dtype=object).flatten()\n",
    "    print(f\"Subset lengths: {len(large_size_splits[0])}. Number of subsets: {len(large_size_splits)}\")\n",
    "    #print([len(s) for s in large_size_splits])\n",
    "\n",
    "    return large_size_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several of the split results appear normally distributed about the mean of whatever the dataset used to generate the splits was.\n",
    "def get_split_results(splits):\n",
    "    \n",
    "    CAR_LIKE_MODES = ['drove_alone', 'shared_ride', 'taxi']\n",
    "    NON_CAR_MOTORIZED_MODES = ['bus', 'free_shuttle', 'train']\n",
    "    split_result_list = []\n",
    "    for s in splits:\n",
    "        ERROR_COLS = ['error_for_confusion',\n",
    "           'error_for_prediction', 'expected', 'predicted', 'user_labeled', 'distance_miles', 'distance', 'duration']\n",
    "        curr_split_trips = energy_consumption_df.loc[s]\n",
    "        curr_split_result = {'count': len(s)}\n",
    "        for e in ERROR_COLS:\n",
    "            curr_split_result[e] = curr_split_trips[e].sum()\n",
    "        curr_split_result['drove_alone_2_shared_ride'] = curr_split_trips.query('mode_confirm == \"drove_alone\"').distance.sum() / curr_split_trips.query('mode_confirm == \"shared_ride\"').distance.sum()\n",
    "        curr_split_result['no_sensed_ratio'] = curr_split_trips.query('primary_mode == \"no_sensed\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['car_like_ratio'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum() / curr_split_trips.distance.sum()        \n",
    "        curr_split_result['e_bike_ratio'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['not_a_trip_ratio'] = curr_split_trips.query('mode_confirm == \"not_a_trip\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        \n",
    "        # car_like_as_not_car: the fraction of car trips that were wrongly labeled as not car. \n",
    "        curr_split_result['car_like_as_not_car'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES & primary_mode != \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['e_bike_as_car'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode == \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "        curr_split_result['e_bike_as_not_car_bike'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode != [\"car\", \"bicycling\"]').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "\n",
    "        curr_split_result['non_car_2_car_user_label'] = curr_split_trips.query('mode_confirm == @NON_CAR_MOTORIZED_MODES').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['non_car_2_car_sensed'] = curr_split_trips.query('primary_mode == [\"bus\", \"train\"]').distance.sum() / curr_split_trips.query('primary_mode == \"car\"').distance.sum()\n",
    "        curr_split_result['mispredicted_as_walk'] = curr_split_trips.query('mode_confirm != \"walk\" & primary_mode == \"walking\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['mispredicted_as_car'] = curr_split_trips.query('mode_confirm != @CAR_LIKE_MODES & primary_mode == \"car\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "    \n",
    "        # if curr_split_result['drove_alone_2_shared_ride'] > 0.5:\n",
    "            # print(f\"CHECK: drove_alone %s, shared_ride %s\" % (curr_split_trips.query('mode_confirm == \"drove_alone\"').distance_miles.sum(),\n",
    "            #                                                   curr_split_trips.query('mode_confirm == \"shared_ride\"').distance_miles.sum()))\n",
    "        # print(curr_split_result)\n",
    "        # print(f\"CHECK user_labeled {energy_consumption_df.loc[s].user_labeled.sum()}\")\n",
    "        # print(f\"CHECK error_for_confusion {energy_consumption_df.loc[s].error_for_confusion.sum()}\")\n",
    "        split_result_list.append(curr_split_result)\n",
    "    split_results = pd.DataFrame(split_result_list)\n",
    "    split_results['error_pct_for_confusion'] = (split_results.error_for_confusion / split_results.user_labeled ) * 100\n",
    "    split_results['error_pct_for_prediction'] = (split_results.error_for_prediction / split_results.user_labeled) * 100\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlations(split_results, IND_VAR, DEP_VAR):\n",
    "    ind_var_correlation_df = pd.DataFrame(columns=[\"Independent Variable\", \"Correlation\", \"p-value\"])\n",
    "    for iv in IND_VAR:\n",
    "        corr, p = scipy.stats.pearsonr(split_results[iv],split_results[DEP_VAR])\n",
    "        ind_var_correlation_df = ind_var_correlation_df.append({\"Independent Variable\": iv, \"Correlation\": corr, \"p-value\": p}, ignore_index=True)\n",
    "    return ind_var_correlation_df.set_index(\"Independent Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_and_correlations(df,n_rounds = 50, n_splits_per_round=10):\n",
    "    df = df.copy()\n",
    "    splits = get_set_splits(df, n_rounds, n_splits_per_round)\n",
    "    split_results = get_split_results(splits)\n",
    "    ind_var_correlation_df = find_correlations(split_results, IND_VAR, DEP_VAR)\n",
    "    return ind_var_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 7 fake programs.\n",
    "fake_programs = get_set_splits(energy_consumption_df,n_rounds=1,n_splits_per_round=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_program_correlations_map = {}\n",
    "for i, program in enumerate(fake_programs):\n",
    "    fake_program_correlations_map[i] = get_splits_and_correlations(energy_consumption_df.loc[program])[\"Correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_program_correlations_df = pd.DataFrame(columns=list(range(0,6)))\n",
    "for i in fake_program_correlations_map.keys():\n",
    "    fake_program_correlations_df[i] = fake_program_correlations_map[i]\n",
    "#fake_program_correlations_df[\"variable\"] = IND_VAR\n",
    "fake_program_correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_splits_and_correlations(energy_consumption_df.loc[fake_programs[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again with larger subsets  of the fake programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, program in enumerate(fake_programs):\n",
    "    fake_program_correlations_map[i] = get_splits_and_correlations(energy_consumption_df.loc[program], n_rounds = 100, n_splits_per_round=5)[\"Correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_program_correlations_df = pd.DataFrame(columns=list(range(0,6)))\n",
    "for i in fake_program_correlations_map.keys():\n",
    "    fake_program_correlations_df[i] = fake_program_correlations_map[i]\n",
    "fake_program_correlations_df.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('emission-private-eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
