{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks at correlations between dataset characteristics and percent error\n",
    "I think a better focus would be targeting and improving specific trip level mode errors eg ebike misclassified as car.\\\n",
    "- To see specific mmisprediction rates, look at the cells with print_actual_percents_given_prediction() and print_prediction_percents_given_actual. I think the latter is more important.\n",
    "- It might be useful to make a confusion matrix based on CanBikeCO data and compare it with the mobilitynet confusion matrices.\n",
    "\n",
    "Split datasets into subsets to get different shares of modes \\\n",
    "Calculate subset wide characteristics (eg, proportion of trips that use a car/taxi).\\\n",
    "Are these characteristics correlated with the percent error for expected value based on the sensed modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from uuid import UUID\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import confusion_matrix_handling as cm_handling\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "import get_EC\n",
    "import helper_functions as hf\n",
    "\n",
    "import sklearn.model_selection as skm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "\n",
    "import scipy\n",
    "\n",
    "METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
    "ECAR_PROPORTION = 0 #0.01 #~1% of cars on the road are electric.\n",
    "DROVE_ALONE_TO_SHARED_RIDE_RATIO = 1\n",
    "\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv') # r stands for raw string, only matters if the path is on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already ran store_expanded_labeled_trips.ipynb already and want to save time vs running the cell below\n",
    "%store -r expanded_labeled_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively you could run \"Store_expanded_labeled_trips.ipynb\" first and then this line.\n",
    "# %store -r expanded_labeled_trips \n",
    "\n",
    "'''import database_related_functions as drf  # all the emission server functions for this notebook are in here.\n",
    "\n",
    "user_list, os_map, uuid_program_map = drf.get_participants_programs_and_operating_systems()\n",
    "\n",
    "# 6-14 minutes for entire 1.5 year CEO + stage + prepilot\n",
    "# Takes ~ 1 min 45 s to 2 min 45 s on Macbook Pro for all ceo data up to May 2022.\n",
    "expanded_labeled_trips = drf.get_expanded_labeled_trips(user_list)\n",
    "expanded_labeled_trips['os'] = expanded_labeled_trips.user_id.map(os_map)\n",
    "expanded_labeled_trips['program'] = expanded_labeled_trips['user_id'].map(uuid_program_map)\n",
    "\n",
    "expanded_labeled_trips = expanded_labeled_trips.drop(labels = ['source', 'end_fmt_time', 'end_loc', 'raw_trip',\n",
    "    'start_fmt_time', 'start_loc','start_local_dt_year', 'start_local_dt_month', 'start_local_dt_day',\n",
    "    'start_local_dt_hour', 'start_local_dt_minute', 'start_local_dt_second',\n",
    "    'start_local_dt_weekday', 'start_local_dt_timezone',\n",
    "    'end_local_dt_year', 'end_local_dt_month', 'end_local_dt_day',\n",
    "    'end_local_dt_hour', 'end_local_dt_minute', 'end_local_dt_second',\n",
    "    'end_local_dt_weekday', 'end_local_dt_timezone'], axis = 1)\n",
    "\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "\n",
    "# Group together the prepilot participants\n",
    "prepilot_list = ['84Q9SsrH','cwZazZLJ','CudLAeg8','sxxcLqbK','Q8T7QTXK','5KEGHHuf','e9MaNVU7','7c797MRD','rhBZukxY','k36cxmfA','FmxVf8u6','F3jxHLSW']\n",
    "expanded_labeled_trips['program'] = expanded_labeled_trips.program.replace(prepilot_list, \"prepilot\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dist_MCS_df = pd.read_csv(\"unit_distance_MCS.csv\").set_index(\"moment\")\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping user labeled AIR trips and trips with no OS.\n",
      "Dropped 93 trips with no sensed sections.\n",
      "Here are the number of labeled trips remaining in each program dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cc          28768\n",
       "pc          17880\n",
       "fc          11744\n",
       "stage       10715\n",
       "sc           9092\n",
       "vail         6348\n",
       "4c           5262\n",
       "prepilot     2425\n",
       "Name: program, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop trips you want to exclude from analysis.\n",
    "expanded_labeled_trips = hf.drop_unwanted_trips(expanded_labeled_trips,drop_not_a_trip=False)\n",
    "\n",
    "# Find the primary mode - the sensed mode with the longest section for each trip.\n",
    "expanded_labeled_trips = hf.get_primary_modes(expanded_labeled_trips,energy_dict,MODE_MAPPING_DICT)\n",
    "\n",
    "print('Here are the number of labeled trips remaining in each program dataset:')\n",
    "expanded_labeled_trips.program.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bicycling': 0.07369370712933099,\n",
       " 'walking': 0.06126433729997695,\n",
       " 'car': 0.768106621953594,\n",
       " 'bus': 0.0029946422293117532,\n",
       " 'no_sensed': 0.04626239543074133,\n",
       " 'air_or_hsr': 0.0423027972352415,\n",
       " 'train': 0.00455116607339093,\n",
       " 'subway': 0.000824332648412599}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the proportions of each mode that were sensed\n",
    "sensed_mode_distance_map = {}\n",
    "for _,ct in expanded_labeled_trips.iterrows():\n",
    "    sections_lengths = np.array(ct[\"section_distances\"])*METERS_TO_MILES \n",
    "    for i, mode in enumerate(ct[\"section_modes\"]):\n",
    "        if mode not in sensed_mode_distance_map.keys():\n",
    "            sensed_mode_distance_map[mode] = 0\n",
    "        # Add to the total distance traveled in this mode.\n",
    "        sensed_mode_distance_map[mode] += sections_lengths[i]\n",
    "{x: sensed_mode_distance_map[x]/sum(sensed_mode_distance_map.values()) for x in sensed_mode_distance_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrices and then the EI moments from those.\n",
    "android_confusion = pd.read_csv(\"android_confusion.csv\").set_index('gt_mode')\n",
    "ios_confusion = pd.read_csv(\"ios_confusion.csv\").set_index('gt_mode')\n",
    "\n",
    "android_confusion = cm_handling.collapse_confusion_matrix(android_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "ios_confusion = cm_handling.collapse_confusion_matrix(ios_confusion, rows_to_collapse={\"Train\": [\"Train\"]}, columns_to_collapse={})\n",
    "\n",
    "# here I'm referring to car_load_factor the number that we divide the drove alone energy intensity by\n",
    "# for r = 1, car_load_factor is 4/3.\n",
    "sensed_car_EI = hf.find_sensed_car_energy_intensity(energy_dict, ECAR_PROPORTION, DROVE_ALONE_TO_SHARED_RIDE_RATIO)\n",
    "energy_dict.update({\"Car, sensed\": sensed_car_EI})\n",
    "expanded_labeled_trips['distance_miles'] = expanded_labeled_trips.distance*METERS_TO_MILES\n",
    "EI_length_cov = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the energy consumption for each trip before shuffling and splitting.\n",
    "android_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(android_confusion,energy_dict)\n",
    "ios_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(ios_confusion,energy_dict)\n",
    "os_EI_moments_map = {'ios': ios_EI_moments_df, 'android': android_EI_moments_df}\n",
    "energy_consumption_df = get_EC.compute_all_EC_values(expanded_labeled_trips,unit_dist_MCS_df,energy_dict,android_EI_moments_df,ios_EI_moments_df,\n",
    "    EI_length_cov, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the frequency of actual labels occuring for each predicted label\n",
    "def print_actual_percents_given_prediction(df,main_mode_confirms, which_primary_modes):\n",
    "    df = df[df['mode_confirm'].isin(main_mode_confirms)].copy()\n",
    "    for mode in which_primary_modes:\n",
    "        mode_df = df[df['primary_mode'] == mode]\n",
    "\n",
    "        print(mode)\n",
    "        #print(mode_df.primary_mode.value_counts(normalize=True)) # prediction percentages by mode count\n",
    "\n",
    "        print(mode_df.groupby('mode_confirm').sum().distance/mode_df.distance.sum()) \n",
    "\n",
    "main_mode_confirms = ['drove_alone','shared_ride','walk','pilot_ebike','bus','bike','train','taxi','free_shuttle', 'not_a_trip']\n",
    "\n",
    "print('ios')\n",
    "print_actual_percents_given_prediction(expanded_labeled_trips[expanded_labeled_trips.os == 'ios'].copy(),main_mode_confirms, which_primary_modes= ['car'])\n",
    "\n",
    "print('android')\n",
    "print_actual_percents_given_prediction(expanded_labeled_trips[expanded_labeled_trips.os == 'android'].copy(),main_mode_confirms, which_primary_modes= ['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the frequency of predicted labels occuring for each actual label\n",
    "def print_prediction_percents_given_actual(df,main_mode_confirms, which_actual_modes):\n",
    "    df = df[df['mode_confirm'].isin(main_mode_confirms)].copy()\n",
    "    for mode in which_actual_modes:\n",
    "        mode_df = df[df['mode_confirm'] == mode]\n",
    "\n",
    "        print(mode)\n",
    "        #print(mode_df.primary_mode.value_counts(normalize=True)) # prediction percentages by mode count\n",
    "\n",
    "        print(mode_df.groupby('primary_mode').sum().distance/mode_df.distance.sum()) \n",
    "\n",
    "main_mode_confirms = ['drove_alone','shared_ride','walk','pilot_ebike','bus','bike','train','taxi','free_shuttle', 'not_a_trip']\n",
    "\n",
    "print('ios')\n",
    "print_prediction_percents_given_actual(expanded_labeled_trips[expanded_labeled_trips.os == 'ios'].copy(),main_mode_confirms, which_actual_modes= ['pilot_ebike'])\n",
    "\n",
    "print('android')\n",
    "print_prediction_percents_given_actual(expanded_labeled_trips[expanded_labeled_trips.os == 'android'].copy(),main_mode_confirms, which_actual_modes= ['pilot_ebike'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What factors contribute to the error?\n",
    "NOTE: for future work with the get_set_splits and related functions, might want to update them in a separate file. See dataset_splitting_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split CEO dataset many times.\n",
    "# compute the factors for each split.\n",
    "# compute the error.\n",
    "# compute the correlation.\n",
    "\n",
    "IND_VAR = ['drove_alone_2_shared_ride', 'no_sensed_ratio', 'car_like_ratio', 'e_bike_ratio', 'not_a_trip_ratio',\n",
    "           \"car_like_as_not_car\", \"e_bike_as_car\", \"e_bike_as_not_car_bike\", \n",
    "           \"non_car_2_car_user_label\", \"mispredicted_as_walk\", \"mispredicted_as_car\", 'distance_miles']\n",
    "DEP_VAR = 'error_pct_for_confusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple crossvalidation splitters in sklearn but all of them split into one training and one test set at a time\n",
    "# If you get this error: \"object of type 'int' has no len()\", just lower the number of splits per round\n",
    "def get_set_splits(df, n_rounds = 50, n_splits_per_round=10):\n",
    "    '''\n",
    "    Splits data into n_rounds * n_splits_per_round sets.\n",
    "    n_splits_per_round controls the size of the resulting data subsets. \n",
    "    To get lots of datasets without shrinking the size too much, we use multiple rounds of splits.\n",
    "\n",
    "    Returns numpy array of arrays of data indices.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    from numpy.random import default_rng\n",
    "    large_size_splits = []\n",
    "    for round in range(n_rounds):\n",
    "        rng = default_rng()\n",
    "        trip_index = np.array(df.index.copy())\n",
    "        rng.shuffle(trip_index)\n",
    "        # print(energy_consumption_df.index, trip_index)\n",
    "\n",
    "        # splits is a list of numpy arrays of trip indices\n",
    "        splits = np.array_split(trip_index, n_splits_per_round)\n",
    "\n",
    "        large_size_splits.append(splits)\n",
    "    \n",
    "    unnested_large_size_splits = list(chain.from_iterable(large_size_splits))\n",
    "    print(f\"Subset lengths: {len(unnested_large_size_splits[0])}. Number of subsets: {len(unnested_large_size_splits)}\")\n",
    "    #print([len(s) for s in large_size_splits])\n",
    "\n",
    "    return unnested_large_size_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several of the split results appear normally distributed about the mean of whatever the dataset used to generate the splits was.\n",
    "def get_split_results(splits):\n",
    "    \n",
    "    CAR_LIKE_MODES = ['drove_alone', 'shared_ride', 'taxi']\n",
    "    NON_CAR_MOTORIZED_MODES = ['bus', 'free_shuttle', 'train']\n",
    "    split_result_list = []\n",
    "    for s in splits:\n",
    "        ERROR_COLS = ['error_for_confusion',\n",
    "           'error_for_prediction', 'expected', 'predicted', 'user_labeled', 'distance_miles', 'distance', 'duration']\n",
    "        curr_split_trips = energy_consumption_df.loc[s]\n",
    "        curr_split_result = {'count': len(s)}\n",
    "        for e in ERROR_COLS:\n",
    "            curr_split_result[e] = curr_split_trips[e].sum()\n",
    "        curr_split_result['drove_alone_2_shared_ride'] = curr_split_trips.query('mode_confirm == \"drove_alone\"').distance.sum() / curr_split_trips.query('mode_confirm == \"shared_ride\"').distance.sum()\n",
    "        curr_split_result['no_sensed_ratio'] = curr_split_trips.query('primary_mode == \"no_sensed\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['car_like_ratio'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum() / curr_split_trips.distance.sum()        \n",
    "        curr_split_result['e_bike_ratio'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['not_a_trip_ratio'] = curr_split_trips.query('mode_confirm == \"not_a_trip\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        \n",
    "        # car_like_as_not_car: the fraction of car trips that were wrongly labeled as not car. \n",
    "        curr_split_result['car_like_as_not_car'] = curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES & primary_mode != \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['e_bike_as_car'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode == \"car\"').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "        curr_split_result['e_bike_as_not_car_bike'] = curr_split_trips.query('mode_confirm == \"pilot_ebike\" & primary_mode != [\"car\", \"bicycling\"]').distance.sum() / curr_split_trips.query('mode_confirm == \"pilot_ebike\"').distance.sum()\n",
    "\n",
    "        curr_split_result['non_car_2_car_user_label'] = curr_split_trips.query('mode_confirm == @NON_CAR_MOTORIZED_MODES').distance.sum() / curr_split_trips.query('mode_confirm == @CAR_LIKE_MODES').distance.sum()\n",
    "        curr_split_result['non_car_2_car_sensed'] = curr_split_trips.query('primary_mode == [\"bus\", \"train\"]').distance.sum() / curr_split_trips.query('primary_mode == \"car\"').distance.sum()\n",
    "        curr_split_result['mispredicted_as_walk'] = curr_split_trips.query('mode_confirm != \"walk\" & primary_mode == \"walking\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "        curr_split_result['mispredicted_as_car'] = curr_split_trips.query('mode_confirm != @CAR_LIKE_MODES & primary_mode == \"car\"').distance.sum() / curr_split_trips.distance.sum()\n",
    "    \n",
    "        # if curr_split_result['drove_alone_2_shared_ride'] > 0.5:\n",
    "            # print(f\"CHECK: drove_alone %s, shared_ride %s\" % (curr_split_trips.query('mode_confirm == \"drove_alone\"').distance_miles.sum(),\n",
    "            #                                                   curr_split_trips.query('mode_confirm == \"shared_ride\"').distance_miles.sum()))\n",
    "        # print(curr_split_result)\n",
    "        # print(f\"CHECK user_labeled {energy_consumption_df.loc[s].user_labeled.sum()}\")\n",
    "        # print(f\"CHECK error_for_confusion {energy_consumption_df.loc[s].error_for_confusion.sum()}\")\n",
    "        split_result_list.append(curr_split_result)\n",
    "    split_results = pd.DataFrame(split_result_list)\n",
    "    split_results['error_pct_for_confusion'] = (split_results.error_for_confusion / split_results.user_labeled ) * 100\n",
    "    split_results['error_pct_for_prediction'] = (split_results.error_for_prediction / split_results.user_labeled) * 100\n",
    "    return split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell finds the correlation between the independent variables and the dependent variable. \n",
    "# It also makes scatterplots of the dependent variable vs each independent variable.\n",
    "splits = get_set_splits(energy_consumption_df)\n",
    "split_results = get_split_results(splits)\n",
    "fig, ax2d = plt.subplots(nrows=3, ncols=4, figsize=(8,8), sharey=True)\n",
    "fig.tight_layout(h_pad = 3)\n",
    "axarray = ax2d.flatten()\n",
    "IND_VAR = ['drove_alone_2_shared_ride', 'no_sensed_ratio', 'car_like_ratio', 'e_bike_ratio', 'not_a_trip_ratio',\n",
    "           \"car_like_as_not_car\", \"e_bike_as_car\", \"e_bike_as_not_car_bike\", \n",
    "           \"non_car_2_car_user_label\", \"mispredicted_as_walk\", \"mispredicted_as_car\", 'distance_miles']\n",
    "DEP_VAR = 'error_pct_for_confusion'\n",
    "\n",
    "ind_var_correlation_map = {}\n",
    "ind_var_correlation_df = pd.DataFrame(columns=[\"Independent Variable\", \"Correlation\", \"p-value\"])\n",
    "for iv, ax in zip(IND_VAR, axarray):\n",
    "    split_results.plot(x=iv, y=DEP_VAR, ax=ax, kind=\"scatter\")\n",
    "\n",
    "    ind_var_correlation_map[iv] = round(split_results[iv].corr(split_results[DEP_VAR]),3)\n",
    "    corr, p = scipy.stats.pearsonr(split_results[iv],split_results[DEP_VAR])\n",
    "    \n",
    "    ind_var_correlation_df = ind_var_correlation_df.append({\"Independent Variable\": iv, \"Correlation\": corr, \"p-value\": p}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlations(split_results, IND_VAR, DEP_VAR):\n",
    "    ind_var_correlation_df = pd.DataFrame(columns=[\"Independent Variable\", \"Correlation\", \"p-value\"])\n",
    "    for iv in IND_VAR:\n",
    "        corr, p = scipy.stats.pearsonr(split_results[iv],split_results[DEP_VAR])\n",
    "        ind_var_correlation_df = ind_var_correlation_df.append({\"Independent Variable\": iv, \"Correlation\": corr, \"p-value\": p}, ignore_index=True)\n",
    "    return ind_var_correlation_df.set_index(\"Independent Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits_and_correlations(df,n_rounds = 50, n_splits_per_round=10):\n",
    "    df = df.copy()\n",
    "    splits = get_set_splits(df, n_rounds, n_splits_per_round)\n",
    "    split_results = get_split_results(splits)\n",
    "    ind_var_correlation_df = find_correlations(split_results, IND_VAR, DEP_VAR)\n",
    "    return ind_var_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_no_update = get_splits_and_correlations(energy_consumption_df)\n",
    "# based on energy consumption df with no prior.\n",
    "# it seems that the \n",
    "ind_var_correlation_df_no_update.query('`p-value` < 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string replace the underscores\n",
    "ind_var_correlation_df_no_update = ind_var_correlation_df_no_update.reset_index()\n",
    "ind_var_correlation_df_no_update['Independent Variable'] = ind_var_correlation_df_no_update['Independent Variable'].str.replace('_',' ')\n",
    "print(ind_var_correlation_df_no_update.query('`p-value` < 0.01')[[\"Independent Variable\",\"Correlation\"]].round(2).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old values with p < 0.01 for up to May 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           car like ratio &        -0.73 \\\\\n",
    "             e bike ratio &        -0.21 \\\\\n",
    "      car like as not car &        -0.44 \\\\\n",
    "            e bike as car &         0.16 \\\\\n",
    "     mispredicted as walk &         0.69 \\\\\n",
    "      mispredicted as car &        -0.15 \\\\\n",
    "           distance miles &         0.37 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_no_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note. Approach differently if the android ground truth modes are not the same set as ios ground truth modes in the test dataset.\n",
    "available_ground_truth_modes = android_confusion.index\n",
    "mostly_car_prior = hf.construct_prior_dict({\"Car, sensed\": 0.80, \"Pilot ebike\": 0.05}, available_ground_truth_modes)\n",
    "uniform_prior = hf.construct_prior_dict({}, available_ground_truth_modes)\n",
    "half_ebike = hf.construct_prior_dict({\"Pilot ebike\": 0.50}, available_ground_truth_modes)\n",
    "\n",
    "\n",
    "prior_mode_distributions_map = {\"80 Percent Car\": mostly_car_prior, \"Uniform Prior\": uniform_prior, \"50 Percent Ebike\": half_ebike}\n",
    "_, prior_and_energy_dataframe_map  = hf.prior_mode_distribution_sensitivity_analysis(expanded_labeled_trips, prior_mode_distributions_map, android_confusion, ios_confusion, unit_dist_MCS_df, energy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 40 seconds for each \"get_splits_and_correlations\" on all CEO up to May 2022\n",
    "ind_var_correlation_df_mostly_car_prior = get_splits_and_correlations(prior_and_energy_dataframe_map[\"80 Percent Car\"])\n",
    "ind_var_correlation_df_uniform_prior = get_splits_and_correlations(prior_and_energy_dataframe_map[\"Uniform Prior\"])\n",
    "ind_var_correlation_df_50_percent_ebike = get_splits_and_correlations(prior_and_energy_dataframe_map[\"50 Percent Ebike\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_mostly_car_prior.query('`p-value` < 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_uniform_prior.query('`p-value` < 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vail_correlations = get_splits_and_correlations(energy_consumption_df.query('program == \"vail\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vail_correlations#.query('`p-value` < 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vail_split_results = get_split_results(get_set_splits(energy_consumption_df.query('program == \"vail\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vail_split_results.drove_alone_2_shared_ride.hist()  # the range of values represented in vail is different than that found in all_ceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ceo_split_results = get_split_results(get_set_splits(energy_consumption_df))\n",
    "all_ceo_split_results.drove_alone_2_shared_ride.hist()\n",
    "plt.xlabel(\"drove alone : shared ride\")\n",
    "plt.title(\"Based on splits of all ceo\")\n",
    "plt.figure()\n",
    "pc_split_results = get_split_results(get_set_splits(energy_consumption_df.query('program == \"pc\"'), n_rounds=40, n_splits_per_round=8))\n",
    "pc_split_results.drove_alone_2_shared_ride.hist()\n",
    "plt.xlabel(\"drove alone : shared ride\")\n",
    "plt.title(\"Based on splits of all pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_correlations(all_ceo_split_results, IND_VAR, DEP_VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_corrs = find_correlations(pc_split_results, IND_VAR, DEP_VAR)\n",
    "pc_corrs\n",
    "\n",
    "# Note that with no Baye's update the ebike ratio's correlation with the error was ~ -0.25 for all ceo splits but 0.25 for pc splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the errors are also distributed differently if we split all ceo vs if we split pc\n",
    "pc_split_results.error_for_confusion.hist()\n",
    "plt.figure()\n",
    "all_ceo_split_results.error_for_confusion.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car like ratio seems to matter in all cases.\n",
    "# not a trip ratio's correlation is very different between pc and all ceo\n",
    "# mispredicted as walk's correlation is very different between pc and all ceo. it is consistently large positive for different priors with all ceo\n",
    "# drove alone to shared ride is negatively correlated in all cases, though very weak in all ceo and strong in pc and vail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var_correlation_df_50_percent_ebike#.query('`p-value` < 0.01')  \n",
    "# ebike ratio is negative here which makes sense because increasing the ebike percent means \n",
    "# that the data is closer to our assumption of 50% ebike. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would I expect for the correlation between error and the dataset's drove alone to shared ride ratio, \n",
    "# given that I always assume that the ratio is 1:1?\n",
    "r = np.arange(0.1,2,0.05)\n",
    "car_load_factor = (r+1)/(r+0.5)    \n",
    "\n",
    "plt.scatter(r,car_load_factor)\n",
    "plt.xlabel(\"Drove Alone to Shared Ride Ratio\")\n",
    "plt.ylabel(\"EI divider\")\n",
    "\n",
    "# smaller car load factor (energy intensity divider) means that to minimize error, we should use an energy intensity closer to that of Gas Car, Drove Alone.\n",
    "# As a reminder we actually use r = 1 and car_load_factor = 1.33.\n",
    "# if the dataset r is below 1, that means there are fewer drove alones than we assumed. Most of the datasets have a smaller drove alone to shared ride ratio than 1. \n",
    "# (shared rides account for more distance traveled than drove alone)\n",
    "# as r increases toward 1, we get closer to our assumption, so the error goes down.\n",
    "# If the drove alone to shared ride is past 1 for the dataset, we are dividing by more than we need to so the gas car sensed EI \n",
    "# that we are using is smaller than it needs to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_4c = get_splits_and_correlations(energy_consumption_df.query('program == \"4c\"'))\n",
    "\n",
    "# we thought that this would happen but something else happened instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does dataset size (number of trips) relate to the percent error?\n",
    "- split all ceo into subsets for 30 possible subset sizes.\n",
    "- compute percent error for each.\n",
    "- compute correlation.\n",
    "- repeat? compute average of the correlations?\n",
    "- try with a different prior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trips_pct_error_corrs_list = []\n",
    "\n",
    "for i in range(30):\n",
    "    pct_error = []\n",
    "    n_trips = []\n",
    "    for j in reversed(range(34)):\n",
    "        if j < 4: continue\n",
    "        splits = hf.get_set_splits(energy_consumption_df, n_rounds=1, n_splits_per_round=j, print_info=False)\n",
    "        sub_df = energy_consumption_df.loc[splits[0]]\n",
    "        pct_error.append(100*hf.relative_error(sub_df.expected.sum(),sub_df.user_labeled.sum()))\n",
    "        n_trips.append(len(sub_df))\n",
    "    n_trips_pct_error_corrs_list.append(scipy.stats.pearsonr(np.array(pct_error),np.array(n_trips))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_error = []\n",
    "n_trips = []\n",
    "for i in range(40):\n",
    "    for j in reversed(range(34)):\n",
    "        if j < 4: continue\n",
    "        splits = hf.get_set_splits(energy_consumption_df, n_rounds=1, n_splits_per_round=j, print_info=False)\n",
    "        sub_df = energy_consumption_df.loc[splits[0]]\n",
    "        pct_error.append(100*hf.relative_error(sub_df.expected.sum(),sub_df.user_labeled.sum()))\n",
    "        n_trips.append(len(sub_df))\n",
    "correlation = scipy.stats.pearsonr(np.array(pct_error),np.array(n_trips))[0]\n",
    "print(correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('emission-private-eval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
